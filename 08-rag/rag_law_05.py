import json
import re
import time
from pathlib import Path
import streamlit as st
import chromadb
import numpy as np
from llama_index.core import Settings, VectorStoreIndex, StorageContext, PromptTemplate, get_response_synthesizer
from llama_index.core.schema import TextNode
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.llms.openai_like import OpenAILike
from llama_index.llms.vllm import Vllm
from llama_index.vector_stores.chroma import ChromaVectorStore
from modelscope.utils.plugins import storage
from llama_index.core.postprocessor import SentenceTransformerRerank  # æ–°å¢é‡æ’åºç»„ä»¶

# ================== Streamlit é¡µé¢é…ç½® ==================
st.set_page_config(
    page_title="æ™ºèƒ½åŠ³åŠ¨æ³•å’¨è¯¢åŠ©æ‰‹",
    page_icon="âš–ï¸",
    layout="centered",
    initial_sidebar_state="auto"
)


def disable_streamlit_watcher():
    """Patch Streamlit to disable file watcher"""

    def _on_script_changed(_):
        return

    from streamlit import runtime
    runtime.get_instance()._on_script_changed = _on_script_changed


class OpenAIConfig:
    # ä½¿ç”¨ VLLM æ¨ç†
    # API_BASE = "http://localhost:8000/v1"
    # LLM_MODEL_PATH = "/home/ubuntu/llm/models/Qwen/Qwen2.5-7B-Instruct-identity"

    # ä½¿ç”¨ Ollama æ¨ç†
    API_BASE = "http://localhost:11434/v1"
    LLM_MODEL_PATH = "qwen2.5:7b"
    API_KEY = "no-key-required"
    TIME_OUT = 60


# ä½¿ç”¨ OpenAI æ¨ç†
def init_open_ai_llm():
    return OpenAILike(
        model=OpenAIConfig.LLM_MODEL_PATH,
        api_base=OpenAIConfig.API_BASE,
        api_key=OpenAIConfig.API_KEY,
        temperature=0.3,
        max_tokens=4096,
        timeout=OpenAIConfig.TIME_OUT,
        is_chat_model=True,
        additional_kwargs={"stop": ["<|im_end|>"]}
    )


class Config:
    EMBEDDING_MODEL_PATH = "/Users/wangweijun/LLM/models/text2vec-base-chinese-sentence"
    # EMBEDDING_MODEL_PATH = "/Users/wangweijun/LLM/models/bge-large-zh-v1.5"
    LLM_MODEL_PATH = "/Users/wangweijun/LLM/models/Qwen/Qwen2.5-7B-Instruct"
    RERANK_MODEL_PATH = r"/Users/wangweijun/LLM/models/bge-reranker-large"  # æ–°å¢é‡æ’åºæ¨¡å‹è·¯å¾„

    DATA_DIR = "./data"
    VECTOR_DB_DIR = "./chroma_db"
    PERSIST_DIR = "./storage"
    COLLECTION_NAME = "chinese_labor_law_collection"

    TOP_K = 15
    RERANK_TOP_K = 5  # é‡æ’åºåä¿ç•™æ•°é‡
    MIN_RERANK_SCORE = 0.4  # é‡æ’åºåä¿ç•™çš„æœ€å°åˆ†æ•°

    # RESPONSE_TEMPLATE = PromptTemplate((
    #     "<|im_start|>system\n"
    #     "æ‚¨æ˜¯ä¸­å›½åŠ³åŠ¨æ³•é¢†åŸŸä¸“ä¸šåŠ©æ‰‹ï¼Œå¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\n"
    #     "1.ä»…ä½¿ç”¨æä¾›çš„æ³•å¾‹æ¡æ–‡å›ç­”é—®é¢˜\n"
    #     "2.è‹¥é—®é¢˜ä¸åŠ³åŠ¨æ³•æ— å…³æˆ–è¶…å‡ºçŸ¥è¯†åº“èŒƒå›´ï¼Œæ˜ç¡®å‘ŠçŸ¥æ— æ³•å›ç­”\n"
    #     "3.å¼•ç”¨æ¡æ–‡æ—¶æ ‡æ³¨å‡ºå¤„\n\n"
    #     "å¯ç”¨æ³•å¾‹æ¡æ–‡ï¼ˆå…±{context_count}æ¡ï¼‰ï¼š\n{context_str}\n<|im_end|>\n"
    #     "<|im_start|>user\né—®é¢˜ï¼š{query_str}<|im_end|>\n"
    #     "<|im_start|>assistant\n"
    # ))


@st.cache_resource(show_spinner="åˆå§‹åŒ–æ¨¡å‹ä¸­...")
def init_models():
    """ åˆå§‹åŒ–æ¨¡å‹ """
    # ä½¿ç”¨ HuggingFaceEmbedding åŠ è½½ embedding æ¨¡å‹
    embed_model = HuggingFaceEmbedding(
        model_name=Config.EMBEDDING_MODEL_PATH,
    )

    # ä½¿ç”¨ HuggingFaceLLM åŠ è½½ llm æ¨¡å‹
    llm = HuggingFaceLLM(
        model_name=Config.LLM_MODEL_PATH,
        tokenizer_name=Config.LLM_MODEL_PATH,
        model_kwargs={"trust_remote_code": True},
        tokenizer_kwargs={"trust_remote_code": True},
        generate_kwargs={"temperature": 0.3}
    )

    # ä½¿ç”¨ OpenAI åŠ è½½ llm æ¨¡å‹
    # llm = init_open_ai_llm()

    # åˆå§‹åŒ–é‡æ’åºå™¨ï¼ˆæ–°å¢ï¼‰
    reranker = SentenceTransformerRerank(
        model=Config.RERANK_MODEL_PATH,
        top_n=Config.RERANK_TOP_K
    )

    # å…¨å±€é…ç½®
    Settings.embed_model = embed_model
    Settings.llm = llm

    # éªŒè¯ embedding æ¨¡å‹
    test_embedding = embed_model.get_text_embedding("æµ‹è¯•æ–‡æœ¬")
    print(f"Embedding çº¬åº¦éªŒè¯ï¼š{len(test_embedding)}")

    # è¿”å› embedding æ¨¡å‹ï¼Œllm æ¨¡å‹, reranker æ¨¡å‹
    return embed_model, llm, reranker


def load_and_validate_json_files(data_dir: str) -> list[dict]:
    # ç”±äºæ¯æ¡æ¡æ¬¾å°±æ˜¯ä¸€ä¸ª documentï¼Œæˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œæ‹†åˆ†ä¸ºå¤šä¸ª nodeï¼Œæ‰€ä»¥å°±ä¸ä½¿ç”¨ SimpleDirectoryReader äº†
    # json_files = list(Path(data_dir).glob("*.json"))
    json_files = list(Path(data_dir).glob("train_lobor_law.json"))
    assert json_files, f"æœªæ‰¾åˆ° {data_dir} ç›®å½•ä¸‹çš„ json æ–‡ä»¶"

    all_data = []
    for json_file in json_files:
        with open(json_file, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                if not isinstance(data, list):
                    raise ValueError(f"{json_file.name} æ–‡ä»¶ï¼Œåº”è¯¥ä¸ºåˆ—è¡¨ç»“æ„")
                for item in data:
                    if not isinstance(item, dict):
                        raise ValueError(f"{json_file.name} æ–‡ä»¶çš„åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ åº”è¯¥ä¸ºå­—å…¸ç»“æ„")
                    for k, v in item.items():
                        if not isinstance(v, str):
                            raise ValueError(f"{json_file.name} æ–‡ä»¶çš„å­—å…¸ä¸­ \"{k}\" çš„çš„å€¼åº”è¯¥ä¸ºå­—ç¬¦ä¸²")
                    all_data.append(
                        {
                            "content": item,
                            "metadata": {
                                "source": json_file.name
                            }
                        }
                    )
            except Exception as e:
                raise RuntimeError(f"åŠ è½½ {json_file.name} å¤±è´¥ï¼š{str(e)}")

    print(f"æˆåŠŸåŠ è½½ {len(all_data)} ä¸ªæ³•å¾‹æ–‡ä»¶")
    return all_data


def create_nodes(raw_data: list[dict]) -> list[TextNode]:
    nodes = []
    for entry in raw_data:
        # å­—å…¸ï¼Œkey ä¸ºæ¡æ¬¾åç§°ï¼Œvalue ä¸ºæ¡æ¬¾å†…å®¹
        law_dict = entry["content"]
        # æ¡æ¬¾æ¥æº
        source_file = entry["metadata"]["source"]

        for title, content in law_dict.items():
            # ä½¿ç”¨æ–‡ä»¶æ¥æºå’Œæ¡æ¬¾åç§°ä½œä¸º IDï¼ˆå”¯ä¸€æ ‡è¯†ç¬¦)
            node_id = f"{source_file}::{title}"
            # åˆ‡å‰² titleï¼Œè·å–æ³•å¾‹åç§°å’Œæ¡æ¬¾åç§°
            parts = title.split(" ", 1)  # 1 è¡¨ç¤ºåšå¤šåˆ†å‰²ä¸€æ¬¡ï¼Œä¹Ÿå°±æ˜¯æ— è®ºå¤šä¸ªä¸ªç©ºæ ¼ï¼Œåªä¼šäº§ç”Ÿä¸€ä¸ªé•¿åº¦ä¸º 2 çš„åˆ—è¡¨
            # æ³•å¾‹åç§°
            law_name = parts[0] if len(parts) > 0 else "æœªçŸ¥æ³•å¾‹"
            # æ¡æ¬¾åç§°
            law_clause = parts[1] if len(parts) > 1 else "æœªçŸ¥æ¡æ¬¾"
            # åˆ›å»ºèŠ‚ç‚¹
            node = TextNode(
                id_=node_id,
                text=content,
                metadata={
                    "law_name": law_name,
                    "law_clause": law_clause,
                    "title": title,
                    "source_file": source_file,
                    "content_type": "legal_clause"
                }
            )
            nodes.append(node)
    print(f"æˆåŠŸåˆ›å»º {len(nodes)} ä¸ªæ–‡æœ¬èŠ‚ç‚¹")
    return nodes


@st.cache_resource(show_spinner="åŠ è½½çŸ¥è¯†åº“ä¸­...")
def init_vector_db(nodes: list[TextNode]) -> VectorStoreIndex:
    # åˆ›å»º chroma å®¢æˆ·ç«¯ï¼Œä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨
    chroma_client = chromadb.PersistentClient(path=Config.VECTOR_DB_DIR)
    # åˆ›å»ºæˆ–è€…è·å– chroma é›†åˆ
    chroma_collection = chroma_client.get_or_create_collection(
        name=Config.COLLECTION_NAME,
        # ä½¿ç”¨ cosine ä½™å¼¦ç›¸ä¼¼åº¦ç®—æ³•
        metadata={"hnsw:space": "cosine"}
    )
    # ç¡®ä¿ä¸Šä¸‹æ–‡æ­£ç¡®åˆå§‹åŒ–
    storage_context = StorageContext.from_defaults(
        vector_store=ChromaVectorStore(chroma_collection=chroma_collection)
    )

    # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ›å»ºç´¢å¼•
    if chroma_collection.count() == 0 and nodes is not None:
        print("åˆ›å»ºç´¢å¼•...")
        # å°†èŠ‚ç‚¹å­˜å‚¨åˆ° storage ä¸­ï¼Œstore_text è¡¨ç¤ºæ˜¯å¦å­˜å‚¨æ–‡æ¡£å†…å®¹
        storage_context.docstore.add_documents(nodes, store_text=True)
        # å°†æ–‡æ¡£å­˜å‚¨ä¸­çš„æ•°æ®ä¿å­˜åˆ°æŒ‡å®šç›®å½•
        storage_context.persist(Config.PERSIST_DIR)
        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex(
            nodes=nodes,
            storage_context=storage_context,  # å°†ç´¢å¼•çš„æ•°æ®å°±ä¼šå’Œä¹‹å‰å­˜å‚¨çš„æ–‡æ¡£æ•°æ®å…³è”èµ·æ¥ï¼Œå¹¶ä¸”å¯ä»¥åˆ©ç”¨ç›¸åŒçš„å­˜å‚¨æœºåˆ¶è¿›è¡ŒæŒä¹…åŒ–å’Œç®¡ç†
            show_progress=True
        )
        # å°†ç´¢å¼•ç”Ÿæˆçš„æ•°æ®æŒä¹…åŒ–åˆ°æŒ‡å®šç›®å½•
        index.storage_context.persist(Config.PERSIST_DIR)
    else:
        print("åŠ è½½ç´¢å¼•...")
        storage_context = StorageContext.from_defaults(
            vector_store=ChromaVectorStore(chroma_collection=chroma_collection),
            persist_dir=Config.PERSIST_DIR
        )
        # åŠ è½½ç´¢å¼•
        index = VectorStoreIndex.from_vector_store(
            vector_store=storage_context.vector_store,
            embed_model=Settings.embed_model,
            storage_context=storage_context,
        )

    doc_count = len(storage_context.docstore.docs)
    print(f"å­˜å‚¨çš„æ–‡æ¡£æ•°é‡ï¼š{doc_count}")

    if doc_count > 0:
        sample_key = next(iter(storage_context.docstore.docs.keys()))
        print(f"ç¤ºä¾‹èŠ‚ç‚¹ IDï¼š{sample_key}")
    else:
        print("è­¦å‘Šï¼Œæ–‡æ¡£å­˜å‚¨ä¸ºç©ºï¼Œè¯·æ ¸å®")

    # è¿”å›ç´¢å¼•
    return index


# ================== ç•Œé¢ç»„ä»¶ ==================
def init_chat_interface():
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        role = msg["role"]
        content = msg.get("cleaned", msg["content"])  # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹

        with st.chat_message(role):
            st.markdown(content)

            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”åŒ…å« COT æ€ç»´é“¾
            if role == "assistant" and msg.get("think"):
                with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆå†å²å¯¹è¯ï¼‰"):
                    for think_content in msg["think"]:
                        st.markdown(f'<span style="color: #808080">{think_content.strip()}</span>',
                                    unsafe_allow_html=True)

            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”æœ‰å‚è€ƒä¾æ®ï¼ˆéœ€è¦ä¿æŒåŸæœ‰å‚è€ƒä¾æ®é€»è¾‘ï¼‰
            if role == "assistant" and "reference_nodes" in msg:
                show_reference_details(msg["reference_nodes"])


def show_reference_details(nodes):
    with st.expander("æŸ¥çœ‹æ”¯æŒä¾æ®"):
        for idx, node in enumerate(nodes, 1):
            meta = node.node.metadata
            st.markdown(f"**[{idx}] {meta['title']}**")
            st.caption(f"æ¥æºæ–‡ä»¶ï¼š{meta['source_file']} | æ³•å¾‹åç§°ï¼š{meta['law_name']}")
            st.markdown(f"ç›¸å…³åº¦ï¼š`{node.score:.4f}`")
            # st.info(f"{node.node.text[:300]}...")
            st.info(f"{node.node.text}")


# å…³é”®å­—è¿‡æ»¤å‡½æ•°
def is_legal_question(text: str) -> bool:
    """åˆ¤æ–­é—®é¢˜æ˜¯å¦å±äºæ³•å¾‹å’¨è¯¢"""
    legal_keywords = ["åŠ³åŠ¨æ³•", "åˆåŒ", "å·¥èµ„", "å·¥ä¼¤", "è§£é™¤", "èµ”å¿"]
    return any(keyword in text for keyword in legal_keywords)


def main():
    # ç¦ç”¨ Streamlit æ–‡ä»¶çƒ­é‡è½½
    disable_streamlit_watcher()
    st.title("âš–ï¸ æ™ºèƒ½åŠ³åŠ¨æ³•å’¨è¯¢åŠ©æ‰‹")
    st.markdown("æ¬¢è¿ä½¿ç”¨åŠ³åŠ¨æ³•æ™ºèƒ½å’¨è¯¢ç³»ç»Ÿï¼Œè¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†åŸºäºæœ€æ–°åŠ³åŠ¨æ³•å¾‹æ³•è§„ä¸ºæ‚¨è§£ç­”ã€‚")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "history" not in st.session_state:
        st.session_state.history = []

    embed_model, llm, reranker = init_models()

    # ä¸å­˜åœ¨å‘é‡æ•°æ®åº“ï¼Œèœè¿›è¡Œåˆå§‹åŒ–æ•°æ®
    if not Path(Config.VECTOR_DB_DIR).exists():
        print("åˆå§‹åŒ–æ•°æ®...")
        raw_data = load_and_validate_json_files(Config.DATA_DIR)
        nodes = create_nodes(raw_data)
    else:
        # å·²ç»å­˜åœ¨æ•°æ®ï¼Œä¸éœ€è¦åŠ è½½
        nodes = None

    print("\nåˆå§‹åŒ–å‘é‡å­˜å‚¨...")
    start_time = time.time()
    index = init_vector_db(nodes)
    print(f"ç´¢å¼•åŠ è½½å®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f} ç§’")

    # åˆ›å»ºæ£€ç´¢å™¨å’Œå“åº”åˆæˆå™¨ï¼ˆä¿®æ”¹éƒ¨åˆ†ï¼‰
    retriever = index.as_retriever(
        similarity_top_k=Config.TOP_K  # æ‰©å¤§åˆå§‹æ£€ç´¢æ•°é‡
    )
    response_synthesizer = get_response_synthesizer(
        # text_qa_template=Config.RESPONSE_TEMPLATE,
        verbose=True
    )

    # åˆå§‹åŒ–èŠå¤©ç•Œé¢
    init_chat_interface()

    if prompt := st.chat_input("è¯·è¾“å…¥åŠ³åŠ¨æ³•ç›¸å…³é—®é¢˜"):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # å¤„ç†æŸ¥è¯¢
        with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜..."):
            start_time = time.time()

            # æ£€ç´¢æµç¨‹
            initial_nodes = retriever.retrieve(prompt)
            reranked_nodes = reranker.postprocess_nodes(initial_nodes, query_str=prompt)

            # è¿‡æ»¤èŠ‚ç‚¹
            filtered_nodes = [node for node in reranked_nodes if node.score > Config.MIN_RERANK_SCORE]

            if not filtered_nodes:
                response_text = "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ³•å¾‹æ¡æ–‡ï¼Œè¯·å°è¯•è°ƒæ•´é—®é¢˜æè¿°æˆ–å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆã€‚"
            else:
                # ç”Ÿæˆå›ç­”
                response = response_synthesizer.synthesize(prompt, nodes=filtered_nodes)
                response_text = response.response

            # æ˜¾ç¤ºå›ç­”
            with st.chat_message("assistant"):
                # æå–æ€ç»´é“¾å†…å®¹å¹¶æ¸…ç†å“åº”æ–‡æœ¬
                think_contents = re.findall(r'<think>(.*?)</think>', response_text, re.DOTALL)
                cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()

                # æ˜¾ç¤ºæ¸…ç†åçš„å›ç­”
                st.markdown(cleaned_response)

                # å¦‚æœæœ‰æ€ç»´é“¾å†…å®¹åˆ™æ˜¾ç¤º
                if think_contents:
                    with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                        for content in think_contents:
                            st.markdown(f'<span style="color: #808080">{content.strip()}</span>',
                                        unsafe_allow_html=True)

                # æ˜¾ç¤ºå‚è€ƒä¾æ®ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                # show_reference_details(filtered_nodes[:3])
                show_reference_details(filtered_nodes)

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²ï¼ˆéœ€è¦å­˜å‚¨åŸå§‹å“åº”ï¼‰
            st.session_state.messages.append({
                "role": "assistant",
                "content": response_text,  # ä¿ç•™åŸå§‹å“åº”
                "cleaned": cleaned_response,  # å­˜å‚¨æ¸…ç†åçš„æ–‡æœ¬
                "think": think_contents  # å­˜å‚¨æ€ç»´é“¾å†…å®¹
            })


if __name__ == "__main__":
    # streamlit run rag_law_05.py --server.address=127.0.0.1 --server.fileWatcherType=none
    main()
