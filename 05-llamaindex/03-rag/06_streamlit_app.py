import streamlit as st
import chromadb
from chromadb.errors import InvalidCollectionException
from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage, ServiceContext
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import os

from llama_index.vector_stores.chroma import ChromaVectorStore

st.set_page_config(page_title="LLamaIndex RAG Demo", page_icon="ğŸ¦™", layout="wide")
st.title("LLamaIndex RAG Demo")

@st.cache_resource
def init_model():
    """
    åˆå§‹åŒ–æ¨¡å‹
    """
    # åˆå§‹åŒ– HuggingFaceEmbedding å¯¹è±¡ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
    embed_model_name = "/Users/wangweijun/LLM/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    embed_model = HuggingFaceEmbedding(model_name=embed_model_name)

    Settings.embed_model = embed_model

    # ä½¿ç”¨ HuggingFaceLLM åŠ è½½æœ¬åœ°å¤§æ¨¡å‹
    model_name = "/Users/wangweijun/LLM/models/Qwen/Qwen2___5-0___5B-Instruct"
    llm = HuggingFaceLLM(
        model_name=model_name,
        tokenizer_name=model_name,
        model_kwargs={"trust_remote_code": True},
        tokenizer_kwargs={"trust_remote_code": True}
    )

    Settings.llm = llm

    # åˆ›å»ºå®¢æˆ·ç«¯
    chroma_client = chromadb.PersistentClient(path="./chroma")

    try:
        # è·å–å·²ç»å­˜åœ¨çš„å‘é‡æ•°æ®åº“
        chroma_collection = chroma_client.get_collection("quickstart")
        print("é›†åˆè·å–å®Œæ¯•", chroma_collection)
    except InvalidCollectionException:
        # å¦‚æœé›†åˆä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ–°çš„é›†åˆï¼Œé»˜è®¤ä¼šåœ¨åŒçº§ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª chroma ç›®å½•ï¼Œæ¥å­˜å‚¨å‘é‡
        chroma_collection = chroma_client.create_collection("quickstart")
        print("é›†åˆåˆ›å»ºå®Œæ¯•", chroma_collection)

    # å®šä¹‰å‘é‡å­˜å‚¨
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨ Chroma å‘é‡å­˜å‚¨
    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    # æ£€æŸ¥é›†åˆä¸­æ˜¯å¦æœ‰æ–‡æ¡£
    # todo å¯ä»¥è€ƒè™‘ä½¿ç”¨ chroma_client.get_or_create_collection
    if chroma_collection.count() == 0:
        # è‹¥å­˜å‚¨ç›®å½•ä¸å­˜åœ¨ï¼Œä»æŒ‡å®šç›®å½•è¯»å–æ–‡æ¡£ï¼Œå°†æ•°æ®åŠ è½½åˆ°å†…å­˜
        documents = SimpleDirectoryReader(input_dir="../data", required_exts=[".md"]).load_data()

        # åˆ›å»ºèŠ‚ç‚¹è§£æå™¨
        node_parser = SimpleNodeParser.from_defaults(chunk_size=512)
        # å°†æ–‡æ¡£è§£æä¸ºèŠ‚ç‚¹
        base_node = node_parser.get_nodes_from_documents(documents=documents)

        # åˆ›å»ºä¸€ä¸ª VectorStoreIndex å¯¹è±¡ï¼Œå¹¶ä½¿ç”¨æ–‡æ¡£æ•°æ®æ¥æ„å»ºå‘é‡ç´¢å¼•
        # æ­¤ç´¢å¼•å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶å­˜å‚¨å‘é‡ï¼Œä»¥ä¾¿å¿«é€Ÿæ£€ç´¢
        # å‘é‡å­˜å‚¨åœ¨ chroma ä¸­
        # index = VectorStoreIndex.from_documents(documents, vector_store=vector_store, storage_context=storage_context)
        # æ ¹æ® node èŠ‚ç‚¹ï¼Œåˆ›å»º index
        index = VectorStoreIndex(nodes=base_node, vector_store=vector_store, storage_context=storage_context)

        # å°†ç´¢å¼•æŒä¹…åŒ–å­˜å‚¨åˆ°æœ¬åœ°çš„å‘é‡æ•°æ®åº“
        index.storage_context.persist()
        print("ç´¢å¼•åˆ›å»ºå®Œæ¯•")

    # å°è¯•ä»å‘é‡å­˜å‚¨ä¸­åŠ è½½ç´¢å¼•
    index = VectorStoreIndex.from_vector_store(
        vector_store=vector_store,
        storage_context=storage_context
    )

    # 5ã€åˆ›å»ºä¸€ä¸ªæŸ¥è¯¢å¼•æ“ï¼Œè¿™ä¸ªç´¢å¼•å¯ä»¥æ¥æ”¶æŸ¥è¯¢ï¼Œå¹¶è¿”å›ç›¸å…³æ–‡æ¡£çš„å“åº”
    query_engine = index.as_query_engine()

    return query_engine

# æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–æ¨¡å‹
if "query_engine" not in st.session_state:
    st.session_state["query_engine"] = init_model()

def greet(question):
    query_engine = st.session_state["query_engine"]
    response = query_engine.query(question)
    return response

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„æ™ºèƒ½åŠ©ç†ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°ä½ çš„ï¼Ÿ"}]


for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„æ™ºèƒ½åŠ©ç†ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°ä½ çš„ï¼Ÿ"}]

st.sidebar.button("æ¸…é™¤èŠå¤©è®°å½•", on_click=clear_chat_history)

def generate_llama_index_response(prompt_input):
    return greet(prompt_input)

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ€è€ƒ..."):
            response = generate_llama_index_response(prompt)
            placeholder = st.empty()
            placeholder.markdown(response)
    messages = {"role": "assistant", "content": response}
    st.session_state.messages.append(messages)

# ç¯å¢ƒæ­å»ºï¼š
# pip install streamlit
# pip install watchdog

# æ³¨æ„ï¼š
# st.set_page_config(page_title="LLamaIndex RAG Demo", page_icon="ğŸ¦™", layout="wide")
# st.title("LLamaIndex RAG Demo")
# å¿…é¡»åœ¨æœ€å‰é¢ï¼Œå‰é¢ä¹Ÿä¸èƒ½åŒ…å«ä»»ä½•æ–‡æ¡£çº§åˆ«çš„æ³¨é‡Š
# æ‰€æœ‰æ–‡æ¡£çº§åˆ«çš„æ³¨é‡Šï¼Œéƒ½ä¼šè¢«é¦–é¡µåŠ è½½

# è¿è¡Œå‘½ä»¤ï¼šstreamlit run 06_streamlit_app.py
