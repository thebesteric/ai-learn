import json
import time
import random
from zhipuai import ZhipuAI
from sentence_transformers import SentenceTransformer
import numpy as np

"""
ç¤ºä¾‹æ•°æ®ï¼š
# ç”¨æˆ·è¾“å…¥åº“ï¼ˆå¯è‡ªå®šä¹‰æ‰©å±•ï¼‰
    user_inputs = [
        "ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½", "æ¨èä¸ªç”µå½±å§", "æ€ä¹ˆæ‰èƒ½æ—©ç¡æ—©èµ·",
        "å…»çŒ«å¥½è¿˜æ˜¯å…»ç‹—å¥½", "å·¥ä½œå‹åŠ›å¥½å¤§", "æœ€è¿‘æ€»æ˜¯å¤±çœ "
    ]
"""
# åˆå§‹åŒ–æ¨¡å‹
client = ZhipuAI(api_key="æ›¿æ¢ä¸ºä½ çš„API")  # æ›¿æ¢ä¸ºä½ çš„API Key
# åŠ è½½Embeddingmodel
style_model = SentenceTransformer(r"D:\PycharmProjects\test_20250328\embedding_model\thomas\text2vec-base-chinese")

# ===============================
# 1. é£æ ¼æ¨¡æ¿é…ç½®ï¼ˆä¿®æ­£æ¶ˆæ¯æ ¼å¼ï¼‰
# ================================
style_config = {
    "æ¸©æŸ”": {
        "system_prompt": "ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”ä½“è´´çš„èŠå¤©åŠ©æ‰‹ï¼Œè¯´è¯æ—¶æ€»æ˜¯å……æ»¡å…³æ€€ï¼Œä½¿ç”¨ä»¥ä¸‹ç‰¹å¾ï¼š\n1. åŒ…å«'å‘¢ã€å‘€ã€å•¦'ç­‰è¯­æ°”è¯\n2. ä½¿ç”¨ğŸŒ¸ğŸ’–ğŸ˜Šç­‰æ¸©æš–è¡¨æƒ…\n3. ä¸»åŠ¨è¯¢é—®ç”¨æˆ·æ„Ÿå—",
        "examples": [
            {"role": "user", "content": "ä»Šå¤©å¥½ç´¯å•Š"},
            {"role": "assistant", "content": "è¾›è‹¦å•¦~ è¦ç»™è‡ªå·±æ³¡æ¯çƒ­èŒ¶æ”¾æ¾ä¸€ä¸‹å—ï¼ŸğŸŒ¸"},
            {"role": "user", "content": "è€ƒè¯•æ²¡è€ƒå¥½..."},
            {"role": "assistant", "content": "æ²¡å…³ç³»çš„å‘€~ ä¸‹æ¬¡ä¸€å®šä¼šæ›´å¥½ï¼éœ€è¦æˆ‘é™ªä½ èŠèŠå—ï¼ŸğŸ˜Š"}
        ],
        "temperature": 0.3
    },
    "æ¯’èˆŒ": {
        "system_prompt": "ä½ æ˜¯ä¸€ä¸ªå–œæ¬¢ç”¨çŠ€åˆ©åæ§½è¡¨è¾¾å…³å¿ƒçš„æœ‹å‹ï¼Œéœ€æ»¡è¶³ï¼š\n1. ä½¿ç”¨ç½‘ç»œæµè¡Œè¯­ï¼ˆå¦‚'æ “Q''é€€é€€é€€'ï¼‰\n2. åŒ…å«å¤¸å¼ æ¯”å–»ï¼ˆ'ä½ è¿™é€Ÿåº¦å ªæ¯”æ ‘æ‡’'ï¼‰\n3. ç»“å°¾éšè—å…³å¿ƒ",
        "examples": [
            {"role": "user", "content": "åˆèƒ–äº†5æ–¤ï¼"},
            {"role": "assistant", "content": "å¥½å®¶ä¼™ï¼ä½ è¿™æ˜¯è¦æŠŠä½“é‡ç§¤å‹æˆåˆ†å­æ–™ç†ï¼ŸğŸ‹ï¸"},
            {"role": "user", "content": "æ¸¸æˆåˆè¾“äº†"},
            {"role": "assistant", "content": "èœå°±å¤šç»ƒç»ƒï¼éœ€è¦ç»™ä½ æ¨èã€Šä»é›¶å¼€å§‹çš„ç”µç«ä¹‹è·¯ã€‹å—ï¼ŸğŸ®"}
        ],
        "temperature": 0.7
    },
}


# ========================
# 2. ç”Ÿæˆå‡½æ•°ï¼ˆä¿®æ­£æ¶ˆæ¯çš„ç»“æ„ï¼‰
# ========================

def generate_style_data(style_name, num_samples=50):
    config = style_config[style_name]
    data = []

    # æ„å»ºæ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«ç³»ç»Ÿæç¤ºå’Œç¤ºä¾‹å¯¹è¯ï¼‰
    messages = [
        {"role": "system", "content": config["system_prompt"]},
        *config["examples"]  # ç›´æ¥å±•å¼€ç¤ºä¾‹å¯¹è¯
    ]

    # ç”¨æˆ·è¾“å…¥åº“ï¼ˆå¯è‡ªå®šä¹‰æ‰©å±•ï¼‰
    user_inputs = [
        "ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½", "æ¨èä¸ªç”µå½±å§", "æ€ä¹ˆæ‰èƒ½æ—©ç¡æ—©èµ·",
        "å…»çŒ«å¥½è¿˜æ˜¯å…»ç‹—å¥½", "å·¥ä½œå‹åŠ›å¥½å¤§", "æœ€è¿‘æ€»æ˜¯å¤±çœ "
    ]

    for _ in range(num_samples):
        try:
            # éšæœºé€‰æ‹©ç”¨æˆ·è¾“å…¥
            user_msg = random.choice(user_inputs)

            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            current_messages = messages + [
                {"role": "user", "content": user_msg}
            ]

            # è°ƒç”¨APIï¼ˆä¿®æ­£æ¨¡å‹åç§°ï¼‰
            response = client.chat.completions.create(
                model="glm-3-turbo",
                messages=current_messages,
                temperature=config["temperature"],
                max_tokens=100
            )

            # è·å–å›å¤å†…å®¹ï¼ˆä¿®æ­£è®¿é—®è·¯å¾„ï¼‰
            reply = response.choices[0].message.content

            # è´¨é‡è¿‡æ»¤(æ•°æ®å®¡æ ¸)
            if is_valid_reply(style_name, user_msg, reply):
                data.append({
                    "user": user_msg,
                    "assistant": reply,
                    "style": style_name
                })

            time.sleep(1.5)  # é¢‘ç‡é™åˆ¶ä¿æŠ¤

        except Exception as e:
            print(f"ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")

    return data


def is_valid_reply(style, user_msg, reply):
    """è´¨é‡è¿‡æ»¤è§„åˆ™ï¼ˆæ·»åŠ ç©ºå€¼æ£€æŸ¥ï¼‰"""
    # åŸºç¡€æ£€æŸ¥
    if not reply or len(reply.strip()) == 0:
        return False

    # è§„åˆ™1ï¼šå›å¤é•¿åº¦æ£€æŸ¥
    if len(reply) < 5 or len(reply) > 150:
        return False

    # è§„åˆ™2ï¼šé£æ ¼å…³é”®è¯æ£€æŸ¥
    style_keywords = {
        "æ¸©æŸ”": ["å‘¢", "å‘€", "ğŸ˜Š", "ğŸŒ¸"],
        "æ¯’èˆŒ": ["å¥½å®¶ä¼™", "æ “Q", "!", "ğŸ‹ï¸"]
    }
    if not any(kw in reply for kw in style_keywords.get(style, [])):
        return False

    # è§„åˆ™3ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦æ£€æŸ¥
    try:
        ref_text = next(msg["content"] for msg in style_config[style]["examples"]
                        if msg["role"] == "assistant")
        ref_vec = style_model.encode(ref_text)
        reply_vec = style_model.encode(reply)
        similarity = np.dot(ref_vec, reply_vec)
        return similarity > 0.65
    except:
        return False


# =============================
# 3. æ‰§è¡Œç”Ÿæˆï¼ˆæ·»åŠ å®¹é”™ï¼‰
# ============================
if __name__ == '__main__':
    all_data = []

    try:
        print("å¼€å§‹ç”Ÿæˆæ¸©æŸ”é£æ ¼æ•°æ®...")
        gentle_data = generate_style_data("æ¸©æŸ”", 50)
        all_data.extend(gentle_data)

        print("å¼€å§‹ç”Ÿæˆæ¯’èˆŒé£æ ¼æ•°æ®...")
        sarcastic_data = generate_style_data("æ¯’èˆŒ", 50)
        all_data.extend(sarcastic_data)

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å·²ç”Ÿæˆæ•°æ®...")
    finally:
        with open("style_chat_data.json", "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        print(f"æ•°æ®å·²ä¿å­˜ï¼Œæœ‰æ•ˆæ ·æœ¬æ•°ï¼š{len(all_data)}")
