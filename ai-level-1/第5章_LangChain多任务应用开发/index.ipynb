{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ç¬¬5ç«  LangChainå¤šä»»åŠ¡åº”ç”¨å¼€å‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. å¦‚ä½•ä½¿ç”¨ LangChainï¼šä¸€å¥—åœ¨å¤§æ¨¡å‹èƒ½åŠ›ä¸Šå°è£…çš„å·¥å…·æ¡†æ¶\n",
    "2. å¦‚ä½•ç”¨å‡ è¡Œä»£ç å®ç°ä¸€ä¸ªå¤æ‚çš„ AI åº”ç”¨\n",
    "3. é¢å‘å¤§æ¨¡å‹çš„æµç¨‹å¼€å‘çš„è¿‡ç¨‹æŠ½è±¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å†™åœ¨å‰é¢\n",
    "\n",
    "- LangChain ä¹Ÿæ˜¯ä¸€å¥—é¢å‘å¤§æ¨¡å‹çš„å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰\n",
    "- LangChain æ˜¯ AGI æ—¶ä»£è½¯ä»¶å·¥ç¨‹çš„ä¸€ä¸ªæ¢ç´¢å’ŒåŸå‹\n",
    "- å­¦ä¹  LangChain è¦å…³æ³¨æ¥å£å˜æ›´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain çš„æ ¸å¿ƒç»„ä»¶\n",
    "\n",
    "1. æ¨¡å‹ I/O å°è£…\n",
    "   - Chat Modelsï¼šå¯¹è¯­è¨€æ¨¡å‹æ¥å£çš„å°è£…\n",
    "   - PromptTempleï¼šæç¤ºè¯æ¨¡æ¿\n",
    "   - OutputParserï¼šè§£æè¾“å‡º\n",
    "2. æ•°æ®è¿æ¥å°è£…ï¼ˆå¼±äº LlamaIndexï¼‰\n",
    "   - Document Loadersï¼šå„ç§æ ¼å¼æ–‡ä»¶çš„åŠ è½½å™¨\n",
    "   - Document Transformersï¼šå¯¹æ–‡æ¡£çš„å¸¸ç”¨æ“ä½œï¼Œå¦‚ï¼šsplit, filter, translate, extract metadata, etc\n",
    "   - Text Embedding Modelsï¼šæ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºï¼Œç”¨äºæ£€ç´¢ç­‰æ“ä½œ\n",
    "   - Verctorstores & Retrieversï¼šå‘é‡æ•°æ®åº“ä¸å‘é‡æ£€ç´¢\n",
    "3. æ¶æ„å°è£…\n",
    "   - Chain/LCELï¼šå®ç°ä¸€ä¸ªåŠŸèƒ½æˆ–è€…ä¸€ç³»åˆ—é¡ºåºåŠŸèƒ½ç»„åˆ\n",
    "   - Agentï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œè‡ªåŠ¨è§„åˆ’æ‰§è¡Œæ­¥éª¤ï¼Œè‡ªåŠ¨é€‰æ‹©æ¯æ­¥éœ€è¦çš„å·¥å…·ï¼Œæœ€ç»ˆå®Œæˆç”¨æˆ·æŒ‡å®šçš„åŠŸèƒ½\n",
    "     - Toolsï¼šè°ƒç”¨å¤–éƒ¨åŠŸèƒ½çš„å‡½æ•°ï¼Œä¾‹å¦‚ï¼šè°ƒ google æœç´¢ã€æ–‡ä»¶ I/Oã€Linux Shell ç­‰ç­‰\n",
    "   - LangGraphï¼šå·¥ä½œæµå¼€å‘æ¡†æ¶\n",
    "5. LangSmithï¼šè¿‡ç¨‹ç›‘æ§ä¸è°ƒè¯•æ¡†æ¶\n",
    "\n",
    "<img src=\"./assets/langchain.svg\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–‡æ¡£ï¼ˆä»¥ Python ç‰ˆä¸ºä¾‹ï¼‰\n",
    "\n",
    "- åŠŸèƒ½æ¨¡å—ï¼šhttps://python.langchain.com/docs/tutorials\n",
    "- API æ–‡æ¡£ï¼šhttps://python.langchain.com/api_reference/\n",
    "- ä¸‰æ–¹ç»„ä»¶é›†æˆï¼šhttps://python.langchain.com/docs/integrations/providers/\n",
    "- æ›´å¤š HowToï¼šhttps://python.langchain.com/docs/how_to/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain æ˜¯å¼€æºé¡¹ç›®\n",
    "\n",
    "é¡¹ç›®åœ°å€ï¼šhttps://github.com/langchain-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ¨¡å‹ I/O å°è£…\n",
    "\n",
    "æŠŠä¸åŒçš„æ¨¡å‹ï¼Œç»Ÿä¸€å°è£…æˆä¸€ä¸ªæ¥å£ï¼Œæ–¹ä¾¿æ›´æ¢æ¨¡å‹è€Œä¸ç”¨é‡æ„ä»£ç ã€‚\n",
    "\n",
    "### 1.1 æ¨¡å‹ API: ChatModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 OpenAI æ¨¡å‹å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain\n",
    "# !pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ—¨åœ¨å›ç­”é—®é¢˜å’Œæä¾›ä¿¡æ¯ã€‚å¦‚æœä½ æœ‰ä»€ä¹ˆæƒ³é—®çš„ï¼Œæˆ–è€…éœ€è¦å¸®åŠ©çš„ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "response = model.invoke(\"ä½ æ˜¯è°\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 å¤šè½®å¯¹è¯ Session å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯å­¦å‘˜å°èšã€‚å¾ˆé«˜å…´ä¸ä½ äº¤æµï¼æœ‰ä»€ä¹ˆé—®é¢˜æˆ–è€…éœ€è¦å¸®åŠ©çš„åœ°æ–¹å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„assistant role\n",
    "    HumanMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„user role\n",
    "    SystemMessage  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„system role\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯èšå®¢AIå¤§æ¨¡å‹è¯¾çš„è¯¾ç¨‹åŠ©ç†ã€‚\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯å­¦å‘˜ï¼Œæˆ‘å«å°èšã€‚\"),\n",
    "    AIMessage(content=\"æ¬¢è¿ï¼\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯è°ï¼Ÿ\")\n",
    "]\n",
    "\n",
    "ret = model.invoke(messages)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>é€šè¿‡æ¨¡å‹å°è£…ï¼Œå®ç°ä¸åŒæ¨¡å‹çš„ç»Ÿä¸€æ¥å£è°ƒç”¨\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 æ¢ä¸ªå›½äº§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-deepseek"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T04:36:30.159790Z",
     "start_time": "2025-05-22T04:36:22.823501Z"
    }
   },
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "response = model.invoke(\"ä½ æ˜¯è°\")\n",
    "print(response.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸ï¼ˆDeepSeekï¼‰å¼€å‘çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼âœ¨ æˆ‘çš„ä½¿å‘½æ˜¯å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€é™ªä½ èŠå¤©ï¼Œè¿˜èƒ½å¸®ä½ å¤„ç†å„ç§æ–‡æœ¬ã€æ–‡ä»¶å†…å®¹ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œï¼Œè¿˜æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„ç–‘é—®ï¼Œæˆ‘éƒ½ä¼šå°½åŠ›å¸®ä½ æ‰¾åˆ°ç­”æ¡ˆï¼ğŸ˜Š  \n",
      "\n",
      "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 æµå¼è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T04:55:05.392587Z",
     "start_time": "2025-05-22T04:54:56.464027Z"
    }
   },
   "source": [
    "for token in model.stream(\"ä½ æ˜¯è°\"):\n",
    "    print(token.content, end=\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸ï¼ˆDeepSeekï¼‰å¼€å‘çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼âœ¨ æˆ‘çš„ç›®æ ‡æ˜¯ä¸ºä½ æä¾›å„ç§é—®é¢˜çš„è§£ç­”ã€çŸ¥è¯†åˆ†äº«ã€åˆ›æ„çµæ„Ÿä»¥åŠæ—¥å¸¸å¸®åŠ©ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œï¼Œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„å°çƒ¦æ¼ï¼Œéƒ½å¯ä»¥æ¥é—®æˆ‘å“¦ï¼ğŸ˜Š  \n",
      "\n",
      "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ¨¡å‹çš„è¾“å…¥ä¸è¾“å‡º\n",
    "\n",
    "<img src=\"./assets/model_io.jpg\" style=\"margin-left: 0px\" width=1400px>\n",
    "\n",
    "#### 1.2.1 Prompt æ¨¡æ¿å°è£…\n",
    "\n",
    "1. PromptTemplate å¯ä»¥åœ¨æ¨¡æ¿ä¸­è‡ªå®šä¹‰å˜é‡\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T05:59:17.550488Z",
     "start_time": "2025-05-22T05:59:17.515387Z"
    }
   },
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(subject='å°æ˜'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['subject'] input_types={} partial_variables={} template='ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯'\n",
      "===Prompt===\n",
      "ç»™æˆ‘è®²ä¸ªå…³äºå°æ˜çš„ç¬‘è¯\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:04:42.822385Z",
     "start_time": "2025-05-22T06:04:32.491089Z"
    }
   },
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# å®šä¹‰ LLM\n",
    "llm = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "# é€šè¿‡ Prompt è°ƒç”¨ LLM\n",
    "ret = llm.invoke(template.format(subject='å°æ˜'))\n",
    "# æ‰“å°è¾“å‡º\n",
    "print(ret.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥½çš„ï¼æ¥ä¸€ä¸ªå…³äºå°æ˜çš„ç»å…¸æ ¡å›­ç¬‘è¯ï¼š\n",
      "\n",
      "**æ•°å­¦è¯¾çš„ç–‘é—®**  \n",
      "æ•°å­¦è€å¸ˆé—®å°æ˜ï¼šâ€œå¦‚æœä½ æœ‰5å—å·§å…‹åŠ›ï¼Œå¦ˆå¦ˆåˆç»™ä½ 3å—ï¼Œçˆ¸çˆ¸ç»™ä½ 2å—ï¼Œæœ€åä½ åˆ†ç»™å¦¹å¦¹4å—ï¼Œè¿˜å‰©å¤šå°‘ï¼Ÿâ€  \n",
      "\n",
      "å°æ˜è®¤çœŸå›ç­”ï¼šâ€œ**é›¶å—**ã€‚â€  \n",
      "\n",
      "è€å¸ˆçš±çœ‰ï¼šâ€œä½ æ ¹æœ¬æ²¡ç®—å¯¹ï¼â€  \n",
      "å°æ˜å§”å±ˆï¼šâ€œå¯å¦‚æœå¦¹å¦¹æ²¡æ‹¿åˆ°4å—ï¼Œæˆ‘ç°åœ¨åº”è¯¥æ­£åœ¨åŒ»é™¢æŠ¢æ•‘â€¦â€¦â€  \n",
      "\n",
      "ï¼ˆç¬‘ç‚¹ï¼šå°æ˜ç†è§£çš„â€œåˆ†ç»™å¦¹å¦¹â€æ˜¯â€œä¸ç»™å¥¹å°±ä¼šæŒ¨æâ€ï¼Œè€Œè€å¸ˆåªæƒ³è€ƒå‡æ³•ğŸ˜‚ï¼‰  \n",
      "\n",
      "éœ€è¦å…¶ä»–ç±»å‹çš„å°æ˜ç¬‘è¯è¿˜å¯ä»¥å†é—®æˆ‘å“¦ï¼\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ChatPromptTemplate ç”¨æ¨¡æ¿è¡¨ç¤ºçš„å¯¹è¯ä¸Šä¸‹æ–‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ä½ æ˜¯èšå®¢AIå¤§æ¨¡å‹è¯¾ç¨‹çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«å°èš', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ æ˜¯è°', additional_kwargs={}, response_metadata={})]\n",
      "æˆ‘æ˜¯å°èšï¼Œèšå®¢AIå¤§æ¨¡å‹è¯¾ç¨‹çš„å®¢æœåŠ©æ‰‹ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–è€…éœ€è¦å¸®åŠ©ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "open_ai_llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"ä½ æ˜¯{product}çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«{name}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    product=\"èšå®¢AIå¤§æ¨¡å‹è¯¾ç¨‹\",\n",
    "    name=\"å°èš\",\n",
    "    query=\"ä½ æ˜¯è°\"\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "ret = open_ai_llm.invoke(prompt)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MessagesPlaceholder æŠŠå¤šè½®å¯¹è¯å˜æˆæ¨¡æ¿\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:04:49.188267Z",
     "start_time": "2025-05-22T06:04:49.185357Z"
    }
   },
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "human_prompt = \"Translate your answer to {language}.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    # variable_name æ˜¯ message placeholder åœ¨æ¨¡æ¿ä¸­çš„å˜é‡å\n",
    "    # ç”¨äºåœ¨èµ‹å€¼æ—¶ä½¿ç”¨\n",
    "    [human_message_template, MessagesPlaceholder(\"history\")]\n",
    ")\n",
    "\n",
    "chat_prompt.pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n",
      "\n",
      "Translate your answer to \u001B[33;1m\u001B[1;3m{language}\u001B[0m.\n",
      "\n",
      "=============================\u001B[1m Messages Placeholder \u001B[0m=============================\n",
      "\n",
      "\u001B[33;1m\u001B[1;3m{history}\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:04:54.700094Z",
     "start_time": "2025-05-22T06:04:54.697276Z"
    }
   },
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "human_message = HumanMessage(content=\"Who is Elon Musk?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"Elon Musk is a billionaire entrepreneur, inventor, and industrial designer\"\n",
    ")\n",
    "\n",
    "messages = chat_prompt.format_prompt(\n",
    "    # å¯¹ \"history\" å’Œ \"language\" èµ‹å€¼\n",
    "    history=[human_message, ai_message], language=\"ä¸­æ–‡\"\n",
    ")\n",
    "\n",
    "print(messages.to_messages())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Translate your answer to ä¸­æ–‡.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Who is Elon Musk?', additional_kwargs={}, response_metadata={}), AIMessage(content='Elon Musk is a billionaire entrepreneur, inventor, and industrial designer', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:05:02.594793Z",
     "start_time": "2025-05-22T06:04:57.737099Z"
    }
   },
   "source": [
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸƒéš†Â·é©¬æ–¯å…‹ï¼ˆElon Muskï¼‰æ˜¯ä¸€ä½äº¿ä¸‡å¯Œç¿ä¼ä¸šå®¶ã€å‘æ˜å®¶å’Œå·¥ä¸šè®¾è®¡å¸ˆã€‚\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>æŠŠPromptæ¨¡æ¿çœ‹ä½œå¸¦æœ‰å‚æ•°çš„å‡½æ•°\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 ä»æ–‡ä»¶åŠ è½½ Prompt æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:06:15.945237Z",
     "start_time": "2025-05-22T06:06:15.942466Z"
    }
   },
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_file(\"example_prompt_template.txt\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(topic='é»‘è‰²å¹½é»˜'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['topic'] input_types={} partial_variables={} template='ä¸¾ä¸€ä¸ªå…³äº{topic}çš„ä¾‹å­'\n",
      "===Prompt===\n",
      "ä¸¾ä¸€ä¸ªå…³äºé»‘è‰²å¹½é»˜çš„ä¾‹å­\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ç»“æ„åŒ–è¾“å‡º\n",
    "\n",
    "#### 1.3.1 ç›´æ¥è¾“å‡º Pydantic å¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:11:03.857175Z",
     "start_time": "2025-05-22T06:11:03.854203Z"
    }
   },
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# å®šä¹‰ä½ çš„è¾“å‡ºå¯¹è±¡\n",
    "class Date(BaseModel):\n",
    "    year: int = Field(description=\"Year\")\n",
    "    month: int = Field(description=\"Month\")\n",
    "    day: int = Field(description=\"Day\")\n",
    "    era: str = Field(description=\"BC or AD\")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-22T06:13:45.109189Z",
     "start_time": "2025-05-22T06:13:17.514928Z"
    }
   },
   "source": [
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "# llm = ChatTongyi(model=\"qwen-max\")\n",
    "\n",
    "# å®šä¹‰ç»“æ„åŒ–è¾“å‡ºçš„æ¨¡å‹\n",
    "structured_llm = llm.with_structured_output(Date)\n",
    "\n",
    "template = \"\"\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\n",
    "ç”¨æˆ·è¾“å…¥:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "query = \"2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...\"\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "\n",
    "response = structured_llm.invoke(input_prompt)\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 è¾“å‡ºæŒ‡å®šæ ¼å¼çš„ JSON"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:15:59.255282Z",
     "start_time": "2025-05-22T06:15:56.591453Z"
    }
   },
   "source": [
    "# OpenAI æ¨¡å‹çš„JSONæ ¼å¼\n",
    "json_schema = {\n",
    "    \"title\": \"Date\",\n",
    "    \"description\": \"Formated date expression\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"year\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"year, YYYY\",\n",
    "        },\n",
    "        \"month\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"month, MM\",\n",
    "        },\n",
    "        \"day\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"day, DD\",\n",
    "        },\n",
    "        \"era\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"BC or AD\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "structured_llm = llm.with_structured_output(json_schema)\n",
    "\n",
    "structured_llm.invoke(input_prompt)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 2023, 'month': 4, 'day': 6, 'era': 'AD'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 ä½¿ç”¨ OutputParser\n",
    "\n",
    "[`OutputParser`](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) å¯ä»¥æŒ‰æŒ‡å®šæ ¼å¼è§£ææ¨¡å‹çš„è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:18:39.648626Z",
     "start_time": "2025-05-22T06:18:24.631461Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Date)\n",
    "\n",
    "# è·å–æ ¼å¼è¯´æ˜\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\\nç”¨æˆ·è¾“å…¥:{query}\\n{format_instructions}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "print(format_instructions)\n",
    "\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "output = llm.invoke(input_prompt)\n",
    "print(\"åŸå§‹è¾“å‡º:\\n\" + output.content)\n",
    "\n",
    "print(\"\\nè§£æå:\")\n",
    "parser.invoke(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"description\": \"Year\", \"title\": \"Year\", \"type\": \"integer\"}, \"month\": {\"description\": \"Month\", \"title\": \"Month\", \"type\": \"integer\"}, \"day\": {\"description\": \"Day\", \"title\": \"Day\", \"type\": \"integer\"}, \"era\": {\"description\": \"BC or AD\", \"title\": \"Era\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "åŸå§‹è¾“å‡º:\n",
      "æ ¹æ®ç”¨æˆ·è¾“å…¥â€œ2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...â€ï¼Œæˆ‘ä»¬å¯ä»¥æå–å‡ºæ—¥æœŸä¿¡æ¯å¹¶æŒ‰ç…§ç»™å®šçš„JSON Schemaæ ¼å¼åŒ–è¾“å‡ºã€‚ä»è¿™ä¸ªå¥å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºå¹´ä»½æ˜¯2023å¹´ï¼Œæœˆä»½æ˜¯å››æœˆï¼ˆ4æœˆï¼‰ï¼Œæ—¥æœŸæ˜¯6æ—¥ã€‚ç”±äºæ²¡æœ‰ç‰¹åˆ«æŒ‡æ˜æ˜¯å…¬å…ƒå‰è¿˜æ˜¯å…¬å…ƒåï¼Œæˆ‘ä»¬é»˜è®¤ä¸ºå…¬å…ƒï¼ˆADï¼‰ã€‚\n",
      "\n",
      "åŸºäºè¿™äº›ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯ç¬¦åˆè¦æ±‚çš„JSONå®ä¾‹ï¼š\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": 4,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "```\n",
      "\n",
      "è¿™æ®µJSONè¡¨ç¤ºäº†ç”¨æˆ·æåˆ°çš„å…·ä½“æ—¥æœŸï¼Œå¹¶ä¸”æ»¡è¶³äº†æä¾›çš„JSON Schemaçš„è¦æ±‚ã€‚\n",
      "\n",
      "è§£æå:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'year': 2023, 'month': 4, 'day': 6, 'era': 'AD'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¹Ÿå¯ä»¥ç”¨ `PydanticOutputParser`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:25:50.626144Z",
     "start_time": "2025-05-22T06:25:34.264170Z"
    }
   },
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# è§£ææˆå¯¹è±¡\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "output = llm.invoke(input_prompt)\n",
    "print(\"åŸå§‹è¾“å‡º:\\n\" + output.content)\n",
    "\n",
    "print(\"\\nè§£æå:\")\n",
    "parser.invoke(output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹è¾“å‡º:\n",
      "æ ¹æ®ç”¨æˆ·è¾“å…¥\"2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...\"ï¼Œæˆ‘ä»¬å¯ä»¥æå–å‡ºæ—¥æœŸä¿¡æ¯ï¼Œå¹¶æŒ‰ç…§ç»™å®šçš„JSON Schemaæ ¼å¼åŒ–è¾“å‡ºã€‚è¿™é‡Œæˆ‘ä»¬å‡è®¾æ‰€æœ‰æåˆ°çš„æ—¥æœŸéƒ½æ˜¯å…¬å…ƒï¼ˆADï¼‰çºªå¹´çš„ã€‚æå–çš„ä¿¡æ¯åŒ…æ‹¬å¹´ã€æœˆã€æ—¥ã€‚å› æ­¤ï¼Œå¯¹äºè¿™ä¸ªä¾‹å­ï¼Œè¾“å‡ºå°†æ˜¯ï¼š\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": 4,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "```\n",
      "\n",
      "è¿™ç¬¦åˆæä¾›çš„JSON Schemaè¦æ±‚ï¼Œå…¶ä¸­åŒ…å«äº†å¿…é¡»çš„`year`, `month`, `day`å­—æ®µä»¥åŠæŒ‡å®šäº†æ—¶ä»£çš„`era`å­—æ®µã€‚åœ¨è¿™ä¸ªåœºæ™¯ä¸‹ï¼Œâ€œeraâ€è¢«è®¾ç½®ä¸º\"AD\"è¡¨ç¤ºå…¬å…ƒçºªå¹´ã€‚\n",
      "\n",
      "è§£æå:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date(year=2023, month=4, day=6, era='AD')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OutputFixingParser` åˆ©ç”¨å¤§æ¨¡å‹åšæ ¼å¼è‡ªåŠ¨çº é”™"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T06:57:38.922382Z",
     "start_time": "2025-05-22T06:57:33.779607Z"
    }
   },
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "# çº é”™èƒ½åŠ›ä¸å¤§æ¨¡å‹èƒ½åŠ›ç›¸å…³\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)\n",
    "\n",
    "bad_output = output.content.replace(\"4\", \"å››\")\n",
    "print(\"PydanticOutputParser:\")\n",
    "try:\n",
    "    parser.invoke(bad_output)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"OutputFixingParser:\")\n",
    "new_parser.invoke(bad_output)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PydanticOutputParser:\n",
      "Invalid json output: æ ¹æ®ç”¨æˆ·è¾“å…¥\"2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...\"ï¼Œæˆ‘ä»¬å¯ä»¥æå–å‡ºæ—¥æœŸä¿¡æ¯ï¼Œå¹¶æŒ‰ç…§ç»™å®šçš„JSON Schemaæ ¼å¼åŒ–è¾“å‡ºã€‚è¿™é‡Œæˆ‘ä»¬å‡è®¾æ‰€æœ‰æåˆ°çš„æ—¥æœŸéƒ½æ˜¯å…¬å…ƒï¼ˆADï¼‰çºªå¹´çš„ã€‚æå–çš„ä¿¡æ¯åŒ…æ‹¬å¹´ã€æœˆã€æ—¥ã€‚å› æ­¤ï¼Œå¯¹äºè¿™ä¸ªä¾‹å­ï¼Œè¾“å‡ºå°†æ˜¯ï¼š\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": å››,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "```\n",
      "\n",
      "è¿™ç¬¦åˆæä¾›çš„JSON Schemaè¦æ±‚ï¼Œå…¶ä¸­åŒ…å«äº†å¿…é¡»çš„`year`, `month`, `day`å­—æ®µä»¥åŠæŒ‡å®šäº†æ—¶ä»£çš„`era`å­—æ®µã€‚åœ¨è¿™ä¸ªåœºæ™¯ä¸‹ï¼Œâ€œeraâ€è¢«è®¾ç½®ä¸º\"AD\"è¡¨ç¤ºå…¬å…ƒçºªå¹´ã€‚\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "OutputFixingParser:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date(year=2023, month=4, day=6, era='AD')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T07:10:53.638968Z",
     "start_time": "2025-05-22T07:10:53.627727Z"
    }
   },
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"The sum of two numbers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiplication of two numbers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T07:11:03.872945Z",
     "start_time": "2025-05-22T07:10:58.076202Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "llm_with_tools = llm.bind_tools([add, multiply])\n",
    "\n",
    "query = \"3.5çš„4å€æ˜¯å¤šå°‘?\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "output = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(json.dumps(output.tool_calls, indent=4))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"multiply\",\n",
      "        \"args\": {\n",
      "            \"a\": 3.5,\n",
      "            \"b\": 4\n",
      "        },\n",
      "        \"id\": \"call_0_028ca877-9d59-4581-af32-72765449f185\",\n",
      "        \"type\": \"tool_call\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å›ä¼  Funtion Call çš„ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T07:11:41.264009Z",
     "start_time": "2025-05-22T07:11:37.074367Z"
    }
   },
   "source": [
    "messages.append(output)\n",
    "\n",
    "available_tools = {\"add\": add, \"multiply\": multiply}\n",
    "\n",
    "for tool_call in output.tool_calls:\n",
    "    selected_tool = available_tools[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "new_output = llm_with_tools.invoke(messages)\n",
    "for message in messages:\n",
    "    print(json.dumps(message.model_dump(), indent=4, ensure_ascii=False))\n",
    "print(new_output.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"content\": \"3.5çš„4å€æ˜¯å¤šå°‘?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"type\": \"human\",\n",
      "    \"name\": null,\n",
      "    \"id\": null,\n",
      "    \"example\": false\n",
      "}\n",
      "{\n",
      "    \"content\": \"\",\n",
      "    \"additional_kwargs\": {\n",
      "        \"tool_calls\": [\n",
      "            {\n",
      "                \"id\": \"call_0_028ca877-9d59-4581-af32-72765449f185\",\n",
      "                \"function\": {\n",
      "                    \"arguments\": \"{\\\"a\\\":3.5,\\\"b\\\":4}\",\n",
      "                    \"name\": \"multiply\"\n",
      "                },\n",
      "                \"type\": \"function\",\n",
      "                \"index\": 0\n",
      "            }\n",
      "        ],\n",
      "        \"refusal\": null\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "        \"token_usage\": {\n",
      "            \"completion_tokens\": 24,\n",
      "            \"prompt_tokens\": 250,\n",
      "            \"total_tokens\": 274,\n",
      "            \"completion_tokens_details\": null,\n",
      "            \"prompt_tokens_details\": {\n",
      "                \"audio_tokens\": null,\n",
      "                \"cached_tokens\": 0\n",
      "            },\n",
      "            \"prompt_cache_hit_tokens\": 0,\n",
      "            \"prompt_cache_miss_tokens\": 250\n",
      "        },\n",
      "        \"model_name\": \"deepseek-chat\",\n",
      "        \"system_fingerprint\": \"fp_8802369eaa_prod0425fp8\",\n",
      "        \"id\": \"ce5626e9-633b-42dc-aecf-9803389b2069\",\n",
      "        \"finish_reason\": \"tool_calls\",\n",
      "        \"logprobs\": null\n",
      "    },\n",
      "    \"type\": \"ai\",\n",
      "    \"name\": null,\n",
      "    \"id\": \"run--a8cb773e-c3fb-4515-b821-cfc7275e3001-0\",\n",
      "    \"example\": false,\n",
      "    \"tool_calls\": [\n",
      "        {\n",
      "            \"name\": \"multiply\",\n",
      "            \"args\": {\n",
      "                \"a\": 3.5,\n",
      "                \"b\": 4\n",
      "            },\n",
      "            \"id\": \"call_0_028ca877-9d59-4581-af32-72765449f185\",\n",
      "            \"type\": \"tool_call\"\n",
      "        }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "        \"input_tokens\": 250,\n",
      "        \"output_tokens\": 24,\n",
      "        \"total_tokens\": 274,\n",
      "        \"input_token_details\": {\n",
      "            \"cache_read\": 0\n",
      "        },\n",
      "        \"output_token_details\": {}\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"content\": \"14.0\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"type\": \"tool\",\n",
      "    \"name\": \"multiply\",\n",
      "    \"id\": null,\n",
      "    \"tool_call_id\": \"call_0_028ca877-9d59-4581-af32-72765449f185\",\n",
      "    \"artifact\": null,\n",
      "    \"status\": \"success\"\n",
      "}\n",
      "3.5çš„4å€æ˜¯14.0ã€‚\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 å°ç»“\n",
    "\n",
    "1. LangChain ç»Ÿä¸€å°è£…äº†å„ç§æ¨¡å‹çš„è°ƒç”¨æ¥å£ï¼ŒåŒ…æ‹¬è¡¥å…¨å‹å’Œå¯¹è¯å‹ä¸¤ç§\n",
    "2. LangChain æä¾›äº† PromptTemplate ç±»ï¼Œå¯ä»¥è‡ªå®šä¹‰å¸¦å˜é‡çš„æ¨¡æ¿\n",
    "3. LangChain æä¾›äº†ä¸€äº›åˆ—è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå°†å¤§æ¨¡å‹çš„è¾“å‡ºè§£ææˆç»“æ„åŒ–å¯¹è±¡\n",
    "4. LangChain æä¾›äº† Function Calling çš„å°è£…\n",
    "5. ä¸Šè¿°æ¨¡å‹å±äº LangChain ä¸­è¾ƒä¸ºå®ç”¨çš„éƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®è¿æ¥å°è£…\n",
    "\n",
    "<img src=\"./assets/data_connection.jpg\" style=\"margin-left: 0px\" width=1400px>\n",
    "\n",
    "### 2.1 æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-community pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T02:14:23.878143Z",
     "start_time": "2025-05-23T02:14:23.811686Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"./data/deepseek-v3-1-4.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0].page_content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3\n",
      "24.8\n",
      "23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3\n",
      "DeepSeek-V2.5\n",
      "Qwen2.5-72B-Inst\n",
      "Llama-3.1-405B-Inst\n",
      "GPT-4o-0513\n",
      "Claude-3.5-Sonnet-1022\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ–‡æ¡£å¤„ç†å™¨\n",
    "\n",
    "#### 2.2.1 TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T02:14:29.216045Z",
     "start_time": "2025-05-23T02:14:29.213882Z"
    }
   },
   "source": [
    "# !pip install --upgrade langchain-text-splitters"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-23T02:14:43.616176Z",
     "start_time": "2025-05-23T02:14:43.613620Z"
    }
   },
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content)\n",
    "    print('-------')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "-------\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "-------\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "-------\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "-------\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3\n",
      "24.8\n",
      "23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3\n",
      "DeepSeek-V2.5\n",
      "Qwen2.5-72B-Inst\n",
      "Llama-3.1-405B-Inst\n",
      "GPT-4o-0513\n",
      "-------\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3\n",
      "24.8\n",
      "23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3\n",
      "DeepSeek-V2.5\n",
      "Qwen2.5-72B-Inst\n",
      "Llama-3.1-405B-Inst\n",
      "GPT-4o-0513\n",
      "Claude-3.5-Sonnet-1022\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "-------\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "ç±»ä¼¼ LlamaIndexï¼ŒLangChain ä¹Ÿæä¾›äº†ä¸°å¯Œçš„ <code><a href=\"https://python.langchain.com/v0.2/docs/how_to/#document-loaders\">Document Loaders</a></code> å’Œ <code><a href=\"https://python.langchain.com/v0.2/docs/how_to/#text-splitters\">Text Splitters</a></code>ã€‚\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3ã€å‘é‡æ•°æ®åº“ä¸å‘é‡æ£€ç´¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dashscope\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:18:44.677001Z",
     "start_time": "2025-05-23T09:18:43.219315Z"
    }
   },
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyMuPDFLoader(\"./data/deepseek-v3-1-4.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:1]]\n",
    ")\n",
    "\n",
    "# çŒåº“\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v1\", dashscope_api_key=os.getenv(\"DASHSCOPE_API_KEY\")\n",
    ")\n",
    "index = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# æ£€ç´¢ top-5 ç»“æœ\n",
    "retriever = index.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "docs = retriever.invoke(\"deepseek v3æœ‰å¤šå°‘å‚æ•°\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"----\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3\n",
      "24.8\n",
      "23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3\n",
      "DeepSeek-V2.5\n",
      "Qwen2.5-72B-Inst\n",
      "Llama-3.1-405B-Inst\n",
      "GPT-4o-0513\n",
      "Claude-3.5-Sonnet-1022\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "----\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "----\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "----\n",
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "----\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ›´å¤šçš„ä¸‰æ–¹æ£€ç´¢ç»„ä»¶é“¾æ¥ï¼Œå‚è€ƒï¼šhttps://python.langchain.com/docs/integrations/vectorstores/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 å°ç»“\n",
    "\n",
    "1. æ–‡æ¡£å¤„ç†éƒ¨åˆ†ï¼Œå»ºè®®åœ¨å®é™…åº”ç”¨ä¸­è¯¦ç»†æµ‹è¯•åä½¿ç”¨\n",
    "2. ä¸å‘é‡æ•°æ®åº“çš„é“¾æ¥éƒ¨åˆ†æœ¬è´¨æ˜¯æ¥å£å°è£…ï¼Œå‘é‡æ•°æ®åº“éœ€è¦è‡ªå·±é€‰å‹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chain å’Œ LangChain Expression Language (LCEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Expression Languageï¼ˆLCELï¼‰æ˜¯ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œå¯è½»æ¾ç»„åˆä¸åŒçš„è°ƒç”¨é¡ºåºæ„æˆ Chainã€‚LCEL è‡ªåˆ›ç«‹ä¹‹åˆå°±è¢«è®¾è®¡ä¸ºèƒ½å¤Ÿæ”¯æŒå°†åŸå‹æŠ•å…¥ç”Ÿäº§ç¯å¢ƒï¼Œ**æ— éœ€ä»£ç æ›´æ”¹**ï¼Œä»æœ€ç®€å•çš„â€œæç¤º+LLMâ€é“¾åˆ°æœ€å¤æ‚çš„é“¾ï¼ˆå·²æœ‰ç”¨æˆ·æˆåŠŸåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒåŒ…å«æ•°ç™¾ä¸ªæ­¥éª¤çš„ LCEL Chainï¼‰ã€‚\n",
    "\n",
    "LCEL çš„ä¸€äº›äº®ç‚¹åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æµæ”¯æŒ**ï¼šä½¿ç”¨ LCEL æ„å»º Chain æ—¶ï¼Œä½ å¯ä»¥è·å¾—æœ€ä½³çš„é¦–ä¸ªä»¤ç‰Œæ—¶é—´ï¼ˆå³ä»è¾“å‡ºå¼€å§‹åˆ°é¦–æ‰¹è¾“å‡ºç”Ÿæˆçš„æ—¶é—´ï¼‰ã€‚å¯¹äºæŸäº› Chainï¼Œè¿™æ„å‘³ç€å¯ä»¥ç›´æ¥ä» LLM æµå¼ä¼ è¾“ä»¤ç‰Œåˆ°æµè¾“å‡ºè§£æå™¨ï¼Œä»è€Œä»¥ä¸ LLM æä¾›å•†è¾“å‡ºåŸå§‹ä»¤ç‰Œç›¸åŒçš„é€Ÿç‡è·å¾—è§£æåçš„ã€å¢é‡çš„è¾“å‡ºã€‚\n",
    "\n",
    "2. **å¼‚æ­¥æ”¯æŒ**ï¼šä»»ä½•ä½¿ç”¨ LCEL æ„å»ºçš„é“¾æ¡éƒ½å¯ä»¥é€šè¿‡åŒæ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ Jupyter ç¬”è®°æœ¬ä¸­è¿›è¡ŒåŸå‹è®¾è®¡æ—¶ï¼‰å’Œå¼‚æ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangServe æœåŠ¡å™¨ä¸­ï¼‰è°ƒç”¨ã€‚è¿™ä½¿å¾—ç›¸åŒçš„ä»£ç å¯ç”¨äºåŸå‹è®¾è®¡å’Œç”Ÿäº§ç¯å¢ƒï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿåœ¨åŒä¸€æœåŠ¡å™¨ä¸­å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚ã€‚\n",
    "\n",
    "3. **ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œ**ï¼šå½“ä½ çš„ LCEL é“¾æ¡æœ‰å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„æ­¥éª¤æ—¶ï¼ˆä¾‹å¦‚ï¼Œä»å¤šä¸ªæ£€ç´¢å™¨ä¸­è·å–æ–‡æ¡£ï¼‰ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œæ— è®ºæ˜¯åœ¨åŒæ­¥è¿˜æ˜¯å¼‚æ­¥æ¥å£ä¸­ï¼Œä»¥å®ç°æœ€å°çš„å»¶è¿Ÿã€‚\n",
    "\n",
    "4. **é‡è¯•å’Œå›é€€**ï¼šä¸º LCEL é“¾çš„ä»»ä½•éƒ¨åˆ†é…ç½®é‡è¯•å’Œå›é€€ã€‚è¿™æ˜¯ä½¿é“¾åœ¨è§„æ¨¡ä¸Šæ›´å¯é çš„ç»ä½³æ–¹å¼ã€‚ç›®å‰æˆ‘ä»¬æ­£åœ¨æ·»åŠ é‡è¯•/å›é€€çš„æµåª’ä½“æ”¯æŒï¼Œå› æ­¤ä½ å¯ä»¥åœ¨ä¸å¢åŠ ä»»ä½•å»¶è¿Ÿæˆæœ¬çš„æƒ…å†µä¸‹è·å¾—å¢åŠ çš„å¯é æ€§ã€‚\n",
    "\n",
    "5. **è®¿é—®ä¸­é—´ç»“æœ**ï¼šå¯¹äºæ›´å¤æ‚çš„é“¾æ¡ï¼Œè®¿é—®åœ¨æœ€ç»ˆè¾“å‡ºäº§ç”Ÿä¹‹å‰çš„ä¸­é—´æ­¥éª¤çš„ç»“æœé€šå¸¸éå¸¸æœ‰ç”¨ã€‚è¿™å¯ä»¥ç”¨äºè®©æœ€ç»ˆç”¨æˆ·çŸ¥é“æ­£åœ¨å‘ç”Ÿä¸€äº›äº‹æƒ…ï¼Œç”šè‡³ä»…ç”¨äºè°ƒè¯•é“¾æ¡ã€‚ä½ å¯ä»¥æµå¼ä¼ è¾“ä¸­é—´ç»“æœï¼Œå¹¶ä¸”åœ¨æ¯ä¸ª LangServe æœåŠ¡å™¨ä¸Šéƒ½å¯ç”¨ã€‚\n",
    "\n",
    "6. **è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼**ï¼šè¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ä¸ºæ¯ä¸ª LCEL é“¾æä¾›äº†ä»é“¾çš„ç»“æ„æ¨æ–­å‡ºçš„ Pydantic å’Œ JSONSchema æ¨¡å¼ã€‚è¿™å¯ä»¥ç”¨äºè¾“å…¥å’Œè¾“å‡ºçš„éªŒè¯ï¼Œæ˜¯ LangServe çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚\n",
    "\n",
    "7. **æ— ç¼ LangSmith è·Ÿè¸ªé›†æˆ**ï¼šéšç€é“¾æ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œç†è§£æ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚é€šè¿‡ LCELï¼Œæ‰€æœ‰æ­¥éª¤éƒ½è‡ªåŠ¨è®°å½•åˆ° LangSmithï¼Œä»¥å®ç°æœ€å¤§çš„å¯è§‚å¯Ÿæ€§å’Œå¯è°ƒè¯•æ€§ã€‚\n",
    "\n",
    "8. **æ— ç¼ LangServe éƒ¨ç½²é›†æˆ**ï¼šä»»ä½•ä½¿ç”¨ LCEL åˆ›å»ºçš„é“¾éƒ½å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ LangServe è¿›è¡Œéƒ¨ç½²ã€‚\n",
    "\n",
    "åŸæ–‡ï¼šhttps://python.langchain.com/docs/expression_language/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pipeline å¼è°ƒç”¨ PromptTemplate, LLM å’Œ OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:23:17.521450Z",
     "start_time": "2025-05-23T09:23:17.439305Z"
    }
   },
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "import json\n",
    "from langchain.chat_models import init_chat_model"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-23T09:32:35.215997Z",
     "start_time": "2025-05-23T09:32:28.240792Z"
    }
   },
   "source": [
    "# è¾“å‡ºç»“æ„\n",
    "class SortEnum(str, Enum):\n",
    "    data = 'data'\n",
    "    price = 'price'\n",
    "\n",
    "\n",
    "class OrderingEnum(str, Enum):\n",
    "    ascend = 'ascend'\n",
    "    descend = 'descend'\n",
    "\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"æµé‡åŒ…åç§°\", default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"ä»·æ ¼ä¸‹é™\", default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"ä»·æ ¼ä¸Šé™\", default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"æµé‡ä¸‹é™\", default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"æµé‡ä¸Šé™\", default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"æŒ‰ä»·æ ¼æˆ–æµé‡æ’åº\", default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(\n",
    "        description=\"å‡åºæˆ–é™åºæ’åˆ—\", default=None)\n",
    "\n",
    "\n",
    "# Prompt æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ä½ æ˜¯ä¸€ä¸ªè¯­ä¹‰è§£æå™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è¾“å…¥è§£ææˆJSONè¡¨ç¤ºã€‚ä¸è¦å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# æ¨¡å‹\n",
    "llm = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Semantics)\n",
    "\n",
    "# LCEL è¡¨è¾¾å¼\n",
    "runnable = (\n",
    "        {\"text\": RunnablePassthrough()} | prompt | structured_llm\n",
    ")\n",
    "\n",
    "# ç›´æ¥è¿è¡Œ\n",
    "ret = runnable.invoke(\"ä¸è¶…è¿‡100å…ƒçš„æµé‡å¤§çš„å¥—é¤æœ‰å“ªäº›\")\n",
    "print(\n",
    "    json.dumps(\n",
    "        ret.model_dump(),\n",
    "        indent=4,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": null,\n",
      "    \"price_lower\": null,\n",
      "    \"price_upper\": 100,\n",
      "    \"data_lower\": null,\n",
      "    \"data_upper\": null,\n",
      "    \"sort_by\": \"data\",\n",
      "    \"ordering\": \"descend\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>ä½¿ç”¨ LCEL çš„ä»·å€¼ï¼Œä¹Ÿå°±æ˜¯ LangChain çš„æ ¸å¿ƒä»·å€¼ã€‚</b> <br />\n",
    "å®˜æ–¹ä»ä¸åŒè§’åº¦ç»™å‡ºäº†ä¸¾ä¾‹è¯´æ˜ï¼š<a href=\"https://python.langchain.com/docs/concepts/lcel/\">https://python.langchain.com/docs/concepts/lcel/</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ç”¨ LCEL å®ç° RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:35:24.267854Z",
     "start_time": "2025-05-23T09:35:23.849928Z"
    }
   },
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyMuPDFLoader(\"./data/deepseek-v3-1-4.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:1]]\n",
    ")\n",
    "\n",
    "# çŒåº“\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v1\", dashscope_api_key=os.getenv(\"DASHSCOPE_API_KEY\")\n",
    ")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# æ£€ç´¢ top-5 ç»“æœ\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:35:28.220997Z",
     "start_time": "2025-05-23T09:35:28.019205Z"
    }
   },
   "source": [
    "docs = retriever.invoke(\"deepseek v3æœ‰å¤šå°‘å‚æ•°\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"----\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3\n",
      "24.8\n",
      "23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3\n",
      "DeepSeek-V2.5\n",
      "Qwen2.5-72B-Inst\n",
      "Llama-3.1-405B-Inst\n",
      "GPT-4o-0513\n",
      "Claude-3.5-Sonnet-1022\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "----\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "----\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "----\n",
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "----\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T09:35:55.359847Z",
     "start_time": "2025-05-23T09:35:47.849117Z"
    }
   },
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Promptæ¨¡æ¿\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "        {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"deepseek V3æœ‰å¤šå°‘å‚æ•°\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepSeek-V3 æ˜¯ä¸€ä¸ªæ··åˆä¸“å®¶ï¼ˆMoEï¼‰è¯­è¨€æ¨¡å‹ï¼Œæ€»å‚æ•°ä¸º **6710äº¿ï¼ˆ671Bï¼‰**ï¼Œå…¶ä¸­æ¯ä¸ª token æ¿€æ´»çš„å‚æ•°ä¸º **370äº¿ï¼ˆ37Bï¼‰**ã€‚'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ç”¨ LCEL å®ç°æ¨¡å‹åˆ‡æ¢ï¼ˆå·¥å‚æ¨¡å¼ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨ä¸ºä½ æä¾›ä¿¡æ¯å’Œæ”¯æŒã€‚æˆ‘å¯ä»¥å›ç­”é—®é¢˜ã€æä¾›å»ºè®®ã€å¸®åŠ©å­¦ä¹ æ–°çŸ¥è¯†ï¼Œæˆ–è€…ååŠ©å¤„ç†å„ç§äº‹åŠ¡ã€‚å¦‚æœä½ æœ‰ä»»ä½•ç‰¹å®šçš„é—®é¢˜æˆ–éœ€è¦å¸®åŠ©çš„åœ°æ–¹ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.utils import ConfigurableField\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "# æ¨¡å‹1\n",
    "ds_model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "# æ¨¡å‹2\n",
    "gpt_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "# é€šè¿‡ configurable_alternatives æŒ‰æŒ‡å®šå­—æ®µé€‰æ‹©æ¨¡å‹\n",
    "model = gpt_model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key=\"gpt\",\n",
    "    deepseek=ds_model,\n",
    "    # claude=claude_model,\n",
    ")\n",
    "\n",
    "# Prompt æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LCEL\n",
    "chain = (\n",
    "        {\"query\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "# è¿è¡Œæ—¶æŒ‡å®šæ¨¡å‹ \"gpt\" or \"deepseek\"\n",
    "ret = chain.with_config(configurable={\"llm\": \"gpt\"}).invoke(\"è¯·è‡ªæˆ‘ä»‹ç»\")\n",
    "\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰©å±•é˜…è¯»ï¼šä»€ä¹ˆæ˜¯[**å·¥å‚æ¨¡å¼**](https://www.runoob.com/design-pattern/factory-pattern.html)ï¼›[**è®¾è®¡æ¨¡å¼**](https://www.runoob.com/design-pattern/design-pattern-intro.html)æ¦‚è§ˆã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>ä»æ¨¡å—é—´è§£ä¾èµ–è§’åº¦ï¼ŒLCELçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 é€šè¿‡ LCELï¼Œè¿˜å¯ä»¥å®ç°\n",
    "\n",
    "1. é…ç½®è¿è¡Œæ—¶å˜é‡ï¼šhttps://python.langchain.com/docs/how_to/configure/\n",
    "2. æ•…éšœå›é€€ï¼šhttps://python.langchain.com/docs/how_to/fallbacks/\n",
    "3. å¹¶è¡Œè°ƒç”¨ï¼šhttps://python.langchain.com/docs/how_to/parallel/\n",
    "4. é€»è¾‘åˆ†æ”¯ï¼šhttps://python.langchain.com/docs/how_to/routing/\n",
    "5. åŠ¨æ€åˆ›å»º Chain: https://python.langchain.com/docs/how_to/dynamic_chain/\n",
    "\n",
    "æ›´å¤šä¾‹å­ï¼šhttps://python.langchain.com/docs/how_to/lcel_cheatsheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å·¥ä½œæµæ¡†æ¶ï¼šLangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph æ˜¯ä¸ºæ™ºèƒ½ä½“å’Œå·¥ä½œæµè®¾è®¡ä¸€å¥—åº•å±‚ç¼–æ’æ¡†æ¶\n",
    "\n",
    "å®‰è£…ï¼š`pip install langgraph`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 å‡ ä¸ªåŸºç¡€æ¦‚å¿µ\n",
    "\n",
    "- `StateGraph` å°†å·¥ä½œæµå®šä¹‰æˆä¸€ä¸ªçŠ¶æ€æœº\n",
    "- `Node` å·¥ä½œæµä¸­çš„èŠ‚ç‚¹\n",
    "- `Edge` è¾¹ï¼Œå®šä¹‰èŠ‚ç‚¹ä¹‹é—´çš„è·³è½¬\n",
    "- `State` çŠ¶æ€ï¼Œéšç€å·¥ä½œæµçš„è¿›è¡Œå¯ä»¥è¢«æ›´æ–°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 å…ˆä»ä¸€ä¸ªæœ€åŸºç¡€çš„ä¾‹å­å¼€å§‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®ç°ä¸€ä¸ªå¸¦ä¸Šä¸‹æ–‡çš„ Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# å®šä¹‰ State\n",
    "class State(TypedDict):\n",
    "    # çŠ¶æ€å˜é‡ messages ç±»å‹æ˜¯ listï¼Œæ›´æ–°æ–¹å¼æ˜¯ add_messages\n",
    "    # add_messages æ˜¯å†…ç½®çš„ä¸€ä¸ªæ–¹æ³•ï¼Œå°†æ–°çš„æ¶ˆæ¯åˆ—è¡¨è¿½åŠ åœ¨åŸåˆ—è¡¨åé¢\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# åˆ›å»º Graph\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªæ‰§è¡ŒèŠ‚ç‚¹\n",
    "# è¾“å…¥æ˜¯ Stateï¼Œè¾“å‡ºæ˜¯ç³»ç»Ÿå›å¤\n",
    "def chatbot(state: State):\n",
    "    # è°ƒç”¨å¤§æ¨¡å‹ï¼Œå¹¶è¿”å›æ¶ˆæ¯ï¼ˆåˆ—è¡¨ï¼‰\n",
    "    # è¿”å›å€¼ä¼šè§¦å‘çŠ¶æ€æ›´æ–° add_messages\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# å¯è§†åŒ–å±•ç¤ºè¿™ä¸ªå·¥ä½œæµ\n",
    "try:\n",
    "    display(Image(data=graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage\n",
    "\n",
    "\n",
    "def stream_graph_updates(user_input: str):\n",
    "    # å‘ graph ä¼ å…¥ä¸€æ¡æ¶ˆæ¯ï¼ˆè§¦å‘çŠ¶æ€æ›´æ–° add_messagesï¼‰\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value and isinstance(value[\"messages\"][-1], AIMessage):\n",
    "                print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "def run():\n",
    "    # æ‰§è¡Œè¿™ä¸ªå·¥ä½œæµ\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.strip() == \"\":\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  ä½ å¥½\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  ä»Šå¤©å¿ƒæƒ…å¥½å—\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: è°¢è°¢ä½ çš„å…³å¿ƒï¼ä½œä¸ºä¸€ä¸ªAIï¼Œæˆ‘æ²¡æœ‰æƒ…æ„Ÿï¼Œä½†æˆ‘å¾ˆé«˜å…´èƒ½å¸®åŠ©ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ä¸ºä½ åšçš„å—ï¼Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  \n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 åŠ å…¥ RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyMuPDFLoader(\"./data/deepseek-v3-1-4.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:2]]\n",
    ")\n",
    "\n",
    "# çŒåº“\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# æ£€ç´¢ top-3 ç»“æœ\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Promptæ¨¡æ¿\n",
    "template = \"\"\"è¯·æ ¹æ®å¯¹è¯å†å²å’Œä¸‹é¢æä¾›çš„ä¿¡æ¯å›ç­”ä¸Šé¢ç”¨æˆ·æå‡ºçš„é—®é¢˜:\n",
    "{input}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(template),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(state: State):\n",
    "    user_query = \"\"\n",
    "    if len(state[\"messages\"]) >= 1:\n",
    "        # è·å–æœ€åä¸€è½®ç”¨æˆ·è¾“å…¥\n",
    "        user_query = state[\"messages\"][-1]\n",
    "    else:\n",
    "        return {\"messages\": []}\n",
    "    # æ£€ç´¢\n",
    "    docs = retriever.invoke(str(user_query))\n",
    "    # å¡« prompt æ¨¡æ¿\n",
    "    messages = prompt.invoke(\"\\n\".join([doc.page_content for doc in docs])).messages\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieval\", retrieval)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"retrieval\")\n",
    "graph_builder.add_edge(\"retrieval\", \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  llama2æœ‰å¤šå°‘å‚æ•°\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Llama 2æœ‰7Bã€13Bå’Œ70Bå‚æ•°çš„ç‰ˆæœ¬ã€‚\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  \n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 åŠ å…¥åˆ†æ”¯ï¼šå¦‚æœæ‰¾ä¸åˆ°ç­”æ¡ˆåˆ™è½¬äººå·¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from typing import Literal\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "\n",
    "def verify(state: State) -> Literal[\"chatbot\", \"ask_human\"]:\n",
    "    message = HumanMessage(\"è¯·æ ¹æ®å¯¹è¯å†å²å’Œä¸Šé¢æä¾›çš„ä¿¡æ¯åˆ¤æ–­ï¼Œå·²çŸ¥çš„ä¿¡æ¯æ˜¯å¦èƒ½å¤Ÿå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ç›´æ¥è¾“å‡ºä½ çš„åˆ¤æ–­'Y'æˆ–'N'\")\n",
    "    ret = llm.invoke(state[\"messages\"] + [message])\n",
    "    if 'Y' in ret.content:\n",
    "        return \"chatbot\"\n",
    "    else:\n",
    "        return \"ask_human\"\n",
    "\n",
    "\n",
    "def ask_human(state: State):\n",
    "    user_query = state[\"messages\"][-2].content\n",
    "    human_response = interrupt(\n",
    "        {\n",
    "            \"question\": user_query\n",
    "        }\n",
    "    )\n",
    "    # Update the state with the human's input or route the graph based on the input.\n",
    "    return {\n",
    "        \"messages\": [AIMessage(human_response)]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()  # ç”¨äºæŒä¹…åŒ–å­˜å‚¨ state (è¿™é‡Œä»¥å†…å­˜æ¨¡æ‹Ÿï¼‰\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"retrieval\", retrieval)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"ask_human\", ask_human)\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieval\")\n",
    "graph_builder.add_conditional_edges(\"retrieval\", verify)\n",
    "graph_builder.add_edge(\"ask_human\", END)\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# ä¸­é€”ä¼šè¢«è½¬äººå·¥æ‰“æ–­ï¼Œæ‰€ä»¥éœ€è¦ checkpointer å­˜å‚¨çŠ¶æ€\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage\n",
    "\n",
    "# å½“ä½¿ç”¨ checkpointer æ—¶ï¼Œéœ€è¦é…ç½®è¯»å– state çš„ thread_id\n",
    "# å¯ä»¥ç±»æ¯” OpenAI Assistants API ç†è§£ï¼Œæˆ–è€…æƒ³è±¡ Redis ä¸­çš„ key \n",
    "thread_config = {\"configurable\": {\"thread_id\": \"my_thread_id\"}}\n",
    "\n",
    "\n",
    "def stream_graph_updates(user_input: str):\n",
    "    # å‘ graph ä¼ å…¥ä¸€æ¡æ¶ˆæ¯ï¼ˆè§¦å‘çŠ¶æ€æ›´æ–° add_messagesï¼‰\n",
    "    for event in graph.stream(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "            thread_config\n",
    "    ):\n",
    "        for value in event.values():\n",
    "            if isinstance(value, tuple):\n",
    "                return value[0].value[\"question\"]\n",
    "            elif \"messages\" in value and isinstance(value[\"messages\"][-1], AIMessage):\n",
    "                print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def resume_graph_updates(human_input: str):\n",
    "    for event in graph.stream(\n",
    "            Command(resume=human_input), thread_config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        for value in event.values():\n",
    "            if \"messages\" in value and isinstance(value[\"messages\"][-1], AIMessage):\n",
    "                print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "def run():\n",
    "    # æ‰§è¡Œè¿™ä¸ªå·¥ä½œæµ\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.strip() == \"\":\n",
    "            break\n",
    "        question = stream_graph_updates(user_input)\n",
    "        if question:\n",
    "            human_answer = input(\"Ask Human: \" + question + \"\\nHuman: \")\n",
    "            resume_graph_updates(human_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# å¯è§†åŒ–å±•ç¤ºè¿™ä¸ªå·¥ä½œæµ\n",
    "try:\n",
    "    display(Image(data=graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
