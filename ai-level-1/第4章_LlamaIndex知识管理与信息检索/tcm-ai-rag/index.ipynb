{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中医临床诊疗智能助手 \n",
    "\n",
    "1. RAG 检索增强生成方案\n",
    "2. Fine tuning 微调方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 这节课会带给你\n",
    "\n",
    "1. 如何用你的垂域数据补充 LLM 的能力\n",
    "2. 如何构建你的垂域（向量）知识库\n",
    "3. 搭建一套完整 RAG 系统需要哪些模块\n",
    "4. 搭建 RAG 系统时更多的有用技巧\n",
    "5. 如何提升 RAG 检索的效果及优化实践\n",
    "6. 生成级部署 RAG 系统方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**学习目标：**\n",
    "1. RAG 技术概述\n",
    "2. RAG WorkFlow 及 RAG 工程化\n",
    "3. 基于 LlamaIndex 快速构建 RAG 项目\n",
    "4. 使用 LlamaIndex 存储和读取 Embedding 向量\n",
    "5. 追踪哪些文档片段被用于检索增强生成\n",
    "6. 深度剖析 RAG 检索底层实现细节\n",
    "7. 自定义 RAG Prompt Template\n",
    "8. RAG 项目企业级生产部署最佳实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、 RAG 技术概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 大模型目前固有的局限性\n",
    "\n",
    "**大语言模型（LLM）是概率生成系统**\n",
    "\n",
    "- **知识时效性**：模型知识截止于训练数据时间点（**联网搜索**）\n",
    "- **推理局限性**：本质是概率预测而非逻辑运算，复杂数学推理易出错（**DeepSeek-R1的架构有所不同**）\n",
    "- **专业领域盲区**：缺乏垂直领域知识\n",
    "- **幻觉现象**：可能生成看似合理但实际错误的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 什么是 RAG？\n",
    "\n",
    "RAG（Retrieval Augmented Generation）顾名思义，通过**检索**的方法来增强**生成模型**的能力。\n",
    "\n",
    "<img src=\"./assets/rag.jpg\" style=\"margin-left: 0px\" width=1024px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、RAG 工程化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 RAG系统的基本搭建流程\n",
    "\n",
    "搭建过程：\n",
    "\n",
    "1. 文档加载，并按一定条件**切割**成片段\n",
    "2. 将切割的文本片段灌入**检索引擎**\n",
    "3. 封装**检索接口**\n",
    "4. 构建**调用流程**：Query -> 检索 -> Prompt -> LLM -> 回复"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 构建索引\n",
    "\n",
    "<img src=\"./assets/rag-1.png\" style=\"margin-left: 0px\" width=1024px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 检索和生成\n",
    "\n",
    "<img src=\"./assets/rag-2.png\" style=\"margin-left: 0px\" width=1024px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、项目环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 使用 conda 创建项目环境\n",
    "\n",
    "```sh\n",
    "# 创建环境\n",
    "conda create -n tcm-ai-rag python=3.10\n",
    "\n",
    "# 激活环境\n",
    "conda activate tcm-ai-rag\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 安装项目所需依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 LlamaIndex 相关包\n",
    "# !pip install llama-index\n",
    "# !pip install llama-index-embeddings-huggingface\n",
    "# !pip install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 CUDA 版本 Pytorch\n",
    "# !pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、模型下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 modelscope\n",
    "# !pip install modelscope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 下载 Embedding 模型权重\n",
    "\n",
    "使用BAAI开源的中文bge模型作为embedding模型，使用modlescope提供的SDK将模型权重下载到本地服务器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 modelscope 提供的 sdk 进行模型下载\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "# model_id 模型的id\n",
    "# cache_dir 缓存到本地的路径\n",
    "model_dir = snapshot_download(model_id=\"BAAI/bge-base-zh-v1.5\", cache_dir=\"/home/kevin/projects/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 下载 LLM 大模型权重\n",
    "\n",
    "使用阿里开源的通义千问大模型，使用modelscope提供的SDK将模型权重下载到服务器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 modelscope 提供的 sdk 进行模型下载\n",
    "from modelscope import snapshot_download\n",
    "\n",
    "# model_id 模型的id\n",
    "# cache_dir 缓存到本地的路径\n",
    "model_dir = snapshot_download(model_id=\"Qwen/Qwen1.5-7B-Chat\", cache_dir=\"/home/kevin/projects/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、构建中医临床诊疗术语证候问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 语料准备\n",
    "\n",
    "本应用使用的文档是由国家卫生健康委员和国家中医药管理局发布的中医临床诊疗术语：\n",
    "\n",
    "- 《中医临床诊疗术语第1部分：疾病》（修订版）.docx\n",
    "- 《中医临床诊疗术语第2部分：证候》（修订版）.docx\n",
    "- 《中医临床诊疗术语第3部分：治法》（修订版）.docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <p><strong>需要对语料进行数据预处理，如去除噪声数据、数据格式化等</strong></p>\n",
    "    <p>本文件中具有类目属性的术语一般不适用于临床诊断。</p>\n",
    "    <p>注：类目属性的术语是指定义中有“泛指……一类证候”表述方式的术语。</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h3>部分内容展示：</h3>\n",
    "\n",
    "<p style=\"border:1px solid red;background:pink;padding:10px\">\n",
    "<span style=\"color:blue\"><i>这种噪声数据就需要删除！</i></span><br>\n",
    "4.1.1.2.1<br>\n",
    "    气机阻滞证  syndrome/pattern of obstructed qi movement<br>\n",
    "    <strong>泛指因各种原因导致气机不畅，或气郁而不散，阻滞脏腑、经络、官窍等所引起的一类证候。</strong><br>\n",
    "</p>\n",
    "\n",
    "4.1.1.2.1.1<br>\n",
    "    **气机郁滞证  syndrome/pattern of qi activity stagnation**<br>\n",
    "    因气机郁结，阻滞经络或脏腑官窍所致。临床以头颈肩背或胸胁脘腹等处闷胀，或攻窜作痛，常随紧张、抑郁等情绪缓解，或得太息、嗳气、肠鸣、矢气而减轻，脉弦，可伴见大便时秘或泻，小便不利，耳鸣、耳聋，嘶哑、呃逆等为特征的证候。<br>\n",
    "\n",
    "4.1.1.2.1.2<br>\n",
    "    **气滞耳窍证  syndrome/pattern of qi stagnation in the ears**<br>\n",
    "    因肝气郁结，气机不利，气滞耳窍所致。临床以突然耳窍失聪，或耳内堵塞，耳鸣，眩晕，脉弦，伴见胸胁胀闷，情绪抑郁等为特征的证候。<br>\n",
    "\n",
    "4.1.1.2.1.3<br>\n",
    "    **气滞声带证  syndrome/pattern of qi stagnation in the vocal fold**<br>\n",
    "    因气机阻滞，痹阻声带所致。临床以声音不扬、嘶哑，言语费劲或磕巴，脉弦，可伴见咽喉不适，胸闷、胁胀等为特征的证候。<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 删除文件中的英文和/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，已生成新文件：./data/demo-2-1.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_english(input_file, output_file):\n",
    "    \"\"\"\n",
    "    去除文件中所有英文字符并生成新文件\n",
    "    :param input_file: 输入文件路径\n",
    "    :param output_file: 输出文件路径\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f_in:\n",
    "            content = f_in.read()\n",
    "\n",
    "        # 使用正则表达式移除所有英文字母\n",
    "        filtered_content = re.sub('[A-Za-z/]', '', content)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "            f_out.write(filtered_content)\n",
    "            \n",
    "        print(f\"处理完成，已生成新文件：{output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理出错：{str(e)}\")\n",
    "\n",
    "# 使用示例\n",
    "remove_english('./data/demo-2.txt', './data/demo-2-1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 基于 LlamaIndex 来快速构建知识库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 导入所需的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from llama_index.core import PromptTemplate, Settings, SimpleDirectoryReader, VectorStoreIndex, load_index_from_storage, StorageContext, QueryBundle\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 定义日志配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 定义 System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant.\"\"\"\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 使用 llama_index_llms_huggingface 调用本地大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = True,  # 启用嵌套量化，在第一轮量化之后会进行第二轮量化，为每个参数额外节省 0.4 比特\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16, # 更改量化模型的计算数据类型来加速训练\n",
    ")\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    context_window = 4096,\n",
    "    max_new_tokens = 2048,\n",
    "    generate_kwargs = {\"temperature\": 0.0, \"do_sample\": False},\n",
    "    query_wrapper_prompt = query_wrapper_prompt,\n",
    "    tokenizer_name = \"/home/kevin/projects/models/Qwen/Qwen1.5-7B-Chat\",\n",
    "    model_name = \"/home/kevin/projects/models/Qwen/Qwen1.5-7B-Chat\",\n",
    "    device_map = \"auto\", #\"auto\",\"balanced\",\"balanced_low_0\",\"sequential\"\n",
    "    model_kwargs = {\n",
    "        \"trust_remote_code\":True,\n",
    "        \"quantization_config\": quantization_config\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<p><strong>注意：为了输出的可复现性</strong></p>\n",
    "<ul>\n",
    "    <li>将大模型的temperature设置为0，do_sample设置为False，所以两次得到的输出基本相同；</li>\n",
    "    <li>如果将temperature设置为大于0的小数，do_sample设置为True，大模型每次的输出可能都是不一样的。</li>\n",
    "    <li>另外，如果你在实验时获得的输出与文中的输出不一致，这也是正常的，这与多个因素有关。</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.5 使用 llama_index_embeddings_huggingface 调用本地 embedding 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"/home/kevin/projects/models/BAAI/bge-base-zh-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.6 读取文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./data\", required_exts=[\".txt\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.7 对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, transformations=[SentenceSplitter(chunk_size=256)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceSplitter 参数详细设置：\n",
    "\n",
    "预设会以 1024 个 token 为界切割片段, 每个片段的开头重叠上一个片段的 200 个 token 的内容。\n",
    "\n",
    "```properties\n",
    "chunk_size = 1024,    # 切片 token 数限制\n",
    "chunk_overlap = 200,  # 切片开头与前一片段尾端的重复 token 数\n",
    "paragraph_separator = '\\n\\n\\n', # 段落的分界\n",
    "secondary_chunking_regex = '[^,.;。？！]+[,.;。？！]?' # 单一句子的样式\n",
    "separator = ' ', # 最小切割的分界字元\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.8 构建查询引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming 流式输出\n",
    "# similarity_top_k 检索结果的数量\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.9 生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"不耐疲劳，口燥、咽干可能是哪些证候？\")\n",
    "response.print_response_stream()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从提供的信息来看，不耐疲劳、口燥、咽干等症状可能与以下几种证候相关：\n",
    "\n",
    "1. **津液不足证 (Syndrome of Fluid and Humor Insufficiency)**：这种证候的特点是口眼喉鼻及皮肤等部位干燥，大便干结，小便短少，舌质偏红而干，脉细数。这些症状与不耐疲劳、口燥、咽干相符。\n",
    "\n",
    "2. **津液亏涸证 (Syndrome of Fluid and Humor Scantiness)**：这种证候表现为口干、唇裂，鼻燥无涕，皮肤干瘪，目陷、螺瘪，甚则肌肤甲错，舌质红而少津，舌中裂，脉细或数。这些症状也包括口燥和咽干。\n",
    "\n",
    "3. **燥邪犯肺证 (Syndrome of Pathogenic Dryness Invading the Lung)**：这种证候的特点是干咳、少痰或无痰，痰黏不易咳出，唇鼻咽喉干燥，声音嘶哑，口渴，咳甚则胸痛，或痰中血丝，舌尖红，舌苔薄黄、少津，脉细或数。其中唇鼻咽喉干燥的症状与口燥、咽干相符。\n",
    "\n",
    "4. **燥干清窍证 (Syndrome of Dryness Harassing the Upper Orifices)**：这种证候的特点是口鼻、咽喉干燥，两眼干涩，少泪、少涕、少津、甚则衄血，舌质瘦小、舌苔干而少津，脉细。这些症状也包括口燥、咽干。\n",
    "\n",
    "综上所述，不耐疲劳、口燥、咽干等症状可能与津液不足证、津液亏涸证、燥邪犯肺证以及燥干清窍证相关。具体诊断需要结合其他临床表现和舌脉象综合判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、使用LlamaIndex存储和读取embedding向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 上面面临的问题\n",
    "\n",
    "- 使用llama-index-llms-huggingface构建本地大模型时，会花费相当一部分时间\n",
    "\n",
    "- 在对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引时，会花费大量的时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 向量存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将embedding向量和向量索引存储到文件中\n",
    "# ./doc_emb 是存储路径\n",
    "index.storage_context.persist(persist_dir='./doc_emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找到刚才定义的persist_dir所在的路径，可以发现该路径下有以下几个文件：\n",
    "\n",
    "- **default_vector_store.json**：用于存储embedding向量\n",
    "- **docstore.json**：用于存储文档切分出来的片段\n",
    "- graph_store.json：用于存储知识图数据\n",
    "- image__vector_store.json：用于存储图像数据\n",
    "- **index_store.json**：用于存储向量索引\n",
    "\n",
    "在上述代码中，我们只用到了纯文本文档，所以生成出来的`graph_store.json`和`image__vector_store.json`中没有数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 从向量数据库检索\n",
    "\n",
    "将embedding向量和向量索引存储到文件中后，我们就不需要重复地执行对文档进行切分，将切分后的片段转化为embedding向量，构建向量索引的操作了。\n",
    "\n",
    "以下代码演示了如何使用LlamaIndex读取结构化文件中的embedding向量和向量索引数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从存储文件中读取embedding向量和向量索引\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./doc_emb\")\n",
    "\n",
    "# 根据存储的embedding向量和向量索引重新构建检索索引\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "# 构建查询引擎\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=5)\n",
    "\n",
    "# 查询获得答案\n",
    "response = query_engine.query(\"不耐疲劳，口燥、咽干可能是哪些证候？\")\n",
    "response.print_response_stream()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从提供的信息来看，不耐疲劳、口燥、咽干等症状可能与以下几种证候相关：\n",
    "\n",
    "1. **津液不足证 (Syndrome of Fluid and Humor Insufficiency)**：这种证候的特点是口眼喉鼻及皮肤等部位干燥，大便干结，小便短少，舌质偏红而干，脉细数。这些症状与不耐疲劳、口燥、咽干相符。\n",
    "\n",
    "2. **津液亏涸证 (Syndrome of Fluid and Humor Scantiness)**：这种证候表现为口干、唇裂，鼻燥无涕，皮肤干瘪，目陷、螺瘪，甚则肌肤甲错，舌质红而少津，舌中裂，脉细或数。这些症状也包括口燥和咽干。\n",
    "\n",
    "3. **燥邪犯肺证 (Syndrome of Pathogenic Dryness Invading the Lung)**：这种证候的特点是干咳、少痰或无痰，痰黏不易咳出，唇鼻咽喉干燥，声音嘶哑，口渴，咳甚则胸痛，或痰中血丝，舌尖红，舌苔薄黄、少津，脉细或数。其中唇鼻咽喉干燥的症状与口燥、咽干相符。\n",
    "\n",
    "4. **燥干清窍证 (Syndrome of Dryness Harassing the Upper Orifices)**：这种证候的特点是口鼻、咽喉干燥，两眼干涩，少泪、少涕、少津、甚则衄血，舌质瘦小、舌苔干而少津，脉细。这些症状也包括口燥、咽干。\n",
    "\n",
    "综上所述，不耐疲劳、口燥、咽干等症状可能与津液不足证、津液亏涸证、燥邪犯肺证以及燥干清窍证相关。具体诊断需要结合其他临床表现和舌脉象综合判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、追踪哪些文档片段被检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从存储文件中读取embedding向量和向量索引\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./doc_emb\")\n",
    "\n",
    "# 根据存储的embedding向量和向量索引重新构建检索索引\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "# 构建查询引擎\n",
    "query_engine = index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "# 获取我们抽取出的相似度 top 5 的片段\n",
    "contexts = query_engine.retrieve(QueryBundle(\"不耐疲劳，口燥、咽干可能是哪些证候？\"))\n",
    "print('-' * 10 + 'ref' + '-' * 10)\n",
    "for i, context in enumerate(contexts):\n",
    "    print('#' * 10 + f'chunk {i} start' + '#' * 10)\n",
    "    content = context.node.get_content(metadata_mode=MetadataMode.LLM)\n",
    "    print(content)\n",
    "    print('#' * 10 + f'chunk {i} end' + '#' * 10)\n",
    "print('-' * 10 + 'ref' + '-' * 10)\n",
    "\n",
    "# 查询获得答案\n",
    "response = query_engine.query(\"不耐疲劳，口燥、咽干可能是哪些证候？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------ref----------\n",
    "\n",
    "##########chunk 0 start##########\n",
    "file_path: /home/jukeai/ai_projects/tcm-ai-rag/documents/demo-1.txt\n",
    "\n",
    "临床以口眼喉鼻及皮肤等干燥，大便干结，小便短少，舌质偏红而干，脉细数等为特征的证候。\n",
    "\n",
    "4.6.1.2\n",
    "    津液亏涸证  syndrome/pattern of fluid and humor scantiness\n",
    "    津液亏耗证\n",
    "    津液干枯证\n",
    "    因津液亏损，形体官窍失养所致。临床以口干、唇裂，鼻燥无涕，皮肤干瘪，目陷、螺瘪，甚则肌肤甲错，舌质红而少津，舌中裂，脉细或数，可伴见口渴、欲饮，干咳，目涩，大便干，小便少等为特征的证候。\n",
    "##########chunk 0 end##########\n",
    "\n",
    "##########chunk 1 start##########\n",
    "file_path: /home/jukeai/ai_projects/tcm-ai-rag/documents/demo-1.txt\n",
    "\n",
    "临床以口干、舌燥，频饮而不解其渴，食多、善饥，夜尿频多，逐渐消瘦，舌质红，舌苔薄黄或少，脉弦细或滑数，伴见皮肤干燥，四肢乏力，大便干结等为特征的证候。\n",
    "\n",
    "4.6.3.2\n",
    "    津亏热结证  syndrome/pattern of fluid depletion and heat binding\n",
    "    液干热结证\n",
    "    因津液亏虚，热邪内结所致。\n",
    "##########chunk 1 end##########\n",
    "\n",
    "##########chunk 2 start##########\n",
    "file_path: /home/jukeai/ai_projects/tcm-ai-rag/documents/demo-1.txt\n",
    "\n",
    "临床以口鼻、咽喉干燥，两眼干涩，少泪、少涕、少津、甚则衄血，舌质瘦小、舌苔干而少津，脉细等为特征的证候。\n",
    "\n",
    "3.6.3.3\n",
    "    燥邪犯肺证  syndrome/pattern of pathogenic dryness invading the lung\n",
    "    燥邪伤肺证\n",
    "    因外感燥邪，或感受风热，化燥伤阴，肺失清肃所致。临床以干咳、少痰或无痰，痰黏不易咳出，唇鼻咽喉干燥，声音嘶哑，口渴，咳甚则胸痛，或痰中血丝，舌尖红，舌苔薄黄、少津，脉细或数，初起或伴见发热、恶寒，头痛等为特征的证候。\n",
    "##########chunk 2 end##########\n",
    "\n",
    "##########chunk 3 start##########\n",
    "file_path: /home/jukeai/ai_projects/tcm-ai-rag/documents/demo-1.txt\n",
    "\n",
    "临床以鼻咽干涩或痛，口唇燥干，舌质红，舌苔白或燥，脉浮或微数，伴见发热、无汗，头痛或肢节酸痛等为特征的证候。\n",
    "\n",
    "3.6.3.2\n",
    "    燥干清窍证  syndrome/pattern of dryness harassing the upper orifices\n",
    "    因气候或环境干燥，津液耗损，清窍失濡所致。临床以口鼻、咽喉干燥，两眼干涩，少泪、少涕、少津、甚则衄血，舌质瘦小、舌苔干而少津，脉细等为特征的证候。\n",
    "##########chunk 3 end##########\n",
    "\n",
    "##########chunk 4 start##########\n",
    "file_path: /home/jukeai/ai_projects/tcm-ai-rag/documents/demo-1.txt\n",
    "\n",
    "4.6.1.1\n",
    "    津液不足证  syndrome/pattern of fluid and humor insufficiency\n",
    "    津亏证\n",
    "    因津液生成不足，或嗜食辛辣，蕴热化燥，邪热灼损津液所致。临床以口眼喉鼻及皮肤等干燥，大便干结，小便短少，舌质偏红而干，脉细数等为特征的证候。\n",
    "##########chunk 4 end##########\n",
    "\n",
    "----------ref----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBUG:llama_index.core.indices.utils:> Top 5 nodes:\n",
    "\n",
    "> [Node 03e55531-d58c-4b27-a688-65eaf6dcbe49] [**Similarity score:             0.728016**] 临床以口眼喉鼻及皮肤等干燥，大便干结，小便短少，舌质偏红而干，脉细数等为特征的证候。\n",
    "\n",
    "4.6.1.2\n",
    "    津液亏涸证  syndrome/pattern of fluid and h...\n",
    "> [Node 8061e5a1-ef80-4126-9caf-d85c09c5fc79] [**Similarity score:             0.717796**] 临床以口干、舌燥，频饮而不解其渴，食多、善饥，夜尿频多，逐渐消瘦，舌质红，舌苔薄黄或少，脉弦细或滑数，伴见皮肤干燥，四肢乏力，大便干结等为特征的证候。\n",
    "\n",
    "4.6.3.2\n",
    "    津亏热结证...\n",
    "> [Node e5fb66ab-4935-49b2-95c8-f3bc29dd3641] [**Similarity score:             0.716694**] 临床以口鼻、咽喉干燥，两眼干涩，少泪、少涕、少津、甚则衄血，舌质瘦小、舌苔干而少津，脉细等为特征的证候。\n",
    "\n",
    "3.6.3.3\n",
    "    燥邪犯肺证  syndrome/pattern of p...\n",
    "> [Node 09c59214-2197-4064-96a8-c6848aab2bcf] [**Similarity score:             0.714603**] 临床以鼻咽干涩或痛，口唇燥干，舌质红，舌苔白或燥，脉浮或微数，伴见发热、无汗，头痛或肢节酸痛等为特征的证候。\n",
    "\n",
    "3.6.3.2\n",
    "    燥干清窍证  syndrome/pattern of...\n",
    "> [Node ebcad040-24a0-4f4e-9c99-907f4102cbbb] [**Similarity score:             0.711374**] 4.6.1.1\n",
    "    津液不足证  syndrome/pattern of fluid and humor insufficiency\n",
    "    津亏证\n",
    "    因津液生成不足，或嗜食辛辣..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<ul>\n",
    "    <li>追踪检索片段，调整chunk_size的值，可以让embedding模型切分出的片段更合理，提高RAG系统的表现。\n",
    "    <li>如果想追踪更多的检索片段，可以提高 similarity_top_k 的值。</li>\n",
    "    <li>如果想追踪片段具体的相似度得分（Similarity Score）的值，可以将log中的level设置为DEBUG级别。</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 八、RAG 检索底层实现细节\n",
    "\n",
    "知道了如何追踪哪些文档片段被用于检索增强生成，但我们仍不知道RAG过程中到底发生了什么，为什么大模型能够根据检索出的文档片段进行回复？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from llama_index.core import PromptTemplate, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.core.callbacks import LlamaDebugHandler, CallbackManager\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# 定义日志\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "\n",
    "# 定义system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant.\"\"\"\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")\n",
    "\n",
    "# 使用llama-index创建本地大模型\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    context_window = 4096,\n",
    "    max_new_tokens = 2048,\n",
    "    generate_kwargs = {\"temperature\": 0.0, \"do_sample\": False},\n",
    "    query_wrapper_prompt = query_wrapper_prompt,\n",
    "    tokenizer_name = \"/home/kevin/projects/models/Qwen/Qwen1.5-7B-Chat\",\n",
    "    model_name = \"/home/kevin/projects/models/Qwen/Qwen1.5-7B-Chat\",\n",
    "    device_map = \"auto\",\n",
    "    model_kwargs = {\"torch_dtype\": torch.float16},\n",
    ")\n",
    "\n",
    "# 使用LlamaDebugHandler构建事件回溯器，以追踪LlamaIndex执行过程中发生的事件\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "Settings.callback_manager = callback_manager\n",
    "\n",
    "# 使用llama-index-embeddings-huggingface构建本地embedding模型\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name = \"/home/kevin/projects/models/BAAI/bge-base-zh-v1.5\"\n",
    ")\n",
    "\n",
    "# 从存储文件中读取embedding向量和向量索引\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./doc_emb\")\n",
    "\n",
    "# 根据存储的embedding向量和向量索引重新构建检索索引\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "# 构建查询引擎\n",
    "query_engine = index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "# 查询获得答案\n",
    "response = query_engine.query(\"不耐疲劳，口燥、咽干可能是哪些证候？\")\n",
    "print(response)\n",
    "\n",
    "# get_llm_inputs_outputs 返回每个LLM调用的开始/结束事件\n",
    "event_pairs = llama_debug.get_llm_inputs_outputs()\n",
    "\n",
    "# print(event_pairs[0][1].payload.keys()) # 输出事件结束时所有相关的属性\n",
    "\n",
    "# 输出 Promt 构建过程\n",
    "print(event_pairs[0][1].payload[\"formatted_prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Query 过程分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "**********\n",
    "Trace: query\n",
    "    |_query -> 63.648696 seconds\n",
    "      |_retrieve -> 1.186543 seconds\n",
    "        |_embedding -> 1.047233 seconds\n",
    "      |_synthesize -> 62.461404 seconds\n",
    "        |_templating -> 3.3e-05 seconds\n",
    "        |_llm -> 62.451146 seconds\n",
    "**********\n",
    "</pre>\n",
    "\n",
    "以上的输出记录了query在程序过程中经历的阶段和所用的时间，整个过程分为两个阶段：\n",
    "\n",
    "  - 抽取（retrieve）\n",
    "  - 合成（synthesize）。\n",
    "\n",
    "合成阶段的templating步骤会将query和抽取出来的文档片段组合成模板，构成新的query，然后调用LLM，得到最终的response。\n",
    "\n",
    "所以，只要找到templating所构建的新query，就可以知道为什么大模型能够根据我们检索出来的文档进行回复了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 formatted_prompt\n",
    "\n",
    "下面这段文本就是 print(event_pairs[0][1].payload[\"formatted_prompt\"]) 语句输出的，\n",
    "\n",
    "下面这段文本就是 `templating` 后的新 `query`\n",
    "\n",
    "原始query由\"不耐疲劳，口燥、咽干可能是哪些证候？\"变成了下面这段很长的新query，由于我们给大模型提供了一些文档片段知识，并且要求大模型根据提供的检索知识回答原始query，因此大模型能够根据检索出的文档片段进行回复。（这其实也就是RAG技术的本质了）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "\n",
    "[INST]<<SYS>>\n",
    "You are a helpful AI assistant.<</SYS>>\n",
    "\n",
    "Context information is below.\n",
    "---------------------\n",
    "file_path: /home/jukeai/ai_projects/tcm-ai-rag/documents/demo-1.txt\n",
    "\n",
    "临床以口眼喉鼻及皮肤等干燥，大便干结，小便短少，舌质偏红而干，脉细数等为特征的证候。\n",
    "\n",
    "4.6.1.2\n",
    "    津液亏涸证  syndrome/pattern of fluid and humor scantiness\n",
    "    津液亏耗证\n",
    "    津液干枯证\n",
    "    因津液亏损，形体官窍失养所致。临床以口干、唇裂，鼻燥无涕，皮肤干瘪，目陷、螺瘪，甚则肌肤甲错，舌质红而少津，舌中裂，脉细或数，可伴见口渴、欲饮，干咳，目涩，大便干，小便少等为特征的证候。\n",
    "\n",
    "file_path: /home/jukeai/ai_projects/tcm-ai-rag/documents/demo-1.txt\n",
    "\n",
    "临床以口干、舌燥，频饮而不解其渴，食多、善饥，夜尿频多，逐渐消瘦，舌质红，舌苔薄黄或少，脉弦细或滑数，伴见皮肤干燥，四肢乏力，大便干结等为特征的证候。\n",
    "\n",
    "4.6.3.2\n",
    "    津亏热结证  syndrome/pattern of fluid depletion and heat binding\n",
    "    液干热结证\n",
    "    因津液亏虚，热邪内结所致。\n",
    "\n",
    "file_path: /home/jukeai/ai_projects/tcm-ai-rag/documents/demo-1.txt\n",
    "\n",
    "临床以口鼻、咽喉干燥，两眼干涩，少泪、少涕、少津、甚则衄血，舌质瘦小、舌苔干而少津，脉细等为特征的证候。\n",
    "\n",
    "3.6.3.3\n",
    "    燥邪犯肺证  syndrome/pattern of pathogenic dryness invading the lung\n",
    "    燥邪伤肺证\n",
    "    因外感燥邪，或感受风热，化燥伤阴，肺失清肃所致。临床以干咳、少痰或无痰，痰黏不易咳出，唇鼻咽喉干燥，声音嘶哑，口渴，咳甚则胸痛，或痰中血丝，舌尖红，舌苔薄黄、少津，脉细或数，初起或伴见发热、恶寒，头痛等为特征的证候。\n",
    "\n",
    "file_path: /home/jukeai/ai_projects/tcm-ai-rag/documents/demo-1.txt\n",
    "\n",
    "临床以鼻咽干涩或痛，口唇燥干，舌质红，舌苔白或燥，脉浮或微数，伴见发热、无汗，头痛或肢节酸痛等为特征的证候。\n",
    "\n",
    "3.6.3.2\n",
    "    燥干清窍证  syndrome/pattern of dryness harassing the upper orifices\n",
    "    因气候或环境干燥，津液耗损，清窍失濡所致。临床以口鼻、咽喉干燥，两眼干涩，少泪、少涕、少津、甚则衄血，舌质瘦小、舌苔干而少津，脉细等为特征的证候。\n",
    "\n",
    "file_path: /home/jukeai/ai_projects/tcm-ai-rag/documents/demo-1.txt\n",
    "\n",
    "4.6.1.1\n",
    "    津液不足证  syndrome/pattern of fluid and humor insufficiency\n",
    "    津亏证\n",
    "    因津液生成不足，或嗜食辛辣，蕴热化燥，邪热灼损津液所致。临床以口眼喉鼻及皮肤等干燥，大便干结，小便短少，舌质偏红而干，脉细数等为特征的证候。\n",
    "\n",
    "4.6.1.\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: 不耐疲劳，口燥、咽干可能是哪些证候？\n",
    "Answer: [/INST]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<ul>\n",
    "    <li>新query中既有中文，也有英文，这是因为LlamaIndex框架默认构建的模板都是英文的</li>\n",
    "    <li>LlamaIndex允许自定义查询流程，构建自己的中文模板</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Retrieve 检索进阶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抽取（retrieve）阶段的retrievers模块规定了针对查询从知识库获取相关上下文的技术。我们之前使用的都是默认的方法，其实LlamaIndex官方为我们提供了一些其他常用的方法：\n",
    "\n",
    "- SimilarityPostprocessor: 使用similarity_cutoff设置阈值。移除低于某个相似度分数的节点。\n",
    "- KeywordNodePostprocessor: 使用required_keywords和exclude_keywords。根据关键字包含或排除过滤节点。\n",
    "- MetadataReplacementPostProcessor: 用其元数据中的数据替换节点内容。\n",
    "- LongContextReorder: 重新排序节点，这有利于需要大量顶级结果的情况，可以解决模型在扩展上下文中的困难。\n",
    "- SentenceEmbeddingOptimizer: 选择percentile_cutoff或threshold_cutoff作为相关性。基于嵌入删除不相关的句子。\n",
    "- CohereRerank: 使用coherence ReRank对节点重新排序，返回前N个结果。\n",
    "- SentenceTransformerRerank: 使用SentenceTransformer交叉编码器对节点重新排序，产生前N个节点。\n",
    "- LLMRerank: 使用LLM对节点重新排序，为每个节点提供相关性评分。\n",
    "- FixedRecencyPostprocessor: 返回按日期排序的节点。\n",
    "- EmbeddingRecencyPostprocessor: 按日期对节点进行排序，但也会根据嵌入相似度删除较旧的相似节点。\n",
    "- TimeWeightedPostprocessor: 对节点重新排序，偏向于最近未返回的信息。\n",
    "- PIINodePostprocessor(β): 可以利用本地LLM或NER模型删除个人身份信息。\n",
    "- PrevNextNodePostprocessor(β): 根据节点关系，按顺序检索在节点之前、之后或两者同时出现的节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 响应合成器 response synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合成（synthesize）阶段的响应合成器（response synthesizer）会引导LLM生成响应，将用户查询与检索到的文本块混合在一起，并给出一个精心设计的答案。\n",
    "\n",
    "LlamaIndex官方为我们提供了多种响应合成器：\n",
    "\n",
    "- Refine: 这种方法遍历每一段文本，一点一点地精炼答案。\n",
    "- Compact: 是Refine的精简版。它将文本集中在一起，因此需要处理的步骤更少。\n",
    "- Tree Summarize: 想象一下，把许多小的答案结合起来，再总结，直到你得到一个主要的答案。\n",
    "- Simple Summarize: 只是把文本片段剪短，然后给出一个快速的总结。\n",
    "- No Text: 这个问题不会给你答案，但会告诉你它会使用哪些文本。\n",
    "- Accumulate: 为每一篇文章找一堆小答案，然后把它们粘在一起。\n",
    "- Compact Accumulate: 是“Compact”和“Accumulate”的合成词。\n",
    "\n",
    "现在，我们选择一种retriever和一种response synthesizer。retriever选择SimilarityPostprocessor，response synthesizer选择Refine。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from llama_index.core import PromptTemplate, Settings, SimpleDirectoryReader, VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.callbacks import LlamaDebugHandler, CallbackManager\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# 定义日志\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# 定义system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant.\"\"\"\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")\n",
    "\n",
    "# 使用 llama_index_llms_huggingface 调用本地大模型\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    context_window = 4096,\n",
    "    max_new_tokens = 2048,\n",
    "    generate_kwargs = {\"temperature\": 0.0, \"do_sample\": False},\n",
    "    query_wrapper_prompt = query_wrapper_prompt,\n",
    "    tokenizer_name = \"/home/kevin/projects/models/Qwen/Qwen1.5-7B-Chat\",\n",
    "    model_name = \"/home/kevin/projects/models/Qwen/Qwen1.5-7B-Chat\",\n",
    "    device_map = \"auto\",\n",
    "    model_kwargs = {\"torch_dtype\": torch.float16},\n",
    ")\n",
    "\n",
    "# 使用LlamaDebugHandler构建事件回溯器，以追踪LlamaIndex执行过程中发生的事件\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "Settings.callback_manager = callback_manager\n",
    "\n",
    "# 使用llama-index-embeddings-huggingface构建本地embedding模型\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"/home/kevin/projects/models/BAAI/bge-base-zh-v1.5\"\n",
    ")\n",
    "\n",
    "# 读取文档并构建索引\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 构建retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index = index,\n",
    "    similarity_top_k = 5,\n",
    ")\n",
    "\n",
    "# 构建response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode = ResponseMode.REFINE\n",
    ")\n",
    "\n",
    "# 构建查询引擎\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever = retriever,\n",
    "    response_synthesizer = response_synthesizer,\n",
    "    node_postprocessors = [SimilarityPostprocessor(similarity_cutoff=0.6)]\n",
    ")\n",
    "\n",
    "# 查询获得答案\n",
    "response = query_engine.query(\"不耐疲劳，口燥、咽干可能是哪些证候？\")\n",
    "print(response)\n",
    "\n",
    "# get_llm_inputs_outputs返回每个LLM调用的开始/结束事件\n",
    "event_pairs = llama_debug.get_llm_inputs_outputs()\n",
    "print(event_pairs[0][1].payload[\"formatted_prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "**********\n",
    "Trace: query\n",
    "    |_query -> 61.077074 seconds\n",
    "      |_synthesize -> 61.03387 seconds\n",
    "        |_templating -> 2.3e-05 seconds\n",
    "        |_llm -> 5.93741 seconds\n",
    "        |_templating -> 2.2e-05 seconds\n",
    "        |_llm -> 6.627645 seconds\n",
    "        |_templating -> 2.3e-05 seconds\n",
    "        |_llm -> 13.074158 seconds\n",
    "        |_templating -> 2.5e-05 seconds\n",
    "        |_llm -> 9.181957 seconds\n",
    "        |_templating -> 2.3e-05 seconds\n",
    "        |_llm -> 26.191079 seconds\n",
    "**********\n",
    "</pre>\n",
    "\n",
    "可以看出，将response synthesizer由默认的Compact替换为Refine之后，query在程序过程中经历的阶段发生了变化，REFINE模式会进行更多次的templating和LLM调用。\n",
    "\n",
    "实际开发中可以自由组合不同的retriever和response synthesizer，以完成我们的需求。当LlamaIndex提供的retriever和response synthesizer不能满足我们的需求的时候，我们还可以自定义retriever和response synthesizer。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 九、自定义 Prompt\n",
    "\n",
    "LlamaIndex中提供的prompt template都是英文的，该如何使用中文的prompt template呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from llama_index.core import PromptTemplate, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.core.callbacks import LlamaDebugHandler, CallbackManager\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# 定义日志\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "\n",
    "# 定义system prompt\n",
    "SYSTEM_PROMPT = \"\"\"你是一个医疗人工智能助手。\"\"\"\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")\n",
    "\n",
    "# 定义qa prompt\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"上下文信息如下。\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"请根据上下文信息而不是先验知识来回答以下的查询。\"\n",
    "    \"作为一个医疗人工智能助手，你的回答要尽可能严谨。\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "# 定义refine prompt\n",
    "refine_prompt_tmpl_str = (\n",
    "    \"原始查询如下：{query_str}\"\n",
    "    \"我们提供了现有答案：{existing_answer}\"\n",
    "    \"我们有机会通过下面的更多上下文来完善现有答案（仅在需要时）。\"\n",
    "    \"------------\"\n",
    "    \"{context_msg}\"\n",
    "    \"------------\"\n",
    "    \"考虑到新的上下文，优化原始答案以更好地回答查询。 如果上下文没有用，请返回原始答案。\"\n",
    "    \"Refined Answer:\"\n",
    ")\n",
    "refine_prompt_tmpl = PromptTemplate(refine_prompt_tmpl_str)\n",
    "\n",
    "# 使用llama-index-llm-huggingface调用本地大模型\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    context_window = 4096,\n",
    "    max_new_tokens = 2048,\n",
    "    generate_kwargs = {\"temperature\": 0.0, \"do_sample\": False},\n",
    "    query_wrapper_prompt = query_wrapper_prompt,\n",
    "    tokenizer_name = \"/home/kevin/projects/models/Qwen/Qwen1.5-7B-Chat\",\n",
    "    model_name = \"/home/kevin/projects/models/Qwen/Qwen1.5-7B-Chat\",\n",
    "    device_map = \"auto\",\n",
    "    model_kwargs = {\"torch_dtype\": torch.float16},\n",
    ")\n",
    "\n",
    "# 使用LlamaDebugHandler构建事件回溯器，以追踪LlamaIndex执行过程中发生的事件\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "Settings.callback_manager = callback_manager\n",
    "\n",
    "# 使用llama-index-embeddings-huggingface调用本地embedding模型\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"/home/kevin/projects/models/BAAI/bge-base-zh-v1.5\"\n",
    ")\n",
    "\n",
    "# 从存储文件中读取embedding向量和向量索引\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"doc_emb\")\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "# 构建查询引擎\n",
    "query_engine = index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "# 输出查询引擎中所有的prompt类型\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "print(list(prompts_dict.keys()))\n",
    "\n",
    "# 更新查询引擎中的prompt template\n",
    "query_engine.update_prompts(\n",
    "    {\n",
    "        \"response_synthesizer:text_qa_template\": qa_prompt_tmpl,\n",
    "        \"response_synthesizer:refine_template\": refine_prompt_tmpl\n",
    "    }\n",
    ")\n",
    "\n",
    "# 查询获得答案\n",
    "response = query_engine.query(\"不耐疲劳，口燥、咽干可能是哪些证候？\")\n",
    "print(response)\n",
    "\n",
    "# 输出formatted_prompt\n",
    "event_pairs = llama_debug.get_llm_inputs_outputs()\n",
    "print(event_pairs[0][1].payload[\"formatted_prompt\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
