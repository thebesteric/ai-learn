{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d0faef-0ef0-4a1b-a7d8-99f59cd72a09",
   "metadata": {},
   "source": [
    "# 第4章 LlamaIndex知识管理与信息检索\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8c5bb",
   "metadata": {},
   "source": [
    "## 💡 学习目标\n",
    "\n",
    "1. 掌握 LlamaIndex 的特点和基本用法\n",
    "2. 掌握 LlamaIndex 内置的工具\n",
    "3. 如何用好 SDK 简化基于 LLM 的应用开发\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194427bf-5807-49d4-ab25-fa0556f835f3",
   "metadata": {},
   "source": [
    "## 1. 大语言模型开发框架的价值是什么？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60e6e5-8f2a-4cd5-a4b5-824259afc229",
   "metadata": {},
   "source": [
    "_SDK：Software Development Kit，它是一组软件工具和资源的集合，旨在帮助开发者创建、测试、部署和维护应用程序或软件。_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d08668-4981-4b84-ae07-f74cfa191309",
   "metadata": {},
   "source": [
    "-所有开发框架（SDK）的核心价值，都是降低开发、维护成本。\n",
    "\n",
    "大语言模型开发框架的价值，是让开发者可以更方便地开发基于大语言模型的应用。主要提供两类帮助：\n",
    "\n",
    "1. 第三方能力抽象。比如 LLM、向量数据库、搜索接口等\n",
    "2. 常用工具、方案封装\n",
    "3. 底层实现封装。比如流式接口、超时重连、异步与并行等\n",
    "\n",
    "好的开发框架，需要具备以下特点：\n",
    "\n",
    "1. 可靠性、鲁棒性高\n",
    "2. 可维护性高\n",
    "3. 可扩展性高\n",
    "4. 学习成本低\n",
    "\n",
    "举些通俗的例子：\n",
    "\n",
    "- 与外部功能解依赖\n",
    "  - 比如可以随意更换 LLM 而不用大量重构代码\n",
    "  - 更换三方工具也同理\n",
    "- 经常变的部分要在外部维护而不是放在代码里\n",
    "  - 比如 Prompt 模板\n",
    "- 各种环境下都适用\n",
    "  - 比如线程安全\n",
    "- 方便调试和测试\n",
    "  - 至少要能感觉到用了比不用方便吧\n",
    "  - 合法的输入不会引发框架内部的报错\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>选对了框架，事半功倍；反之，事倍功半。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b589b-5a4e-451e-baf0-d2a409a9cb4b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>什么是 SDK?</b> https://aws.amazon.com/cn/what-is/sdk/\n",
    "<br/>\n",
    "<b>SDK 和 API 的区别是什么?</b> https://aws.amazon.com/cn/compare/the-difference-between-sdk-and-api/\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30361cf7-ec31-4c45-871e-28ac4f0db1a9",
   "metadata": {},
   "source": [
    "#### 🌰 举个例子：使用 SDK，4 行代码实现一个简易的 RAG 系统\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517bfe1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<p>LlamaIndex 默认的 Embedding 模型是 <code>OpenAIEmbedding(model=\"text-embedding-ada-002\")</code></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03dc82-649d-4fe8-8ba5-023220c8cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade llama-index\n",
    "\n",
    "# !pip install llama-index-llms-dashscope\n",
    "# !pip install llama-index-llms-openai-like\n",
    "# !pip install llama-index-embeddings-dashscope"
   ]
  },
  {
   "cell_type": "code",
   "id": "f36875ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:00:34.530504Z",
     "start_time": "2025-05-20T11:00:32.749235Z"
    }
   },
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.llms.dashscope import DashScope, DashScopeGenerationModels\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels\n",
    "\n",
    "# LlamaIndex默认使用的大模型被替换为百炼\n",
    "# Settings.llm = OpenAILike(\n",
    "#     model=\"qwen-max\",\n",
    "#     api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "#     api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "#     is_chat_model=True\n",
    "# )\n",
    "\n",
    "Settings.llm = DashScope(model_name=DashScopeGenerationModels.QWEN_MAX, api_key=os.getenv(\"DASHSCOPE_API_KEY\"))\n",
    "\n",
    "# LlamaIndex默认使用的Embedding模型被替换为百炼的Embedding模型\n",
    "Settings.embed_model = DashScopeEmbedding(\n",
    "    # model_name=\"text-embedding-v1\"\n",
    "    model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1,\n",
    "    # api_key=os.getenv(\"DASHSCOPE_API_KEY\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "dea32ce7-c7a7-4692-a217-45cf632281ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:00:41.907038Z",
     "start_time": "2025-05-20T11:00:38.410963Z"
    }
   },
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "# index.storage_context.persist(persist_dir=\"./storage\")\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"deepseek v3有多少参数？\")\n",
    "\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 拥有总计 6710 亿参数，其中每个 token 激活 370 亿参数。\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "24e4c617-2fb8-489d-a0d0-34d5ca196f1c",
   "metadata": {},
   "source": [
    "## 2. LlamaIndex 介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313487e1-a2c7-4d4b-ad41-e3a9f0a0b07e",
   "metadata": {},
   "source": [
    "官网标题：_「 Build AI Knowledge Assistants over your enterprise data 」_\n",
    "\n",
    "LlamaIndex 是一个为开发「知识增强」的大语言模型应用的框架（也就是 SDK）。**知识增强**，泛指任何在私有或特定领域数据基础上应用大语言模型的情况。例如：\n",
    "\n",
    "<img src=\"./assets/basic_rag.png\" width=800px>\n",
    "\n",
    "- Question-Answering Chatbots (也就是 RAG)\n",
    "- Document Understanding and Extraction （文档理解与信息抽取）\n",
    "\n",
    "- Autonomous Agents that can perform research and take actions （智能体应用）\n",
    "- Workflow orchestrating single and multi-agent (编排单个或多个智能体形成工作流）\n",
    "\n",
    "LlamaIndex 有 Python 和 Typescript 两个版本，Python 版的文档相对更完善。\n",
    "\n",
    "- Python 文档地址：https://docs.llamaindex.ai/en/stable/\n",
    "- Python API 接口文档：https://docs.llamaindex.ai/en/stable/api_reference/\n",
    "\n",
    "- TS 文档地址：https://ts.llamaindex.ai/\n",
    "\n",
    "LlamaIndex 是一个开源框架，Github 链接：https://github.com/run-llama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4822b2-52d5-411c-8244-3432f8733da2",
   "metadata": {},
   "source": [
    "### LlamaIndex 的核心模块\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c474b43-018a-4687-a51e-54b391a6bbca",
   "metadata": {},
   "source": [
    "<img src=\"./assets/llamaindex.png\" alt=\"LlamaIndex 核心模块\" width=\"1400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66bf37-48e4-45eb-9e7e-a24e86108ac1",
   "metadata": {},
   "source": [
    "### 安装 LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80bba90",
   "metadata": {},
   "outputs": [],
   "source": "# ！pip install -U llama-index"
  },
  {
   "cell_type": "markdown",
   "id": "74341a28-fb7f-4e5b-9342-cf5cd850654a",
   "metadata": {},
   "source": [
    "## 3.数据加载（Loading）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d134a56-ae9a-41a5-a703-ce9dfb3fc600",
   "metadata": {},
   "source": [
    "### 3.1、加载本地数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35bfa25-9a09-4c40-afa1-9f9156e56dc2",
   "metadata": {},
   "source": [
    "`SimpleDirectoryReader` 是一个简单的本地文件加载器。它会遍历指定目录，并根据文件扩展名自动加载文件（**文本内容**）。\n",
    "\n",
    "支持的文件类型：\n",
    "\n",
    "- `.csv` - comma-separated values\n",
    "- `.docx` - Microsoft Word\n",
    "- `.epub` - EPUB ebook format\n",
    "- `.hwp` - Hangul Word Processor\n",
    "- `.ipynb` - Jupyter Notebook\n",
    "- `.jpeg`, `.jpg` - JPEG image\n",
    "- `.mbox` - MBOX email archive\n",
    "- `.md` - Markdown\n",
    "- `.mp3`, `.mp4` - audio and video\n",
    "- `.pdf` - Portable Document Format\n",
    "- `.png` - Portable Network Graphics\n",
    "- `.ppt`, `.pptm`, `.pptx` - Microsoft PowerPoint\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "05b6a279-3eca-4685-af91-80fb443fca1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:00:47.583209Z",
     "start_time": "2025-05-20T11:00:47.579904Z"
    }
   },
   "source": [
    "import json\n",
    "from pydantic.v1 import BaseModel\n",
    "\n",
    "def show_json(data):\n",
    "    \"\"\"用于展示json数据\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        obj = json.loads(data)\n",
    "        print(json.dumps(obj, indent=4, ensure_ascii=False))\n",
    "    elif isinstance(data, dict) or isinstance(data, list):\n",
    "        print(json.dumps(data, indent=4, ensure_ascii=False))\n",
    "    elif issubclass(type(data), BaseModel):\n",
    "        print(json.dumps(data.dict(), indent=4, ensure_ascii=False))\n",
    "\n",
    "def show_list_obj(data):\n",
    "    \"\"\"用于展示一组对象\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            show_json(item)\n",
    "    else:\n",
    "        raise ValueError(\"Input is not a list\")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "198ebc20-fab6-46a9-8cca-4b114fe2640f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T03:23:04.172365Z",
     "start_time": "2025-05-20T03:23:04.122703Z"
    }
   },
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "        input_dir=\"./data\", # 目标目录\n",
    "        recursive=False, # 是否递归遍历子目录\n",
    "        required_exts=[\".pdf\"] # (可选)只读取指定后缀的文件\n",
    "    )\n",
    "documents = reader.load_data()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "dfb3f1ac-75cb-4c5e-b0a1-53621d8d325e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T03:23:06.832093Z",
     "start_time": "2025-05-20T03:23:06.829777Z"
    }
   },
   "source": [
    "print(documents[0].text)\n",
    "show_json(documents[0].json())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3 24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\n",
      "Figure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "{\n",
      "    \"id_\": \"583ed317-f1fb-444a-bb6f-a53645bca5b3\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {\n",
      "        \"page_label\": \"1\",\n",
      "        \"file_name\": \"deepseek-v3-1-4.pdf\",\n",
      "        \"file_path\": \"/Users/wangweijun/PycharmProjects/ai-learn/ai-level-1/第4章_LlamaIndex知识管理与信息检索/data/deepseek-v3-1-4.pdf\",\n",
      "        \"file_type\": \"application/pdf\",\n",
      "        \"file_size\": 192218,\n",
      "        \"creation_date\": \"2025-05-19\",\n",
      "        \"last_modified_date\": \"2025-03-12\"\n",
      "    },\n",
      "    \"excluded_embed_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"excluded_llm_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"relationships\": {},\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text_resource\": {\n",
      "        \"embeddings\": null,\n",
      "        \"text\": \"DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\",\n",
      "        \"path\": null,\n",
      "        \"url\": null,\n",
      "        \"mimetype\": null\n",
      "    },\n",
      "    \"image_resource\": null,\n",
      "    \"audio_resource\": null,\n",
      "    \"video_resource\": null,\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"Document\",\n",
      "    \"text\": \"DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "277be7f4-1993-4b74-bd3e-f9d8cb5a827d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>注意：</b>对图像、视频、语音类文件，默认不会自动提取其中文字。如需提取，参考下面介绍的 <code>Data Connectors</code>。\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c98851-0858-4215-858d-bea70e310d5f",
   "metadata": {},
   "source": [
    " 默认的 `PDFReader` 效果并不理想，我们可以更换文件加载器\n",
    "\n",
    "<b>LlamaParse</b>\n",
    "\n",
    "首先，登录并从 https://cloud.llamaindex.ai ↗ 注册并获取 api-key 。\n",
    "\n",
    "然后，安装该包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-cloud-services"
   ]
  },
  {
   "cell_type": "code",
   "id": "b093a88b-5d4a-423b-bfe7-fa28b34744e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:00:30.189695Z",
     "start_time": "2025-05-20T10:00:21.372930Z"
    }
   },
   "source": [
    "# 在系统环境变量里配置 LLAMA_CLOUD_API_KEY=XXX\n",
    "\n",
    "from llama_cloud_services import LlamaParse\n",
    "from llama_cloud_services.parse import ResultType\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import nest_asyncio, os\n",
    "nest_asyncio.apply() # 只在Jupyter笔记环境中需要此操作，否则会报错\n",
    "\n",
    "# set up parser\n",
    "parser = LlamaParse(\n",
    "    api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),\n",
    "    result_type=ResultType.MD  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data\", required_exts=[\".pdf\"], file_extractor=file_extractor).load_data()\n",
    "print(documents[0].text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id e164d61e-7800-43fb-b013-de9c5a8ab9fb\n",
      "# DeepSeek-V3 Technical Report\n",
      "\n",
      "# DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "# Abstract\n",
      "\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "\n",
      "|DeepSeek-V3|DeepSeek-V2.5|Qwen2.5-72B-Inst|Llama-3.1-405B-Inst|GPT-4o-0513|Claude-3.5-Sonnet-1022| |\n",
      "|---|---|---|---|---|---|---|\n",
      "|100|90.2| | | | | |\n",
      "|80|75.9|71.6|73.3|72.6|78.0| |\n",
      "| |74.7|80.0|73.8|74.6|78.3| |\n",
      "|66.2| |65.0| | | | |\n",
      "|60| |59.1| | | | |\n",
      "|49.0|51.1|49.9| |51.6|50.8| |\n",
      "|40|41.3|39.2| |42.0|38.8| |\n",
      "| | | | |35.6| | |\n",
      "|20| | | |20.3| | |\n",
      "| |16.7|16.0| |9.3| | |\n",
      "|0|MMLU-Pro|GPQA-Diamond|MATH 500|AIME 2024|Codeforces|SWE-bench Verified|\n",
      "\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "a116d296-e4a4-4710-a41a-318bbab8ec24",
   "metadata": {},
   "source": [
    "### 3.2、Data Connectors\n",
    "\n",
    "用于处理更丰富的数据类型，并将其读取为 `Document` 的形式。\n",
    "\n",
    "例如：直接读取网页\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19471210-6cb3-4822-8bad-dc701b6ab335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-readers-web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2d73abf3-91df-488c-a5f3-c7de0c6d4c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login\n",
      "\n",
      "__\n",
      "\n",
      "用户登录\n",
      "\n",
      "![](/api/Ajax/vertify/type/users_login.html)\n",
      "\n",
      "[ 登录](javascript:void\\(0\\))\n",
      "\n",
      "[忘记密码?](/user/Users/retrieve_password.html) [立即注册](/reg)\n",
      "\n",
      "快捷登录 | [__](/index.php?m=plugins&c=QqLogin&a=login)[__](/index.php?m=plugins&c=WxLogin&a=login)[__](/index.php?m=plugins&c=Wblogin&a=login)\n",
      "\n",
      "[\n",
      "![聚客AI学院大模型应用开发微调项目实践课程学习平台](https://oss.guangjuke.com/uploads/allimg/20250224/1-250224200331L9.png)\n",
      "](https://edu.guangjuke.com)\n",
      "\n",
      "[首页](https://edu.guangjuke.com)\n",
      "[全部课程](https://edu.guangjuke.com/shipinkecheng/)\n",
      "[资源下载](https://edu.guangjuke.com/ziyuanxiazai/)\n",
      "[系统课](https://edu.guangjuke.com/tx/)\n",
      "[图书馆](https://edu.guangjuke.com/document.html)\n",
      "[精选好文](https://edu.guangjuke.com/haowen/)\n",
      "[聚客社区](https://edu.guangjuke.com/ask.html)\n",
      "[关于我们](https://www.guangjuke.com/about/)\n",
      "\n",
      "[登录/注册](https://edu.guangjuke.com/user)\n",
      "\n",
      "![](/template/pc/static/images/banner-tip-bar.28e923ae.png)\n",
      "\n",
      "# 锤炼前沿实战精华，独创多领域大模型人才培养方案\n",
      "\n",
      "Kevin聚客科技联合创始人/技术总监（CTO）\n",
      "\n",
      "华为高级架构师互联网AI领域专家\n",
      "\n",
      "互联网后端技术领域15年从业经验，曾任职华为、新一代技术研究院，对Open AI、Azure AI、Google AI等大模型有丰富的实战项目经验。\n",
      "\n",
      "Aron人工智能研究院研究员\n",
      "\n",
      "人工智能算法研究员医疗领域AI专家\n",
      "\n",
      "8年深度学习算法研发经验，精通深度学习框架，有丰富的GPU模型加速，移动端模型加速，模型优化，模型部署经验 。\n",
      "\n",
      "BoboAI系列课程的布道者\n",
      "\n",
      "互联网AI技术专家企业高级架构师\n",
      "\n",
      "互联网后端技术领域15年从业经验，曾在知名企业用友、华电和百丽担任要职。历任高级软件开发工程师、系统架构师及首席技术官（CTO）。\n",
      "\n",
      "Anny国家工信部AI认证专家\n",
      "\n",
      "高级Python工程师首批大模型研发者\n",
      "\n",
      "数十年开发经验，深耕大数据、智能体、大模型垂直应用解决方案、AI应用工程等领域。\n",
      "\n",
      "Ray深度人工智能教育创始人\n",
      "\n",
      "高级AI算法工程师深度智谷科技创始人\n",
      "\n",
      "5年人工智能算法领域研发经验，6年人工智能教学经验，具备扎实的人工智能算法理论基础知识和丰富的项目实战经验.\n",
      "\n",
      "Cyber\n",
      "\n",
      "金融大厂架构师互联网连续创业者\n",
      "\n",
      "AI人工智能领域6年从业经验，对Open AI、Azure AI、Google AI、SD AI等AI大模型有丰富的实战项目经验。\n",
      "\n",
      "#### 福利一：高性能GPU资源\n",
      "\n",
      "提供学习阶段进行训练的线上实验室算力资源，帮助学生进行学习阶段的模型训练、练习。\n",
      "\n",
      "#### 福利二：大模型项目资源库\n",
      "\n",
      "超值附赠大量大模型项目资源库，包括丰富的代码示例和数据集，供学员在学习过程中使用和参考，加速学员的项目实践和技能提升。\n",
      "\n",
      "扫码添加专属老师  \n",
      "获取更多福利信息\n",
      "\n",
      "# 六大模块递进式学习，更顺滑、更高效实现大模型能力跃迁\n",
      "\n",
      "Hugging Face核心组件使用\n",
      "\n",
      "模型部署推理\n",
      "\n",
      "Datasets数据工程\n",
      "\n",
      "DeepSpeed分布式训练\n",
      "\n",
      "SFT微调训练\n",
      "\n",
      "模型合并、打包、部署\n",
      "\n",
      "模型量化核心算法与最佳实践\n",
      "\n",
      "模型蒸馏原理及动手实践\n",
      "\n",
      "模型评估方法和最佳实践\n",
      "\n",
      "项目  \n",
      "场景\n",
      "\n",
      "1、基于 Bert 的中文评价情感分析（分类任务）      2、定制化模型输出（生成任务）      3、基于特定数据集训练情绪对话模型\n",
      "\n",
      "RAG工程化\n",
      "\n",
      "Embedding Models嵌入模型原理\n",
      "\n",
      "Vector Store向量存储\n",
      "\n",
      "LlamaIndex框架深度应用\n",
      "\n",
      "Dify LLMOps\n",
      "\n",
      "RAG方案最佳实践\n",
      "\n",
      "项目  \n",
      "场景\n",
      "\n",
      "1、基于 DeepSeek + Dify 快速构建私有知识库      2、法律助手 - 基于 LlamaIndex + Chroma 构建法律条文助手\n",
      "\n",
      "智能体原理深度剖析\n",
      "\n",
      "强化学习\n",
      "\n",
      "Dify Agent应用\n",
      "\n",
      "LangGraph 框架深度学习\n",
      "\n",
      "项目  \n",
      "场景\n",
      "\n",
      "1、基于 Dify 快速构建智能体应用      2、基于 LangGraph 构建企业级复杂多代理应用\n",
      "\n",
      "深入理解DeepSeek设计思想和训练过程\n",
      "\n",
      "DeepSeek本地部署，多卡联合部署，vLLM多卡推理\n",
      "\n",
      "DeepSeek微调训练/多卡训练\n",
      "\n",
      "解锁  \n",
      "技能\n",
      "\n",
      "掌握 DeepSeek 本地部署及企业落地场景\n",
      "\n",
      "模态与多模态的概念\n",
      "\n",
      "多模态机器学习与典型任务\n",
      "\n",
      "本地私有化部署图文描述模型\n",
      "\n",
      "本地私有化部署文生视频模型\n",
      "\n",
      "本地部署 Llama-3.2-11B-Vision-Instruct-GGUF 实现视觉问答\n",
      "\n",
      "解锁  \n",
      "技能\n",
      "\n",
      "掌握多模态大模型解决典型任务：跨模态预训练/Language-Audio / Vision-Audio / Vision-Language/定位相关任务  \n",
      "Affect Computing 情感计算/Medical Image 医疗图像模态\n",
      "\n",
      "多套高薪Offer的AI大模型简历分享及参考\n",
      "\n",
      "简历项目1对1个性化指导\n",
      "\n",
      "技术模拟面试1对1指导\n",
      "\n",
      "解锁  \n",
      "技能\n",
      "\n",
      "高效写出高含金量的AI技术简历 / 项目场景模拟面试\n",
      "\n",
      "# 场景化深度落地实践，多重维度构建能力模型\n",
      "\n",
      "![](/template/pc/static/images/shizhan1.png)\n",
      "\n",
      "  * 基于LlamaIndex构建企业私有知识库（RAG项目） \n",
      "  * 基于Bert的中文评价情感分析（分类任务） \n",
      "  * 中文生成模型定制化（生成任务） \n",
      "  * 基于本地大模型的在线心理问诊系统（微调项目） \n",
      "  * 企业招标采购智能客服系统（RAG+微调项目） \n",
      "  * 基于YOLO的骨龄识别项目（视觉项目） \n",
      "  * 基于RAG的法律咨询智能助手 \n",
      "\n",
      "# 五大全方位闭环硬核服务，为你的学习和面试保驾护航\n",
      "\n",
      "#### 大家说好，才是真的好\n",
      "\n",
      "# 无论你是转行、进阶，都是你的不二之选\n",
      "\n",
      "AI大模型工程师专注于LLM领域的工程师，渴望深入探索大模型的高级应用和解决方案设计。\n",
      "\n",
      "NLP算法工程师寻求LLM实战经验，提升专业技能，增强大厂面试竞争力。\n",
      "\n",
      "AI算法工程师希望迅速掌握LLM技术，实现职业转型或深化AI领域技能。\n",
      "\n",
      "IT转行求职者对现有IT职业不满，寻求向AI领域转型的求职者，寻找技术转型的新起点，快速积累LLM实战经验。\n",
      "\n",
      "在职提升者在职工程师，希望提升LLM技能，提高工作效率，为职业晋升打下坚实基础。\n",
      "\n",
      "计算机应届毕业生计算机专业师生和应届毕业生，追求快速掌握LLM技能，增强就业竞争力。\n",
      "\n",
      "# 学前技术储备\n",
      "\n",
      "具备良好的编程能力，熟悉Python语言\n",
      "\n",
      "了解机器学习和深度学习的基本概念\n",
      "\n",
      "对大模型算法有初步的了解和兴趣\n",
      "\n",
      "帮助与常见问题\n",
      "\n",
      "Q：是否有基础要求？\n",
      "\n",
      "掌握基本的 Python 编程技能即可学习。\n",
      "\n",
      "Q：是否有详细的课程表？\n",
      "\n",
      "有的哦，可以联系官方客服领取。\n",
      "\n",
      "Q：上课形式和课时量是怎样的呢？\n",
      "\n",
      "课程为全程直播。课时2个月左右\n",
      "\n",
      "Q：直播是否有回看？\n",
      "\n",
      "直播的录播视频会上传到官网方便大家回看，但为了更好的学习互动效果，建议各位学员提前预留好时间，准备好问题，准时参加直播。\n",
      "\n",
      "Q: 课程录播视频的观看期限是多久？\n",
      "\n",
      "一年有效学习权益\n",
      "\n",
      "Q：可以跟老师互动交流吗？\n",
      "\n",
      "当然啦，我们会建立班级社群，群内可以互动交流。同时，大家还可以通过直播向老师提问。\n",
      "\n",
      "Q：报名缴费后可以退款吗？\n",
      "\n",
      "付款后 3 个自然日内，如果觉得课程不适合自己，可申请退款，超出 3 个自然日，就不再办理退款啦。退款流程预计为 10 个工作日。\n",
      "\n",
      "Q：可以分期付款吗？\n",
      "\n",
      "我们支持花呗信用卡分期付款。\n",
      "\n",
      "Q: 如何开发票，签合同？\n",
      "\n",
      "我们可以为学员开具正规的发票和合同。开发票相关事宜，请联系带班班主任。合同相关事宜，请联系报名老师。\n",
      "\n",
      "大厂标准培训\n",
      "\n",
      "海量精品课程\n",
      "\n",
      "汇聚优秀团队\n",
      "\n",
      "打造完善体系\n",
      "\n",
      "![](/template/pc/skin/images/cxun.gif) ![](/template/pc/skin/images/cxun2.gif)\n",
      "![](/template/pc/skin/images/cxun3.gif)\n",
      "\n",
      "Copyright © 2023-2025 聚客AI 版权所有  \n",
      "网站备案号：[湘ICP备2024094305号-1](https://beian.miit.gov.cn/)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://edu.guangjuke.com/tx/\"]\n",
    ")\n",
    "\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a88893-01b8-44a1-9bed-e93e60cec328",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>更多 Data Connectors</b>\n",
    "    <ul>\n",
    "        <li>内置的<a href=\"https://llamahub.ai/l/readers/llama-index-readers-file\">文件加载器</a></li>\n",
    "        <li>连接三方服务的<a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/modules/\">数据加载器</a>，例如数据库</li>\n",
    "        <li>更多加载器可以在 <a href=\"https://llamahub.ai/\">LlamaHub</a> 上找到</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed8a42-f6fc-430f-9e92-07736e7f359c",
   "metadata": {},
   "source": [
    "## 4. 文本切分与解析（Chunking）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad7ebd-9e56-47f9-8136-2e1098139c01",
   "metadata": {},
   "source": [
    "为方便检索，我们通常把 `Document` 切分为 `Node`。\n",
    "\n",
    "在 LlamaIndex 中，`Node` 被定义为一个文本的「chunk」。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881c7d9-9107-4704-b322-d3bc34af96f2",
   "metadata": {},
   "source": [
    "### 4.1、使用 TextSplitters 对文本做切分\n",
    "\n",
    "例如：`TokenTextSplitter` 按指定 token 数切分文本\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "69a083f7-cda9-45d9-be3e-397ce866e440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:53:26.157643Z",
     "start_time": "2025-05-20T10:53:26.142369Z"
    }
   },
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "node_parser = TokenTextSplitter(\n",
    "    chunk_size=512,  # 每个 chunk 的最大长度\n",
    "    chunk_overlap=200  # chunk 之间重叠长度\n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(\n",
    "    documents, show_progress=False\n",
    ")\n",
    "len(nodes)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:53:28.569162Z",
     "start_time": "2025-05-20T10:53:28.566843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "show_json(nodes[1].json())\n",
    "show_json(nodes[2].json())"
   ],
   "id": "81b363de-c5ef-4e20-ad4d-39bccabb3369",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id_\": \"4d105334-4581-4c59-86a2-c6485667fd02\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {\n",
      "        \"file_path\": \"/Users/wangweijun/PycharmProjects/ai-learn/ai-level-1/第4章_LlamaIndex知识管理与信息检索/data/deepseek-v3-1-4.pdf\",\n",
      "        \"file_name\": \"deepseek-v3-1-4.pdf\",\n",
      "        \"file_type\": \"application/pdf\",\n",
      "        \"file_size\": 192218,\n",
      "        \"creation_date\": \"2025-05-19\",\n",
      "        \"last_modified_date\": \"2025-03-12\"\n",
      "    },\n",
      "    \"excluded_embed_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"excluded_llm_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"relationships\": {\n",
      "        \"1\": {\n",
      "            \"node_id\": \"66d4000d-24ad-4bdd-a908-52167f2b8038\",\n",
      "            \"node_type\": \"4\",\n",
      "            \"metadata\": {\n",
      "                \"file_path\": \"/Users/wangweijun/PycharmProjects/ai-learn/ai-level-1/第4章_LlamaIndex知识管理与信息检索/data/deepseek-v3-1-4.pdf\",\n",
      "                \"file_name\": \"deepseek-v3-1-4.pdf\",\n",
      "                \"file_type\": \"application/pdf\",\n",
      "                \"file_size\": 192218,\n",
      "                \"creation_date\": \"2025-05-19\",\n",
      "                \"last_modified_date\": \"2025-03-12\"\n",
      "            },\n",
      "            \"hash\": \"cd50a4c8fb6e8eca1d642700222618bbf1cce09c03977cc29bd4289122caecf7\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        },\n",
      "        \"2\": {\n",
      "            \"node_id\": \"0811c0be-3da8-421f-9da7-3afbd2c536bd\",\n",
      "            \"node_type\": \"1\",\n",
      "            \"metadata\": {\n",
      "                \"file_path\": \"/Users/wangweijun/PycharmProjects/ai-learn/ai-level-1/第4章_LlamaIndex知识管理与信息检索/data/deepseek-v3-1-4.pdf\",\n",
      "                \"file_name\": \"deepseek-v3-1-4.pdf\",\n",
      "                \"file_type\": \"application/pdf\",\n",
      "                \"file_size\": 192218,\n",
      "                \"creation_date\": \"2025-05-19\",\n",
      "                \"last_modified_date\": \"2025-03-12\"\n",
      "            },\n",
      "            \"hash\": \"844ae33c9af8e7c8e7a5f6b31e3b03dd9fdf8adb467f08225b5a1d6047c8d370\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        }\n",
      "    },\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text\": \"|\\n|---|---|---|---|---|---|---|\\n|100|90.2| | | | | |\\n|80|75.9|71.6|73.3|72.6|78.0| |\\n| |74.7|80.0|73.8|74.6|78.3| |\\n|66.2| |65.0| | | | |\\n|60| |59.1| | | | |\\n|49.0|51.1|49.9| |51.6|50.8| |\\n|40|41.3|39.2| |42.0|38.8| |\\n| | | | |35.6| | |\\n|20| | | |20.3| | |\\n| |16.7|16.0| |9.3| | |\\n|0|MMLU-Pro|GPQA-Diamond|MATH 500|AIME 2024|Codeforces|SWE-bench Verified|\\n\\nFigure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\",\n",
      "    \"mimetype\": \"text/plain\",\n",
      "    \"start_char_idx\": 1372,\n",
      "    \"end_char_idx\": 1798,\n",
      "    \"metadata_seperator\": \"\\n\",\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"TextNode\"\n",
      "}\n",
      "{\n",
      "    \"id_\": \"ce0f87aa-f65f-4ed0-a1c4-c3cae3eb26bf\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {\n",
      "        \"file_path\": \"/Users/wangweijun/PycharmProjects/ai-learn/ai-level-1/第4章_LlamaIndex知识管理与信息检索/data/deepseek-v3-1-4.pdf\",\n",
      "        \"file_name\": \"deepseek-v3-1-4.pdf\",\n",
      "        \"file_type\": \"application/pdf\",\n",
      "        \"file_size\": 192218,\n",
      "        \"creation_date\": \"2025-05-19\",\n",
      "        \"last_modified_date\": \"2025-03-12\"\n",
      "    },\n",
      "    \"excluded_embed_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"excluded_llm_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"relationships\": {\n",
      "        \"1\": {\n",
      "            \"node_id\": \"64c4e893-9c72-4862-9d0a-b52b0b9eac1b\",\n",
      "            \"node_type\": \"4\",\n",
      "            \"metadata\": {\n",
      "                \"file_path\": \"/Users/wangweijun/PycharmProjects/ai-learn/ai-level-1/第4章_LlamaIndex知识管理与信息检索/data/deepseek-v3-1-4.pdf\",\n",
      "                \"file_name\": \"deepseek-v3-1-4.pdf\",\n",
      "                \"file_type\": \"application/pdf\",\n",
      "                \"file_size\": 192218,\n",
      "                \"creation_date\": \"2025-05-19\",\n",
      "                \"last_modified_date\": \"2025-03-12\"\n",
      "            },\n",
      "            \"hash\": \"ef158d74701492864e309de20bcbe65f7d9ec39dc2c81a758a37d328152b7727\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        },\n",
      "        \"3\": {\n",
      "            \"node_id\": \"3bb5ba56-a508-4498-8e2f-3f9c9de50b6c\",\n",
      "            \"node_type\": \"1\",\n",
      "            \"metadata\": {},\n",
      "            \"hash\": \"c82a67df26473e6046ac82c4e2d161785874c7def13ed67adadc6777735c04bc\",\n",
      "            \"class_name\": \"RelatedNodeInfo\"\n",
      "        }\n",
      "    },\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text\": \"# 1. Introduction\\n\\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Beyond closed-source models, open-source models, including DeepSeek series (DeepSeek-AI, 2024a,b,c; Guo et al., 2024), LLaMA series (AI@Meta, 2024a,b; Touvron et al., 2023a,b), Qwen series (Qwen, 2023, 2024a,b), and Mistral series (Jiang et al., 2023; Mistral, 2024), are also making significant strides, endeavoring to close the gap with their closed-source counterparts. To further push the boundaries of open-source model capabilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token.\\n\\nWith a forward-looking perspective, we consistently strive for strong model performance and economical costs. Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for cost-effective training. These two architectures have been validated in DeepSeek-V2 (DeepSeek-AI, 2024c), demonstrating their capability to maintain robust model performance while achieving efficient training and inference. Beyond the basic architecture, we implement two additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy (Wang et al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. Secondly, DeepSeek-V3 employs a multi-token prediction training objective, which we have observed to enhance the overall performance on evaluation benchmarks.\\n\\nIn order to achieve efficient training, we support the FP8 mixed precision training and implement comprehensive optimizations for the training framework. Low-precision training has emerged as a\",\n",
      "    \"mimetype\": \"text/plain\",\n",
      "    \"start_char_idx\": 0,\n",
      "    \"end_char_idx\": 2046,\n",
      "    \"metadata_seperator\": \"\\n\",\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"TextNode\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "d1f56dbf-4247-4a06-a127-2fa1929455f0",
   "metadata": {},
   "source": [
    "LlamaIndex 提供了丰富的 `TextSplitter`，例如：\n",
    "\n",
    "- [`SentenceSplitter`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/)：在切分指定长度的 chunk 同时尽量保证句子边界不被切断；\n",
    "- [`CodeSplitter`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/code/)：根据 AST（编译器的抽象句法树）切分代码，保证代码功能片段完整；\n",
    "- [`SemanticSplitterNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/semantic_splitter/)：根据语义相关性对将文本切分为片段。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692f9fb-af99-4bf5-9d4e-c745438173d6",
   "metadata": {},
   "source": [
    "### 4.2、使用 NodeParsers 对有结构的文档做解析\n",
    "\n",
    "例如：`HTMLNodeParser`解析 HTML 文档\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6f533-a203-42a2-b312-3d89e0cbf22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import HTMLNodeParser\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=False).load_data(\n",
    "    [\"https://edu.guangjuke.com/tx/\"]\n",
    ")\n",
    "\n",
    "# 默认解析 [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"]\n",
    "parser = HTMLNodeParser(tags=[\"span\"])  # 可以自定义解析哪些标签\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.text+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc613c-36a8-4afb-871b-097496703eaf",
   "metadata": {},
   "source": [
    "更多的 `NodeParser` 包括 [`MarkdownNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/markdown/)，[`JSONNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/json/)等等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8557f-20af-477d-918d-14761a9c986d",
   "metadata": {},
   "source": [
    "## 5. 索引（Indexing）与检索（Retrieval）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df338b16-df37-412d-a385-2d1f4b681112",
   "metadata": {},
   "source": [
    "**基础概念**：在「检索」相关的上下文中，「索引」即`index`， 通常是指为了实现快速检索而设计的特定「数据结构」。\n",
    "\n",
    "索引的具体原理与实现不是本课程的教学重点，感兴趣的同学可以参考：[传统索引](https://en.wikipedia.org/wiki/Search_engine_indexing)、[向量索引](https://medium.com/kx-systems/vector-indexing-a-roadmap-for-vector-databases-65866f07daf5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd397fe-8932-49de-ac37-bed0b585205c",
   "metadata": {},
   "source": [
    "### 5.1、向量检索\n",
    "\n",
    "1. `VectorStoreIndex` 直接在内存中构建一个 Vector Store 并建索引\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ea17e80-d25c-43ac-b9b5-983c6acb3adb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:01:12.508888Z",
     "start_time": "2025-05-20T11:01:11.807451Z"
    }
   },
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import TokenTextSplitter, SentenceSplitter\n",
    "\n",
    "# 加载 pdf 文档\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\", \n",
    "    required_exts=[\".pdf\"],\n",
    ").load_data()\n",
    "\n",
    "# 定义 Node Parser\n",
    "node_parser = TokenTextSplitter(chunk_size=512, chunk_overlap=200)\n",
    "\n",
    "# 切分文档\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# 构建 index，默认是在内存中\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# 另外一种实现方式\n",
    "# index = VectorStoreIndex.from_documents(documents=documents, transformations=[SentenceSplitter(chunk_size=512)])\n",
    "\n",
    "# 写入本地文件\n",
    "# index.storage_context.persist(persist_dir=\"./doc_emb\")\n",
    "\n",
    "# 获取 retriever\n",
    "vector_retriever = index.as_retriever(\n",
    "    similarity_top_k=2 # 返回2个结果\n",
    ")\n",
    "\n",
    "# 检索\n",
    "results = vector_retriever.retrieve(\"deepseek v3数学能力怎么样？\")\n",
    "\n",
    "print(results[0].text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\n",
      "reasoning performance. Meanwhile, we also maintain control over the output style and\n",
      "length of DeepSeek-V3.\n",
      "Summary of Core Evaluation Results\n",
      "• Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\n",
      "DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9\n",
      "on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source\n",
      "models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\n",
      "and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\n",
      "demonstrates superior performance among open-source models on both SimpleQA and\n",
      "Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual\n",
      "knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge.\n",
      "• Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "90ebec20-7c12-4d2d-a4f4-5cb2abcc5f32",
   "metadata": {},
   "source": [
    "2. 使用自定义的 Vector Store，以 `Qdrant` 为例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e7648-0598-4df7-923f-42fe2f172da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "id": "a65a38da-3e71-4fa8-872d-eff6b4f3856b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:03:02.305716Z",
     "start_time": "2025-05-20T11:03:01.087764Z"
    }
   },
   "source": [
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "collection_name = \"demo\"\n",
    "collection = client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "# storage: 指定存储空间\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# 创建 index：通过 Storage Context 关联到自定义的 Vector Store\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "\n",
    "# 获取 retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=1)\n",
    "\n",
    "# 检索\n",
    "results = vector_retriever.retrieve(\"deepseek v3数学能力怎么样\")\n",
    "\n",
    "print(results[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 8f52a139-d168-4b48-8f42-3657b8b60523\n",
      "Text: verification and reflection patterns of R1 into DeepSeek-V3 and\n",
      "notably improves its reasoning performance. Meanwhile, we also\n",
      "maintain control over the output style and length of DeepSeek-V3.\n",
      "Summary of Core Evaluation Results • Knowledge: (1) On educational\n",
      "benchmarks such as MMLU, MMLU-Pro, and GPQA, DeepSeek-V3 outperforms\n",
      "all other open-sou...\n",
      "Score:  0.681\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "9913e5d5-7c84-4833-b21d-2fe440749a63",
   "metadata": {},
   "source": [
    "### 5.2、更多索引与检索方式\n",
    "\n",
    "LlamaIndex 内置了丰富的检索机制，例如：\n",
    "\n",
    "- 关键字检索\n",
    "\n",
    "  - [`BM25Retriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/bm25/)：基于 tokenizer 实现的 BM25 经典检索算法\n",
    "  - [`KeywordTableGPTRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableGPTRetriever)：使用 GPT 提取检索关键字\n",
    "  - [`KeywordTableSimpleRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableSimpleRetriever)：使用正则表达式提取检索关键字\n",
    "  - [`KeywordTableRAKERetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableRAKERetriever)：使用[`RAKE`](https://pypi.org/project/rake-nltk/)算法提取检索关键字（有语言限制）\n",
    "\n",
    "- RAG-Fusion [`QueryFusionRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/query_fusion/)\n",
    "\n",
    "- 还支持 [KnowledgeGraph](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/knowledge_graph/)、[SQL](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.SQLRetriever)、[Text-to-SQL](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.NLSQLRetriever) 等等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53e99b-2ab5-4520-ba96-4a9979a94480",
   "metadata": {},
   "source": [
    "### 5.3、检索后处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c53c7c-9077-42bc-a5c0-832688b352b8",
   "metadata": {},
   "source": [
    "LlamaIndex 的 `Node Postprocessors` 提供了一系列检索后处理模块。\n",
    "\n",
    "例如：我们可以用不同模型对检索后的 `Nodes` 做重排序\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7176f29c-be6b-491c-b2e8-6e604ad201b4",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T11:05:00.064348Z",
     "start_time": "2025-05-20T11:04:59.823619Z"
    }
   },
   "source": [
    "# 获取 retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "# 检索\n",
    "nodes = vector_retriever.retrieve(\"deepseek v3有多少参数?\")\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"[{i}] {node.text}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment strategy, and our suggestions on future hardware design. Next, we describe our\n",
      "pre-training process, including the construction of training data, hyper-parameter settings, long-\n",
      "context extension techniques, the associated evaluations, as well as some discussions (Section 4).\n",
      "Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\n",
      "Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\n",
      "we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\n",
      "directions for future research (Section 6).\n",
      "2. Architecture\n",
      "We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\n",
      "tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\n",
      "for economical training. Then, we present a Multi-Token Prediction (MTP) training objective,\n",
      "which we have observed to enhance the overall performance on evaluation benchmarks. For\n",
      "other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeek-\n",
      "V2 (DeepSeek-AI, 2024c).\n",
      "2.1. Basic Architecture\n",
      "The basic architecture of\n",
      "\n",
      "[1] on post-training, which include Supervised Fine-Tuning (SFT),\n",
      "Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\n",
      "we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\n",
      "directions for future research (Section 6).\n",
      "2. Architecture\n",
      "We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\n",
      "tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\n",
      "for economical training. Then, we present a Multi-Token Prediction (MTP) training objective,\n",
      "which we have observed to enhance the overall performance on evaluation benchmarks. For\n",
      "other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeek-\n",
      "V2 (DeepSeek-AI, 2024c).\n",
      "2.1. Basic Architecture\n",
      "The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017)\n",
      "framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA\n",
      "and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with\n",
      "DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing\n",
      "6\n",
      "\n",
      "[2] verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\n",
      "reasoning performance. Meanwhile, we also maintain control over the output style and\n",
      "length of DeepSeek-V3.\n",
      "Summary of Core Evaluation Results\n",
      "• Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\n",
      "DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9\n",
      "on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source\n",
      "models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\n",
      "and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\n",
      "demonstrates superior performance among open-source models on both SimpleQA and\n",
      "Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual\n",
      "knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge.\n",
      "• Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment\n",
      "\n",
      "[3] DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "\n",
      "[4] costly tensor parallelism.\n",
      "Combining these efforts, we achieve high training efficiency.\n",
      "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The\n",
      "pre-training process is remarkably stable. Throughout the entire training process, we did not\n",
      "encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage\n",
      "context length extension for DeepSeek-V3. In the first stage, the maximum context length is\n",
      "extended to 32K, and in the second stage, it is further extended to 128K. Following this, we\n",
      "conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)\n",
      "on the base model of DeepSeek-V3, to align it with human preferences and further unlock its\n",
      "potential. During the post-training stage, we distill the reasoning capability from the DeepSeek-\n",
      "R1 series of models, and meanwhile carefully maintain the balance between model accuracy\n",
      "4\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "65bae2b4-7c36-44fa-9566-0d1b3f34972a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:07:57.022093Z",
     "start_time": "2025-05-20T11:07:50.226186Z"
    }
   },
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "# 重排序（默认使用 Settings.llm 的模型）\n",
    "postprocessor = LLMRerank(top_n=2)\n",
    "\n",
    "nodes = postprocessor.postprocess_nodes(nodes, query_str=\"deepseek v3有多少参数?\")\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"[{i}] {node.text}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "69ca41ea-3112-491a-a1a1-5ec8db295b61",
   "metadata": {},
   "source": "更多的 Rerank 及其它后处理方法，参考官方文档：[Node Postprocessor Modules](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/#llm-rerank)\n"
  },
  {
   "cell_type": "markdown",
   "id": "392d8d69-cd42-445c-a0cd-0dd7c66d07bc",
   "metadata": {},
   "source": [
    "## 6. 生成回复（QA & Chat）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4248c1-602c-4df6-bd6a-72e994faf1ed",
   "metadata": {},
   "source": [
    "### 6.1 单轮问答（Query Engine）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "261d35c3-8b54-4840-ab88-c04b348b0fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3在数学相关基准测试中表现出色，特别是在非长链思维开放源码和闭源模型中处于领先地位。它在某些特定的基准测试如MATH-500上甚至超过了o1-preview，这表明它具有强大的数学推理能力。\n"
     ]
    }
   ],
   "source": [
    "qa_engine = index.as_query_engine()\n",
    "response = qa_engine.query(\"deepseek v3数学能力怎么样?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d55521-5b67-4c26-a8de-440d2e1046a7",
   "metadata": {},
   "source": [
    "#### 流式输出\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1817400e-9311-4161-89e4-507267862524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:43:06.068002Z",
     "start_time": "2025-05-20T11:43:00.262222Z"
    }
   },
   "source": [
    "qa_engine = index.as_query_engine(streaming=True)\n",
    "response = qa_engine.query(\"deepseek v3数学能力怎么样?\")\n",
    "response.print_response_stream()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3在数学相关基准测试中表现出色，特别是在非长链思维开放源码和闭源模型中处于领先地位。它在某些特定的基准测试如MATH-500上甚至超过了o1-preview，这表明它具有强大的数学推理能力。"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "1cf53205-beb2-4571-81aa-c5f8e12142f5",
   "metadata": {},
   "source": [
    "### 6.2 多轮对话（Chat Engine）\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "81a73a48-469e-4d40-adf3-e0e5ec44305a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T12:08:35.550281Z",
     "start_time": "2025-05-20T12:08:23.893960Z"
    }
   },
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "response = chat_engine.chat(\"deepseek v3数学能力怎么样?\")\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3在数学相关基准测试中表现出色，特别是在非长链思维开放源码和闭源模型中处于领先地位。它在某些特定的基准测试如MATH-500上甚至超过了o1-preview，这表明它具有强大的数学推理能力。\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "712c8c71-6420-4c6f-821c-1bd1c31dc8bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T12:08:53.438144Z",
     "start_time": "2025-05-20T12:08:38.464218Z"
    }
   },
   "source": [
    "response = chat_engine.chat(\"代码能力呢?\")\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3在编码相关任务上表现出色，特别是在编程竞赛基准测试如LiveCodeBench中，它成为了表现最好的模型。对于工程相关的任务，虽然DeepSeek-V3的表现略低于Claude-Sonnet-3.5，但它仍然显著优于其他所有模型，显示出其在多种技术基准测试中的竞争力。\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "3094d188-2241-4995-9704-1d2aeb878e1e",
   "metadata": {},
   "source": [
    "#### 流式输出\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf33effa-ad36-4eea-b1d1-926ca5cc8dff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T12:09:32.341526Z",
     "start_time": "2025-05-20T12:09:23.522135Z"
    }
   },
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "streaming_response = chat_engine.stream_chat(\"deepseek v3数学能力怎么样?\")\n",
    "# streaming_response.print_response_stream()\n",
    "for token in streaming_response.response_gen:\n",
    "    print(token, end=\"\", flush=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3在数学相关基准测试中表现出色，特别是在非长链思维开放源码和闭源模型中处于领先地位。它在某些特定的基准测试如MATH-500上甚至超过了o1-preview的表现，这证明了其强大的数学推理能力。"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "d7eb76fa-e4ba-4c47-9785-53eadddf478e",
   "metadata": {},
   "source": [
    "## 7. 底层接口：Prompt、LLM 与 Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cce84-9f79-41ef-a6c4-caaeb70d029e",
   "metadata": {},
   "source": [
    "### 7.1 Prompt 模板\n",
    "\n",
    "#### `PromptTemplate` 定义提示词模板\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "41358d83-4fcb-430c-9c2e-c1ac0839cbfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T12:09:38.963796Z",
     "start_time": "2025-05-20T12:09:38.961215Z"
    }
   },
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\"写一个关于{topic}的笑话\")\n",
    "\n",
    "prompt.format(topic=\"小明\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'写一个关于小明的笑话'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "a4a94232-e8d5-4bf2-8751-8851d924fcee",
   "metadata": {},
   "source": [
    "#### `ChatPromptTemplate` 定义多轮消息模板\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "edb05592-835f-4cb2-bfa0-862796c36fdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T12:13:15.467114Z",
     "start_time": "2025-05-20T12:13:15.464456Z"
    }
   },
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "chat_text_qa_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=\"你叫{name}，你必须根据用户提供的上下文回答问题。\",\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER, \n",
    "        content=(\n",
    "            \"已知上下文：\\n\" \\\n",
    "            \"{context}\\n\\n\" \\\n",
    "            \"问题：{question}\"\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n",
    "\n",
    "print(\n",
    "    text_qa_template.format(\n",
    "        name=\"小明\",\n",
    "        context=\"这是一个测试\",\n",
    "        question=\"这是什么\"\n",
    "    )\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: 你叫小明，你必须根据用户提供的上下文回答问题。\n",
      "user: 已知上下文：\n",
      "这是一个测试\n",
      "\n",
      "问题：这是什么\n",
      "assistant: \n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "c98931d9-f411-4433-9900-649f74a40b6e",
   "metadata": {},
   "source": [
    "### 7.2 语言模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1febbfc2-06c1-4692-9ac1-8843e5554098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d3842-32ff-4510-9853-47e7282b56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(prompt.format(topic=\"小明\"))\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbe1cc-9dfc-4274-8cdd-b4dd6d3eb5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = llm.complete(\n",
    "    text_qa_template.format(\n",
    "        name=\"小明\",\n",
    "        context=\"这是一个测试\",\n",
    "        question=\"你是谁，我们在干嘛\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234c43f-d46c-4755-afb3-ade10c308bf8",
   "metadata": {},
   "source": [
    "#### 连接DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92e176-6153-4059-a01b-e2957a052a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-llms-deepseek"
   ]
  },
  {
   "cell_type": "code",
   "id": "511a86bd-c5eb-45e7-8e0a-7e56b2fe7f7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:39:04.405029Z",
     "start_time": "2025-05-20T16:38:54.788557Z"
    }
   },
   "source": [
    "import os\n",
    "from llama_index.llms.deepseek import DeepSeek\n",
    "\n",
    "llm = DeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"), temperature=1.5)\n",
    "\n",
    "response = llm.complete(\"写个笑话\")\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《AI的烦恼》  \n",
      "\n",
      "程序员给AI布置新任务：“写个笑话，要让人笑到代码都乱了。”  \n",
      "\n",
      "AI认真检索了8TB的幽默数据库，输出了30页的《人类笑点行为分析报告》。  \n",
      "\n",
      "程序员叹气：“算了，还是我自己讲吧……‘为什么程序员总分不清万圣节和圣诞节？’”  \n",
      "\n",
      "AI立刻抢答：“因为Oct 31 == Dec 25！（八进制31等于十进制25）”  \n",
      "\n",
      "程序员沉默两秒，突然把键盘一推：“这才是真恐怖故事——你连谐音梗都学会了！”\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "2f49f6da-d220-4c36-b697-e3b525902af7",
   "metadata": {},
   "source": [
    "#### 设置全局使用的语言模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94943bec-563c-44b5-80bb-9c0a7c05382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = DeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"), temperature=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd7b76-2594-4c39-8e81-33e3c985df38",
   "metadata": {},
   "source": [
    "除 OpenAI 外，LlamaIndex 已集成多个大语言模型，包括云服务 API 和本地部署 API，详见官方文档：[Available LLM integrations](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110fc0b-ec6c-445b-a3c2-3ddef41d9589",
   "metadata": {},
   "source": [
    "### 7.3 Embedding 模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5be1257-eb1a-4e2d-bad9-2cd002841093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# 全局设定\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb025ce6-c065-4cf2-9e9d-5a01f04c26ee",
   "metadata": {},
   "source": [
    "LlamaIndex 同样集成了多种 Embedding 模型，包括云服务 API 和开源模型（HuggingFace）等，详见[官方文档](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be91aa7d-6f3a-4387-859c-c7357d7c5d15",
   "metadata": {},
   "source": [
    "## 8. 基于 LlamaIndex 实现一个功能较完整的 RAG 系统\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1b844-605e-4be2-9cae-2e9e56e46b40",
   "metadata": {},
   "source": [
    "功能要求：\n",
    "\n",
    "- 加载指定目录的文件\n",
    "- 支持 RAG-Fusion\n",
    "- 使用 Qdrant 向量数据库，并持久化到本地\n",
    "- 支持检索后排序\n",
    "- 支持多轮对话\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "90348baf-3a5e-4cc4-968d-e90f9d315495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:35:21.664172Z",
     "start_time": "2025-05-20T16:35:21.654026Z"
    }
   },
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "EMBEDDING_DIM = 1536\n",
    "COLLECTION_NAME = \"full_demo\"\n",
    "PATH = \"./qdrant_db\"\n",
    "\n",
    "client = QdrantClient(path=PATH)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "cf485f1b-7da4-412e-a121-a7e6b5f3ece9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T17:38:31.798836Z",
     "start_time": "2025-05-20T17:38:29.738317Z"
    }
   },
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, get_response_synthesizer\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.postprocessor import LLMRerank, SimilarityPostprocessor\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.llms.dashscope import DashScope, DashScopeGenerationModels\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels\n",
    "\n",
    "# 1. 指定全局llm与embedding模型\n",
    "Settings.llm = DashScope(model_name=DashScopeGenerationModels.QWEN_MAX,api_key=os.getenv(\"DASHSCOPE_API_KEY\"))\n",
    "Settings.embed_model = DashScopeEmbedding(model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1)\n",
    "\n",
    "# 2. 指定全局文档处理的 Ingestion Pipeline\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=512, chunk_overlap=200)]\n",
    "\n",
    "# 3. 加载本地文档\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "if client.collection_exists(collection_name=COLLECTION_NAME):\n",
    "    client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "# 4. 创建 collection\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# 5. 创建 Vector Store\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "# 6. 指定 Vector Store 的 Storage 用于 index\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# 7. 定义检索后排序模型\n",
    "reranker = LLMRerank(top_n=2)\n",
    "# 最终打分低于 0.6 的文档被过滤掉\n",
    "sp = SimilarityPostprocessor(similarity_cutoff=0.6)\n",
    "\n",
    "# 8. 定义 RAG Fusion 检索器（查询改写）\n",
    "fusion_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever()],\n",
    "    similarity_top_k=5, # 检索召回 top k 结果\n",
    "    num_queries=3,  # 生成 query 数\n",
    "    use_async=False,\n",
    "    # query_gen_prompt=\"\",  # 可以自定义 query 生成的 prompt 模板\n",
    ")\n",
    "\n",
    "# 9. 构建单轮 query engine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    fusion_retriever,\n",
    "    node_postprocessors=[reranker, sp],\n",
    "    response_synthesizer=get_response_synthesizer(\n",
    "        response_mode = ResponseMode.REFINE\n",
    "    )\n",
    ")\n",
    "\n",
    "# 10. 对话引擎\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine, \n",
    "    # condense_question_prompt=\"\" # 可以自定义 chat message prompt 模板\n",
    ")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mllama_index\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01membeddings\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdashscope\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DashScopeEmbedding, DashScopeTextEmbeddingModels\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# 1. 指定全局llm与embedding模型\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m Settings.llm = DashScope(model_name=DashScopeGenerationModels.QWEN_MAX,api_key=\u001B[43mos\u001B[49m.getenv(\u001B[33m\"\u001B[39m\u001B[33mDASHSCOPE_API_KEY\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m     17\u001B[39m Settings.embed_model = DashScopeEmbedding(model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1)\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# 2. 指定全局文档处理的 Ingestion Pipeline\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'os' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "0124ef05-eb4b-417b-aa56-7128460162dd",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T17:06:47.193410Z",
     "start_time": "2025-05-20T17:06:21.305429Z"
    }
   },
   "source": [
    "from llama_index.core.schema import MetadataMode, QueryBundle\n",
    "\n",
    "# 测试多轮对话\n",
    "# User: deepseek v3有多少参数\n",
    "# User: 每次激活多少\n",
    "while True:\n",
    "    question=input(\"User:\")\n",
    "    if question.strip() == \"q\":\n",
    "        break\n",
    "    # 现实文档来源\n",
    "    # contexts = query_engine.retrieve(QueryBundle(question))\n",
    "    # print('-' * 10 + 'ref' + '-' * 10)\n",
    "    # for i, context in enumerate(contexts):\n",
    "    #     print('#' * 10 + f'chunk {i} start' + '#' * 10)\n",
    "    #     content = context.node.get_content(metadata_mode=MetadataMode.LLM)\n",
    "    #     print(content)\n",
    "    #     print('#' * 10 + f'chunk {i} end' + '#' * 10)\n",
    "    # print('-' * 10 + 'ref' + '-' * 10)\n",
    "    response = chat_engine.chat(question)\n",
    "    print(f\"AI: {response}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: DeepSeek-V3 拥有总计671亿个参数，每个令牌激活37亿个参数。\n",
      "AI: DeepSeek-V3每次令牌激活370亿个参数。\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "741c79d5-5990-42ec-9c89-3e2eaa9b31ff",
   "metadata": {},
   "source": [
    "## 9. 工作流（Workflow）简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071629c4-1cbd-456c-bc55-ab38e4bedc7c",
   "metadata": {},
   "source": [
    "工作流顾名思义是对一些列工作步骤的抽象。\n",
    "\n",
    "LlamaIndex 的工作流是事件（`event`）驱动的：\n",
    "\n",
    "- 工作流由 `step` 组成\n",
    "- 每个 `step` 处理特定的事件\n",
    "- `step` 也会产生新的事件（交由后继的 `step` 进行处理）\n",
    "- 直到产生 `StopEvent` 整个工作流结束"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b8edc-2764-4f07-ad2e-1e8c549b8157",
   "metadata": {},
   "source": [
    "举个具体的例子：用自然语言查询数据库，数据库中包含多张表\n",
    "\n",
    "工作流：\n",
    "\n",
    "<img src=\"./assets/workflow.png\" alt=\"工作流\" width=\"1000\"/>\n",
    "\n",
    "分步说明：\n",
    "\n",
    "1. 用户输入自然语言查询\n",
    "2. 系统先去检索跟查询相关的表\n",
    "3. 根据表的 Schema 让大模型生成 SQL\n",
    "4. 用生成的 SQL 查询数据库\n",
    "5. 根据查询结果，调用大模型生成自然语言回复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be28368-98b7-4dcf-8cca-a05f443cb66b",
   "metadata": {},
   "source": [
    "### 9.1 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7487c6-f529-4987-b069-1e96d20303ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载 WikiTableQuestions\n",
    "# WikiTableQuestions 是一个为表格问答设计的数据集。其中包含 2,108 个从维基百科提取的 HTML 表格\n",
    "\n",
    "# !wget \"https://github.com/ppasupat/WikiTableQuestions/releases/download/v1.0.2/WikiTableQuestions-1.0.2-compact.zip\" -O wiki_data.zip\n",
    "# !unzip wiki_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7dfdbf-c064-4575-b59e-6562e86aff21",
   "metadata": {},
   "source": [
    "1. 遍历目录加载表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd4661dd-856a-4476-b771-1164fecadf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file: WikiTableQuestions\\csv\\200-csv\\0.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\1.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\10.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\11.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\12.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\14.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\15.csv\n",
      "Error parsing WikiTableQuestions\\csv\\200-csv\\15.csv: Error tokenizing data. C error: Expected 4 fields in line 16, saw 5\n",
      "\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\17.csv\n",
      "Error parsing WikiTableQuestions\\csv\\200-csv\\17.csv: Error tokenizing data. C error: Expected 6 fields in line 5, saw 7\n",
      "\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\18.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\20.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\22.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\24.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\25.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\26.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\28.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\29.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\3.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\30.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\31.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\32.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\33.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\34.csv\n",
      "Error parsing WikiTableQuestions\\csv\\200-csv\\34.csv: Error tokenizing data. C error: Expected 4 fields in line 6, saw 13\n",
      "\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\35.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\36.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\37.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\38.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\4.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\41.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\42.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\44.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\45.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\46.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\47.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\48.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\7.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\8.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\9.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"./WikiTableQuestions/csv/200-csv\")\n",
    "csv_files = sorted([f for f in data_dir.glob(\"*.csv\")])\n",
    "dfs = []\n",
    "for csv_file in csv_files:\n",
    "    print(f\"processing file: {csv_file}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {csv_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e7eef-d81a-4dd2-bcfd-1fb5dd89603a",
   "metadata": {},
   "source": [
    "2. 为每个表生成一段文字表述（用于检索），保存在 `WikiTableQuestions_TableInfo` 目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25c1a118-6f5e-470a-bb02-54fd5f1497ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "\n",
    "class TableInfo(BaseModel):\n",
    "    \"\"\"Information regarding a structured table.\"\"\"\n",
    "\n",
    "    table_name: str = Field(\n",
    "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
    "    )\n",
    "    table_summary: str = Field(\n",
    "        ..., description=\"short, concise summary/caption of the table\"\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_str = \"\"\"\\\n",
    "Give me a summary of the table with the following JSON format.\n",
    "\n",
    "- The table name must be unique to the table and describe it while being concise. \n",
    "- Do NOT output a generic table name (e.g. table, my_table).\n",
    "\n",
    "Do NOT make the table name one of the following: {exclude_table_name_list}\n",
    "\n",
    "Table:\n",
    "{table_str}\n",
    "\n",
    "Summary: \"\"\"\n",
    "prompt_tmpl = ChatPromptTemplate(\n",
    "    message_templates=[ChatMessage.from_str(prompt_str, role=\"user\")]\n",
    ")\n",
    "\n",
    "llm = DashScope(model_name=DashScopeGenerationModels.QWEN_MAX, api_key=os.getenv(\"DASHSCOPE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9aaded9f-4af5-4ab9-b994-374ac420bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableinfo_dir = \"WikiTableQuestions_TableInfo\"\n",
    "# !mkdir {tableinfo_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8e4bf4ce-f969-45a2-b705-adc23a0c1cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def _get_tableinfo_with_index(idx: int) -> str:\n",
    "    results_gen = Path(tableinfo_dir).glob(f\"{idx}_*\")\n",
    "    results_list = list(results_gen)\n",
    "    if len(results_list) == 0:\n",
    "        return None\n",
    "    elif len(results_list) == 1:\n",
    "        path = results_list[0]\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file) \n",
    "            return TableInfo.model_validate(data)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"More than one file matching index: {list(results_gen)}\"\n",
    "        )\n",
    "    \n",
    "\n",
    "table_names = set()\n",
    "table_infos = []\n",
    "for idx, df in enumerate(dfs):\n",
    "    table_info = _get_tableinfo_with_index(idx)\n",
    "    if table_info:\n",
    "        table_infos.append(table_info)\n",
    "    else:\n",
    "        while True:\n",
    "            df_str = df.head(10).to_csv()\n",
    "            table_info = llm.structured_predict(\n",
    "                TableInfo,\n",
    "                prompt_tmpl,\n",
    "                table_str=df_str,\n",
    "                exclude_table_name_list=str(list(table_names)),\n",
    "            )\n",
    "            table_name = table_info.table_name\n",
    "            print(f\"Processed table: {table_name}\")\n",
    "            if table_name not in table_names:\n",
    "                table_names.add(table_name)\n",
    "                break\n",
    "            else:\n",
    "                # try again\n",
    "                print(f\"Table name {table_name} already exists, trying again.\")\n",
    "                pass\n",
    "\n",
    "        out_file = f\"{tableinfo_dir}/{idx}_{table_name}.json\"\n",
    "        json.dump(table_info.dict(), open(out_file, \"w\"))\n",
    "    table_infos.append(table_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73345b81-07ff-49b0-9ccd-ed553c195f93",
   "metadata": {},
   "source": [
    "3. 将上述表格存入 SQLite 数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09b6fd32-3255-4f2d-9962-eb9449f56565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table: progressive_rock_album_chart_positions\n",
      "Creating table: filmography_of_diane\n",
      "Creating table: annual_fatalities_and_accidents_statistics\n",
      "Creating table: academy_awards_1972_results\n",
      "Creating table: theatrical_award_nominations_and_wins\n",
      "Creating table: bad_boy_artists_album_release_summary\n",
      "Creating table: south_dakota_radio_stations\n",
      "Creating table: missing_persons_case_summary_1982\n",
      "Creating table: chart_performance_of_singles\n",
      "Creating table: kodachrome_film_types_and_dates\n",
      "Creating table: bbc_radio_service_costs_2012_2013\n",
      "Creating table: french_airports_usage_summary\n",
      "Creating table: voter_registration_summary_by_party\n",
      "Creating table: norwegian_club_performance_statistics\n",
      "Creating table: triple_crown_winners_history\n",
      "Creating table: grammy_awards_summary_for_artist\n",
      "Creating table: boxing_fight_results_history\n",
      "Creating table: historical_sports_team_performance\n",
      "Creating table: yamato_population_density_summary\n",
      "Creating table: voter_registration_summary_by_party_distribution\n",
      "Creating table: best_actress_award_nominations_and_wins\n",
      "Creating table: uk_ministerial_positions_and_titles_history\n",
      "Creating table: municipality_merger_summary\n",
      "Creating table: euro_2020_group_stage_results\n",
      "Creating table: binary_encoding_probabilities\n",
      "Creating table: monthly_climate_statistics\n",
      "Creating table: italian_government_term_history\n",
      "Creating table: new_mexico_government_officials\n",
      "Creating table: monthly_climate_statistics_summary\n",
      "Creating table: historical_rainfall_experiment_drops\n",
      "Creating table: monthly_weather_statistics\n",
      "Creating table: multilingual_greetings_and_phrases\n",
      "Creating table: ohio_private_schools_summary\n",
      "Creating table: cancer_related_genetic_factors\n"
     ]
    }
   ],
   "source": [
    "# put data into sqlite db\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    ")\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to create a sanitized column name\n",
    "def sanitize_column_name(col_name):\n",
    "    # Remove special characters and replace spaces with underscores\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "\n",
    "# Function to create a table from a DataFrame using SQLAlchemy\n",
    "def create_table_from_dataframe(\n",
    "    df: pd.DataFrame, table_name: str, engine, metadata_obj\n",
    "):\n",
    "    # Sanitize column names\n",
    "    sanitized_columns = {col: sanitize_column_name(col) for col in df.columns}\n",
    "    df = df.rename(columns=sanitized_columns)\n",
    "\n",
    "    # Dynamically create columns based on DataFrame columns and data types\n",
    "    columns = [\n",
    "        Column(col, String if dtype == \"object\" else Integer)\n",
    "        for col, dtype in zip(df.columns, df.dtypes)\n",
    "    ]\n",
    "\n",
    "    # Create a table with the defined columns\n",
    "    table = Table(table_name, metadata_obj, *columns)\n",
    "\n",
    "    # Create the table in the database\n",
    "    metadata_obj.create_all(engine)\n",
    "\n",
    "    # Insert data from DataFrame into the table\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in df.iterrows():\n",
    "            insert_stmt = table.insert().values(**row.to_dict())\n",
    "            conn.execute(insert_stmt)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "# engine = create_engine(\"sqlite:///:memory:\")\n",
    "engine = create_engine(\"sqlite:///wiki_table_questions.db\")\n",
    "metadata_obj = MetaData()\n",
    "for idx, df in enumerate(dfs):\n",
    "    tableinfo = _get_tableinfo_with_index(idx)\n",
    "    print(f\"Creating table: {tableinfo.table_name}\")\n",
    "    create_table_from_dataframe(df, tableinfo.table_name, engine, metadata_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6ab4f-7db7-47ef-ad8a-898b19540d56",
   "metadata": {},
   "source": [
    "### 9.2 构建基础工具"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ebdd1-88e4-4e3e-b025-000890ac300e",
   "metadata": {},
   "source": [
    "1. 创建基于表的描述的向量索引\n",
    "\n",
    "- `ObjectIndex` 是一个 LlamaIndex 内置的模块，通过索引 (Index）检索任意 Python 对象\n",
    "- 这里我们使用 `VectorStoreIndex` 也就是向量检索，并通过 `SQLTableNodeMapping` 将文本描述的 `node` 和数据库的表形成映射\n",
    "- 相关文档：https://docs.llamaindex.ai/en/stable/examples/objects/object_index/#the-objectindex-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49e12988-cc89-4618-b3f6-a6a038c3c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.objects import (\n",
    "    SQLTableNodeMapping,\n",
    "    ObjectIndex,\n",
    "    SQLTableSchema,\n",
    ")\n",
    "from llama_index.core import SQLDatabase, VectorStoreIndex\n",
    "\n",
    "sql_database = SQLDatabase(engine)\n",
    "\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "table_schema_objs = [\n",
    "    SQLTableSchema(table_name=t.table_name, context_str=t.table_summary)\n",
    "    for t in table_infos\n",
    "]  # add a SQLTableSchema for each table\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    table_schema_objs,\n",
    "    table_node_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92b78b-0530-468d-90ec-309d1614d390",
   "metadata": {},
   "source": [
    "2. 创建 SQL 查询器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "948b8bee-2af6-4487-8b27-a2b2a5d7420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import SQLRetriever\n",
    "from typing import List\n",
    "\n",
    "sql_retriever = SQLRetriever(sql_database)\n",
    "\n",
    "\n",
    "def get_table_context_str(table_schema_objs: List[SQLTableSchema]):\n",
    "    \"\"\"Get table context string.\"\"\"\n",
    "    context_strs = []\n",
    "    for table_schema_obj in table_schema_objs:\n",
    "        table_info = sql_database.get_single_table_info(\n",
    "            table_schema_obj.table_name\n",
    "        )\n",
    "        if table_schema_obj.context_str:\n",
    "            table_opt_context = \" The table description is: \"\n",
    "            table_opt_context += table_schema_obj.context_str\n",
    "            table_info += table_opt_context\n",
    "\n",
    "        context_strs.append(table_info)\n",
    "    return \"\\n\\n\".join(context_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5bac2-4c10-4411-bcd1-d825727df41f",
   "metadata": {},
   "source": [
    "3. 创建 Text2SQL 的提示词（系统默认模板），和输出结果解析器（从生成的文本中抽取SQL）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da0c9ea7-5aa5-4069-8baf-9ca62c9e278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. You can order the results by a relevant column to return the most interesting examples in the database.\n",
      "\n",
      "Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
      "\n",
      "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Pay attention to which column is in which table. Also, qualify column names with the table name when needed. You are required to use the following format, each taking one line:\n",
      "\n",
      "Question: Question here\n",
      "SQLQuery: SQL Query to run\n",
      "SQLResult: Result of the SQLQuery\n",
      "Answer: Final answer here\n",
      "\n",
      "Only use tables listed below.\n",
      "{schema}\n",
      "\n",
      "Question: {query_str}\n",
      "SQLQuery: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.prompts.default_prompts import DEFAULT_TEXT_TO_SQL_PROMPT\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.llms import ChatResponse\n",
    "\n",
    "def parse_response_to_sql(chat_response: ChatResponse) -> str:\n",
    "    \"\"\"Parse response to SQL.\"\"\"\n",
    "    response = chat_response.message.content\n",
    "    sql_query_start = response.find(\"SQLQuery:\")\n",
    "    if sql_query_start != -1:\n",
    "        response = response[sql_query_start:]\n",
    "        # TODO: move to removeprefix after Python 3.9+\n",
    "        if response.startswith(\"SQLQuery:\"):\n",
    "            response = response[len(\"SQLQuery:\") :]\n",
    "    sql_result_start = response.find(\"SQLResult:\")\n",
    "    if sql_result_start != -1:\n",
    "        response = response[:sql_result_start]\n",
    "    return response.strip().strip(\"```\").strip()\n",
    "\n",
    "\n",
    "text2sql_prompt = DEFAULT_TEXT_TO_SQL_PROMPT.partial_format(\n",
    "    dialect=engine.dialect.name\n",
    ")\n",
    "print(text2sql_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c7cf0-1217-4d16-8196-2937411bbbed",
   "metadata": {},
   "source": [
    "4. 创建自然语言回复生成模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1fbf506-9c46-44bd-9148-0712e4e835a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesis_prompt_str = (\n",
    "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"SQL: {sql_query}\\n\"\n",
    "    \"SQL Response: {context_str}\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "response_synthesis_prompt = PromptTemplate(\n",
    "    response_synthesis_prompt_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f4b9121-7da6-496c-8506-48fee7d4ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = DashScope(model_name=DashScopeGenerationModels.QWEN_MAX, api_key=os.getenv(\"DASHSCOPE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258cc672-bed9-45b6-a9cd-80d162f96fd1",
   "metadata": {},
   "source": [
    "### 9.3 定义工作流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5f9db-6dc7-4828-bd2b-1ff7d1e16abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    "    Context,\n",
    "    Event,\n",
    ")\n",
    "\n",
    "# 事件：找到了数据库中相关的表\n",
    "class TableRetrieveEvent(Event):\n",
    "    \"\"\"Result of running table retrieval.\"\"\"\n",
    "\n",
    "    table_context_str: str\n",
    "    query: str\n",
    "\n",
    "# 事件：文本转为了 SQL\n",
    "class TextToSQLEvent(Event):\n",
    "    \"\"\"Text-to-SQL event.\"\"\"\n",
    "\n",
    "    sql: str\n",
    "    query: str\n",
    "\n",
    "\n",
    "class TextToSQLWorkflow1(Workflow):\n",
    "    \"\"\"Text-to-SQL Workflow that does query-time table retrieval.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obj_retriever,\n",
    "        text2sql_prompt,\n",
    "        sql_retriever,\n",
    "        response_synthesis_prompt,\n",
    "        llm,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.obj_retriever = obj_retriever\n",
    "        self.text2sql_prompt = text2sql_prompt\n",
    "        self.sql_retriever = sql_retriever\n",
    "        self.response_synthesis_prompt = response_synthesis_prompt\n",
    "        self.llm = llm\n",
    "\n",
    "    @step\n",
    "    def retrieve_tables(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> TableRetrieveEvent:\n",
    "        \"\"\"Retrieve tables.\"\"\"\n",
    "        table_schema_objs = self.obj_retriever.retrieve(ev.query)\n",
    "        table_context_str = get_table_context_str(table_schema_objs)\n",
    "        print(\"====\\n\"+table_context_str+\"\\n====\")\n",
    "        return TableRetrieveEvent(\n",
    "            table_context_str=table_context_str, query=ev.query\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    def generate_sql(\n",
    "        self, ctx: Context, ev: TableRetrieveEvent\n",
    "    ) -> TextToSQLEvent:\n",
    "        \"\"\"Generate SQL statement.\"\"\"\n",
    "        fmt_messages = self.text2sql_prompt.format_messages(\n",
    "            query_str=ev.query, schema=ev.table_context_str\n",
    "        )\n",
    "        chat_response = self.llm.chat(fmt_messages)\n",
    "        sql = parse_response_to_sql(chat_response)\n",
    "        print(\"====\\n\"+sql+\"\\n====\")\n",
    "        return TextToSQLEvent(sql=sql, query=ev.query)\n",
    "\n",
    "    @step\n",
    "    def generate_response(self, ctx: Context, ev: TextToSQLEvent) -> StopEvent:\n",
    "        \"\"\"Run SQL retrieval and generate response.\"\"\"\n",
    "        retrieved_rows = self.sql_retriever.retrieve(ev.sql)\n",
    "        print(\"====\\n\"+str(retrieved_rows)+\"\\n====\")\n",
    "        fmt_messages = self.response_synthesis_prompt.format_messages(\n",
    "            sql_query=ev.sql,\n",
    "            context_str=str(retrieved_rows),\n",
    "            query_str=ev.query,\n",
    "        )\n",
    "        chat_response = llm.chat(fmt_messages)\n",
    "        return StopEvent(result=chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b3060a2-20e4-4153-b3f9-6b91ed4d7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = TextToSQLWorkflow1(\n",
    "    obj_retriever,\n",
    "    text2sql_prompt,\n",
    "    sql_retriever,\n",
    "    response_synthesis_prompt,\n",
    "    llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ead701cd-c79d-4b48-8a20-9c2322140712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step retrieve_tables\n",
      "Step retrieve_tables produced event TableRetrieveEvent\n",
      "Running step generate_sql\n",
      "Step generate_sql produced event TextToSQLEvent\n",
      "Running step generate_response\n",
      "Step generate_response produced event StopEvent\n",
      "assistant: The Notorious B.I.G. was signed to Bad Boy in the year 1993.\n"
     ]
    }
   ],
   "source": [
    "response = workflow.run(\n",
    "    query=\"What was the year that The Notorious B.I.G was signed to Bad Boy?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b64fac-b8b8-4fad-8ecc-5a9431469706",
   "metadata": {},
   "source": [
    "### 9.4 可视化工作流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b370ab8-9c87-4baa-92ba-3e729a481857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-utils-workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "933d00bd-a602-4958-bed3-d1b530192895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "<class '__main__.TextToSQLEvent'>\n",
      "<class '__main__.TableRetrieveEvent'>\n",
      "text_to_sql_table_retrieval.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(\n",
    "    TextToSQLWorkflow1, filename=\"text_to_sql_table_retrieval.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc8317-0cac-458b-9ee5-969e4b46442c",
   "metadata": {},
   "source": [
    "### 9.5 工作流管理框架意义是什么\n",
    "\n",
    "思考以下情况：\n",
    "\n",
    "- `step` 的执行顺序有逻辑分支\n",
    "- `step` 的执行有循环\n",
    "- `step` 的执行可以并行\n",
    "- 一个 `step` 的触发条件依赖前面若干 `step` 的结果，且它们之间可能有循环或者并行\n",
    "\n",
    "<img src=\"./assets/workflow2.png\" alt=\"工作流举例\" width=\"800\"/>\n",
    "\n",
    "所以，工作流管理框架的意思是便于将单个事件的处理逻辑和事件之间的执行顺序独立开\n",
    "\n",
    "关于 LlamaIndex 工作流的更详细文档：https://docs.llamaindex.ai/en/stable/examples/workflow/workflows_cookbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c98c0-6636-4d71-9156-731e7ca28b6f",
   "metadata": {},
   "source": [
    "## 10. LlamaIndex 的更多功能\n",
    "\n",
    "- 智能体（Agent）开发框架：https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/\n",
    "- RAG 的评测：https://docs.llamaindex.ai/en/stable/module_guides/evaluating/\n",
    "- 过程监控：https://docs.llamaindex.ai/en/stable/module_guides/observability/\n",
    "\n",
    "以上内容涉及较多背景知识，暂时不在本课展开，相关知识会在后面课程中逐一详细讲解。\n",
    "\n",
    "此外，LlamaIndex 针对生产级的 RAG 系统中遇到的各个方面的细节问题，总结了很多高端技巧（[Advanced Topics](https://docs.llamaindex.ai/en/stable/optimizing/production_rag/)），对实战很有参考价值，非常推荐有能力的同学阅读。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9b8c6",
   "metadata": {},
   "source": [
    "## 11. 学习打卡\n",
    "\n",
    "1. 掌握 LlamaIndex 框架核心模块\n",
    "2. 使用 LlamaIndex 高效开发一个贴合自己需求的RAG系统\n",
    "3. 理解 LlamaIndex 中的工作流实现"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
